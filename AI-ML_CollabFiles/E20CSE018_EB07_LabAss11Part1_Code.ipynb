{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab Ass11Part1_Code_Dharvi Mittal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "wTEMsvrpCzPG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categories = [\n",
        "    \"alt.atheism\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"comp.graphics\",\n",
        "    \"sci.space\",\n",
        "]\n",
        "\n",
        "# complete the below statement(s). Take parameter subset=\"all\" \n",
        "dataset = fetch_20newsgroups(subset='all', categories=categories, \n",
        "                             shuffle=False, remove=('headers', 'footers', 'quotes'))\n",
        "\n",
        "labels = dataset.target\n",
        "true_k = np.unique(labels).shape[0]\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "data_tfidf = vectorizer.fit_transform(dataset.data)"
      ],
      "metadata": {
        "id": "jm2D9BV6EchC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y-78naQ2_hKu"
      },
      "outputs": [],
      "source": [
        "def get_initial_centroids(data, k, seed=None):\n",
        "    '''Randomly choose k data points as initial centroids'''\n",
        "    if seed is not None: # useful for obtaining consistent results\n",
        "        np.random.seed(seed)\n",
        "    n = data.shape[0] # number of data points\n",
        "        \n",
        "    # Pick K indices from range [0, N) using randint.\n",
        "    rand_indices = np.random.randint(0, n, k)\n",
        "    \n",
        "   \n",
        "    # As long as at least one document in a cluster contains a word,\n",
        "    # it will carry a nonzero weight in the TF-IDF vector of the centroid.\n",
        "    centroids = data[rand_indices,:].toarray()\n",
        "    \n",
        "    return centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After initialization, the k-means algorithm iterates between the following two steps:\n",
        "1. Assign each data point to the closest centroid.\n",
        "$$\n",
        "z_i \\gets \\mathrm{argmin}_j \\|\\mu_j - \\mathbf{x}_i\\|^2\n",
        "$$\n",
        "2. Revise centroids as the mean of the assigned data points.\n",
        "$$\n",
        "\\mu_j \\gets \\frac{1}{n_j}\\sum_{i:z_i=j} \\mathbf{x}_i\n",
        "$$"
      ],
      "metadata": {
        "id": "t4UCscV-E-0T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In pseudocode, we iteratively do the following:\n",
        "```\n",
        "cluster_assignment = assign_clusters(data, centroids)\n",
        "centroids = revise_centroids(data, k, cluster_assignment)\n",
        "```"
      ],
      "metadata": {
        "id": "EVTPfz1zFFfS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do we implement Step 1 of the main k-means loop above? First import `pairwise_distances` function from scikit-learn, which calculates Euclidean distances between rows of given arrays. See [this documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.pairwise_distances.html) for more information.\n",
        "\n",
        "For the sake of demonstration, let's look at documents 100 through 102 as query documents and compute the distances between each of these documents and every other document in the corpus. In the k-means algorithm, we will have to compute pairwise distances between the set of centroids and the set of documents."
      ],
      "metadata": {
        "id": "1xEB_f82FNWG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Goo3XN5FTST",
        "outputId": "e7d60b8b-e65e-4de6-c683-b8ad634c3a30"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3387, 33836)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "\n",
        "# Get the TF-IDF vectors for documents 100 through 102.\n",
        "queries = data_tfidf[100:102,:]\n",
        "\n",
        "# Compute pairwise distances from every data point to each query vector.\n",
        "dist = pairwise_distances(data_tfidf, queries, metric='euclidean')\n",
        "\n",
        "print(dist)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4rTOuxUFEfP",
        "outputId": "667f0d1e-d7a2-4e11-acaa-ab896cfada8c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.38066901 1.        ]\n",
            " [1.35581767 1.        ]\n",
            " [1.37696773 1.        ]\n",
            " ...\n",
            " [1.39380324 1.        ]\n",
            " [1.35955448 1.        ]\n",
            " [1.37841014 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dist.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8EIWvZEFjbO",
        "outputId": "deef1cba-631b-4c6b-f95e-01645dbacbbf"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3387, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint:** Next, given the pairwise distances, we take the minimum of the distances for each data point. Fittingly, NumPy provides an `argmin` function. See [this documentation](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.argmin.html) for details.\n",
        "\n",
        "Read the documentation and write code to produce a 1D array whose i-th entry indicates the centroid that is the closest to the i-th data point. Use the list of distances from the previous checkpoint and save them as `distances`. The value 0 indicates closeness to the first centroid, 1 indicates closeness to the second centroid, and so forth. Save this array as `closest_cluster`.\n",
        "\n",
        "**Hint:** the resulting array should be as long as the number of data points."
      ],
      "metadata": {
        "id": "ZZgqsPfqF2zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "closest_cluster = np.argmin(dist, axis = 1)"
      ],
      "metadata": {
        "id": "zgif3iYmHOs7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are ready to fill in the blanks in this function:"
      ],
      "metadata": {
        "id": "_eWRW6mfIgGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def assign_clusters(data, centroids):\n",
        "    \n",
        "    # Compute distances between each data point and the set of centroids:\n",
        "    # Fill in the blank (RHS only)\n",
        "    distances_from_centroids = pairwise_distances(data, centroids, metric='euclidean')\n",
        "\n",
        "    \n",
        "    # Compute cluster assignments for each data point:\n",
        "    # Fill in the blank (RHS only)\n",
        "    cluster_assignment = np.argmin(distances_from_centroids, axis = 1)\n",
        "    \n",
        "    return cluster_assignment"
      ],
      "metadata": {
        "id": "0Kz6SjL0GkJ4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revising clusters"
      ],
      "metadata": {
        "id": "DoDk5wYpI4M2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SciPy and NumPy arrays allow for filtering via Boolean masks. For instance, we filter all data points that are assigned to cluster 0 by writing\n",
        "```\n",
        "data[cluster_assignment==0,:]\n",
        "```"
      ],
      "metadata": {
        "id": "bQGwQ0J-I6qd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To develop intuition about filtering, let's look at a toy example consisting of 3 data points and 2 clusters."
      ],
      "metadata": {
        "id": "GnjASCPrJKFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array([[1., 2., 0.],\n",
        "                 [0., 0., 0.],\n",
        "                 [2., 2., 0.]])\n",
        "centroids = np.array([[0.5, 0.5, 0.],\n",
        "                      [0., -0.5, 0.]])"
      ],
      "metadata": {
        "id": "ZefzfZvSIvyP"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's assign these data points to the closest centroid."
      ],
      "metadata": {
        "id": "pDNNbGNKJO5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_assignment = assign_clusters(data, centroids)\n",
        "print(cluster_assignment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SIChpKLJMkk",
        "outputId": "b69415e9-1a9b-487e-fc49-61505b54829a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQzaS3zZ5TRt"
      },
      "source": [
        "The expression `cluster_assignment==1` gives a list of Booleans that says whether each data point is assigned to cluster 1 or not:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "XiDavFpE5TRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75e48a6e-7c87-4cb4-95b0-f4b1fb9a8ee7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False,  True, False])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "cluster_assignment==1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcxeFvNu5TRt"
      },
      "source": [
        "Likewise for cluster 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qHGq0sjD5TRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4d60074-7e61-470f-a90f-019a3c2cc3ae"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True, False,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "cluster_assignment==0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5xeGCYC5TRt"
      },
      "source": [
        "In lieu of indices, we can put in the list of Booleans to pick and choose rows. Only the rows that correspond to a `True` entry will be retained.\n",
        "\n",
        "First, let's look at the data points (i.e., their values) assigned to cluster 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "YEUo-59K5TRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3996e6-894b-4bfa-87b0-241c8ff46322"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "data[cluster_assignment==1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQYJZYC75TRt"
      },
      "source": [
        "This makes sense since [0 0 0] is closer to [0 -0.5 0] than to [0.5 0.5 0].\n",
        "\n",
        "Now let's look at the data points assigned to cluster 0:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iBhy1tlP5TRt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f466c175-e5be-4749-b8c8-846d78ad48df"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2., 0.],\n",
              "       [2., 2., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "data[cluster_assignment==0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABlj36L45TRu"
      },
      "source": [
        "Again, this makes sense since these values are each closer to [0.5 0.5 0] than to [0 -0.5 0].\n",
        "\n",
        "Given all the data points in a cluster, it only remains to compute the mean. Use [np.mean()](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.mean.html). By default, the function averages all elements in a 2D array. To compute row-wise or column-wise means, add the `axis` argument. See the linked documentation for details. \n",
        "\n",
        "Use this function to average the data points in cluster 0:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data[cluster_assignment==0].mean(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opAPfYeFKiAN",
        "outputId": "90a4b0d1-5a1d-4aec-897e-ddffa0df737f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1.5, 2. , 0. ])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "evSylzGr5TRu"
      },
      "source": [
        "We are now ready to complete this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "collapsed": true,
        "id": "NbMgzvhn5TRu"
      },
      "outputs": [],
      "source": [
        "def revise_centroids(data, k, cluster_assignment):\n",
        "    new_centroids = []\n",
        "    for i in range(k):\n",
        "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
        "        member_data_points = data[cluster_assignment==i]\n",
        "        # Compute the mean of the data points. Fill in the blank (RHS only)\n",
        "        centroid = data[cluster_assignment==i].mean(axis=0)\n",
        "        \n",
        "        # Convert numpy.matrix type to numpy.ndarray type\n",
        "        centroid = centroid.A1\n",
        "        new_centroids.append(centroid)\n",
        "    new_centroids = np.array(new_centroids)\n",
        "    \n",
        "    return new_centroids"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Checkpoint**. Let's check our Step 2 implementation. Letting rows 0, 10, ..., 90 of `tf_idf` as the data points and the cluster labels `[0, 1, 1, 0, 0, 2, 0, 2, 2, 1]`, we compute the next set of centroids. Each centroid is given by the average of all member data points in corresponding cluster."
      ],
      "metadata": {
        "id": "zrhKyGcuLtDA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = revise_centroids(data_tfidf[0:100:10], 3, np.array([0, 1, 1, 0, 0, 2, 0, 2, 2, 1]))\n",
        "if np.allclose(result[0], np.mean(data_tfidf[[0,30,40,60]].toarray(), axis=0)) and \\\n",
        "   np.allclose(result[1], np.mean(data_tfidf[[10,20,90]].toarray(), axis=0))   and \\\n",
        "   np.allclose(result[2], np.mean(data_tfidf[[50,70,80]].toarray(), axis=0)):\n",
        "    print('Pass')\n",
        "else:\n",
        "    print('Check your code')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oi_xkKhLZDX",
        "outputId": "3fec78c0-5d3c-4c38-f99f-0dd4fb61a7c8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pass\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assessing convergence"
      ],
      "metadata": {
        "id": "6vP547M4LxeL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How can we tell if the k-means algorithm is converging? We can look at the cluster assignments and see if they stabilize over time. In fact, we'll be running the algorithm until the cluster assignments stop changing at all. To be extra safe, and to assess the clustering performance, we'll be looking at an additional criteria: the sum of all squared distances between data points and centroids. This is defined as\n",
        "$$\n",
        "J(\\mathcal{Z},\\mu) = \\sum_{j=1}^k \\sum_{i:z_i = j} \\|\\mathbf{x}_i - \\mu_j\\|^2.\n",
        "$$\n",
        "The smaller the distances, the more homogeneous the clusters are. In other words, we'd like to have \"tight\" clusters."
      ],
      "metadata": {
        "id": "5UU8tOd6Lyb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_heterogeneity(data, k, centroids, cluster_assignment):\n",
        "    \n",
        "    heterogeneity = 0.0\n",
        "    for i in range(k):\n",
        "        \n",
        "        # Select all data points that belong to cluster i. Fill in the blank (RHS only)\n",
        "        member_data_points = data[cluster_assignment==i, :]\n",
        "        \n",
        "        if member_data_points.shape[0] > 0: # check if i-th cluster is non-empty\n",
        "            # Compute distances from centroid to data points (RHS only)\n",
        "            distances = pairwise_distances(member_data_points, [centroids[i]], metric='euclidean')\n",
        "            squared_distances = distances**2\n",
        "            heterogeneity += np.sum(squared_distances)\n",
        "        \n",
        "    return heterogeneity"
      ],
      "metadata": {
        "id": "3QtztRLZLhUL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compute the cluster heterogeneity for the 2-cluster example we've been considering based on our current cluster assignments and centroids."
      ],
      "metadata": {
        "id": "PF8GmmIxMJYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "compute_heterogeneity(data, 2, centroids, cluster_assignment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaQkiv-kMCHf",
        "outputId": "b2d6ff9f-4935-44fe-b14e-59837b49acd7"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.25"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combining into a single function"
      ],
      "metadata": {
        "id": "pXOtLJl-MYx2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the two k-means steps have been implemented, as well as our heterogeneity metric we wish to monitor, it is only a matter of putting these functions together to write a k-means algorithm that\n",
        "\n",
        "* Repeatedly performs Steps 1 and 2\n",
        "* Tracks convergence metrics\n",
        "* Stops if either no assignment changed or we reach a certain number of iterations."
      ],
      "metadata": {
        "id": "b4oaAWxAMbKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill in the blanks\n",
        "def kmeans(data, k, initial_centroids, maxiter, record_heterogeneity=None, verbose=False):\n",
        "    '''This function runs k-means on given data and initial set of centroids.\n",
        "       maxiter: maximum number of iterations to run.\n",
        "       record_heterogeneity: (optional) a list, to store the history of heterogeneity as function of iterations\n",
        "                             if None, do not store the history.\n",
        "       verbose: if True, print how many data points changed their cluster labels in each iteration'''\n",
        "    centroids = initial_centroids[:]\n",
        "    prev_cluster_assignment = None\n",
        "    \n",
        "    for itr in range(maxiter):        \n",
        "        if verbose:\n",
        "            print(itr)\n",
        "        \n",
        "        # 1. Make cluster assignments using nearest centroids\n",
        "        # YOUR CODE HERE\n",
        "        cluster_assignment = assign_clusters(data, centroids)\n",
        "            \n",
        "        # 2. Compute a new centroid for each of the k clusters, averaging all data points assigned to that cluster.\n",
        "        # YOUR CODE HERE\n",
        "        centroids = revise_centroids(data, k, cluster_assignment)\n",
        "            \n",
        "        # Check for convergence: if none of the assignments changed, stop\n",
        "        if prev_cluster_assignment is not None and \\\n",
        "          (prev_cluster_assignment==cluster_assignment).all():\n",
        "            break\n",
        "        \n",
        "        # Print number of new assignments \n",
        "        if prev_cluster_assignment is not None:\n",
        "            num_changed = np.sum(prev_cluster_assignment!=cluster_assignment)\n",
        "            if verbose:\n",
        "                print('    {0:5d} elements changed their cluster assignment.'.format(num_changed))   \n",
        "        \n",
        "        # Record heterogeneity convergence metric\n",
        "        if record_heterogeneity is not None:\n",
        "            # YOUR CODE HERE\n",
        "            score = compute_heterogeneity(data, k, centroids, cluster_assignment)\n",
        "            record_heterogeneity.append(score)\n",
        "        \n",
        "        prev_cluster_assignment = cluster_assignment[:]\n",
        "        \n",
        "    return centroids, cluster_assignment"
      ],
      "metadata": {
        "id": "BlvCHHEnMWB2"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plotting convergence metric"
      ],
      "metadata": {
        "id": "SKNtdEi5NLxl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the above function to plot the convergence metric across iterations."
      ],
      "metadata": {
        "id": "cvpy3cUCNTQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "g3q9aighNi2w"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_heterogeneity(heterogeneity, k):\n",
        "    plt.figure(figsize=(7,4))\n",
        "    plt.plot(heterogeneity, linewidth=4)\n",
        "    plt.xlabel('# Iterations')\n",
        "    plt.ylabel('Heterogeneity')\n",
        "    plt.title('Heterogeneity of clustering over time, K={0:d}'.format(k))\n",
        "    plt.rcParams.update({'font.size': 16})\n",
        "    plt.tight_layout()"
      ],
      "metadata": {
        "id": "_cIVlEhSNH4v"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider running k-means with K=3 clusters for a maximum of 400 iterations, recording cluster heterogeneity at every step.  Then, let's plot the heterogeneity over iterations using the plotting function above."
      ],
      "metadata": {
        "id": "ZlnhKbKgNcJS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3\n",
        "heterogeneity = []\n",
        "initial_centroids = get_initial_centroids(data_tfidf, k, seed=0)\n",
        "centroids, cluster_assignment = kmeans(data_tfidf, k, initial_centroids, maxiter=400,\n",
        "                                       record_heterogeneity=heterogeneity, verbose=True)\n",
        "# Identify the 10 most relevant terms in each cluster\n",
        "\n",
        "# plot_heterogeneity(heterogeneity, k)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBjRON4YNZV7",
        "outputId": "d8908c72-1d4e-47a4-adf8-928b362e4e32"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "      785 elements changed their cluster assignment.\n",
            "2\n",
            "      452 elements changed their cluster assignment.\n",
            "3\n",
            "      216 elements changed their cluster assignment.\n",
            "4\n",
            "      121 elements changed their cluster assignment.\n",
            "5\n",
            "       74 elements changed their cluster assignment.\n",
            "6\n",
            "       38 elements changed their cluster assignment.\n",
            "7\n",
            "       25 elements changed their cluster assignment.\n",
            "8\n",
            "       22 elements changed their cluster assignment.\n",
            "9\n",
            "       17 elements changed their cluster assignment.\n",
            "10\n",
            "       16 elements changed their cluster assignment.\n",
            "11\n",
            "       11 elements changed their cluster assignment.\n",
            "12\n",
            "        7 elements changed their cluster assignment.\n",
            "13\n",
            "        4 elements changed their cluster assignment.\n",
            "14\n",
            "        5 elements changed their cluster assignment.\n",
            "15\n",
            "        3 elements changed their cluster assignment.\n",
            "16\n",
            "        2 elements changed their cluster assignment.\n",
            "17\n",
            "        3 elements changed their cluster assignment.\n",
            "18\n",
            "        1 elements changed their cluster assignment.\n",
            "19\n",
            "        1 elements changed their cluster assignment.\n",
            "20\n",
            "        1 elements changed their cluster assignment.\n",
            "21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quiz Question**. Which of the cluster contains the greatest number of data points in the end? Hint: Use [`np.bincount()`](http://docs.scipy.org/doc/numpy-1.11.0/reference/generated/numpy.bincount.html) to count occurrences of each cluster label.\n",
        " 1. Cluster #0\n",
        " 2. Cluster #1\n",
        " 3. Cluster #2"
      ],
      "metadata": {
        "id": "6UupUFlZOuJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Beware of local maxima"
      ],
      "metadata": {
        "id": "nCtxNUhypqez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One weakness of k-means is that it tends to get stuck in a local minimum. To see this, let us run k-means multiple times, with different initial centroids created using different random seeds.\n",
        "\n",
        "**Note:** Again, in practice, you should set different seeds for every run. We give you a list of seeds for this assignment so that everyone gets the same answer.\n",
        "\n",
        "This may take several minutes to run."
      ],
      "metadata": {
        "id": "P5fjzz9kplHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n"
      ],
      "metadata": {
        "id": "NUoOfxgLSaJQ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "k = 10\n",
        "heterogeneity = {}\n",
        "import time\n",
        "start = time.time()\n",
        "try:\n",
        "  for seed in [0, 20000, 40000, 60000, 80000, 100000, 120000]:\n",
        "    initial_centroids = get_initial_centroids(data_tfidf, k, seed)\n",
        "    centroids, cluster_assignment = kmeans(data_tfidf, k, initial_centroids, maxiter=400,\n",
        "                                           record_heterogeneity=None, verbose=False)\n",
        "    # To save time, compute heterogeneity only once in the end\n",
        "    heterogeneity[seed] = compute_heterogeneity(data_tfidf, k, centroids, cluster_assignment)\n",
        "    print('seed={0:06d}, heterogeneity={1:.5f}'.format(seed, heterogeneity[seed]))\n",
        "    sys.stdout.flush()\n",
        "except:\n",
        "  pass\n",
        "end = time.time()\n",
        "print(end-start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-zsbeiTMN2HK",
        "outputId": "a3797f82-dc10-48fd-a8e9-9a451ee581ba"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "seed=000000, heterogeneity=3005.81526\n",
            "seed=020000, heterogeneity=3008.10874\n",
            "seed=040000, heterogeneity=3045.28692\n",
            "seed=060000, heterogeneity=3044.03155\n",
            "seed=080000, heterogeneity=3010.36519\n",
            "2.7634224891662598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OYfE7ayjPAtt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}