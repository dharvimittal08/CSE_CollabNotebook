{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation of Adam optimizer from scratch**"
      ],
      "metadata": {
        "id": "-LuEzWcGQ38_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "grrYyrAMQqQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28VQ-Sr5Qnu8"
      },
      "outputs": [],
      "source": [
        "# Define Class Adam Optimizer\n",
        "class AdamOptim():\n",
        "    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
        "        self.m_dw, self.v_dw = 0, 0\n",
        "        self.m_db, self.v_db = 0, 0\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.eta = eta\n",
        "    def update(self, t, w, b, dw, db):\n",
        "        ## define momentum gradient equation using beta1\n",
        "        # *** weights *** #\n",
        "        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n",
        "        # *** biases *** #\n",
        "        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n",
        "\n",
        "        ## define rmsprop gradient using beta2\n",
        "        # *** weights *** #\n",
        "        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n",
        "        # *** biases *** #\n",
        "        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n",
        "\n",
        "        ## bias correction\n",
        "        m_dw_corr = self.m_dw/(1-self.beta1**t)\n",
        "        m_db_corr = self.m_db/(1-self.beta1**t)\n",
        "        v_dw_corr = self.v_dw/(1-self.beta2**t)\n",
        "        v_db_corr = self.v_db/(1-self.beta2**t)\n",
        "\n",
        "        ## update weights and biases\n",
        "        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n",
        "        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n",
        "        return w, b"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(m):\n",
        "    return m**2-2*m+1\n",
        "\n",
        "## take derivative\n",
        "def grad_function(m):\n",
        "    return 2*m-2\n",
        "\n",
        "def check_convergence(w0, w1):\n",
        "    return (w0 == w1)"
      ],
      "metadata": {
        "id": "w0saAsWJRcF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize w_0 and b_0 with random values using function\n",
        "import random\n",
        "w_0 = random.random()\n",
        "b_0 = random.random()\n",
        "adam = AdamOptim()\n",
        "# Initialize iteration with 1\n",
        "t = 1\n",
        "converged = False\n",
        "\n",
        "while not converged:\n",
        "    dw = grad_function(w_0)\n",
        "    db = grad_function(b_0)\n",
        "    w_0_old = w_0\n",
        "    w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
        "    if check_convergence(w_0, w_0_old):\n",
        "        print('converged after '+str(t)+' iterations')\n",
        "        break\n",
        "    else:\n",
        "        print('iteration '+str(t)+': weight='+str(w_0))\n",
        "        t+=1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYk2OdjNRdfR",
        "outputId": "dd315f88-315a-45d5-d28f-1504f57d1651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 1: weight=0.015104835208899842\n",
            "iteration 2: weight=0.025102074447928162\n",
            "iteration 3: weight=0.03509468118910826\n",
            "iteration 4: weight=0.04508076540701274\n",
            "iteration 5: weight=0.05505842311483637\n",
            "iteration 6: weight=0.06502574066869864\n",
            "iteration 7: weight=0.07498079906614061\n",
            "iteration 8: weight=0.08492167820626587\n",
            "iteration 9: weight=0.09484646108009658\n",
            "iteration 10: weight=0.1047532378612828\n",
            "iteration 11: weight=0.1146401098692683\n",
            "iteration 12: weight=0.12450519337933096\n",
            "iteration 13: weight=0.13434662325652189\n",
            "iteration 14: weight=0.14416255639335906\n",
            "iteration 15: weight=0.15395117493412785\n",
            "iteration 16: weight=0.1637106892717357\n",
            "iteration 17: weight=0.1734393408061982\n",
            "iteration 18: weight=0.18313540445693954\n",
            "iteration 19: weight=0.19279719092411693\n",
            "iteration 20: weight=0.20242304869707686\n",
            "iteration 21: weight=0.21201136581077862\n",
            "iteration 22: weight=0.22156057135354149\n",
            "iteration 23: weight=0.23106913673176094\n",
            "iteration 24: weight=0.24053557669927406\n",
            "iteration 25: weight=0.24995845016082274\n",
            "iteration 26: weight=0.25933636076056127\n",
            "iteration 27: weight=0.26866795726777987\n",
            "iteration 28: weight=0.27795193377297744\n",
            "iteration 29: weight=0.287187029708121\n",
            "iteration 30: weight=0.29637202970539717\n",
            "iteration 31: weight=0.30550576330900336\n",
            "iteration 32: weight=0.3145871045545688\n",
            "iteration 33: weight=0.32361497143065554\n",
            "iteration 34: weight=0.33258832523649356\n",
            "iteration 35: weight=0.3415061698496688\n",
            "iteration 36: weight=0.3503675509169377\n",
            "iteration 37: weight=0.3591715549807011\n",
            "iteration 38: weight=0.3679173085529592\n",
            "iteration 39: weight=0.376603977147805\n",
            "iteration 40: weight=0.3852307642827113\n",
            "iteration 41: weight=0.3937969104580454\n",
            "iteration 42: weight=0.40230169212341715\n",
            "iteration 43: weight=0.41074442063864214\n",
            "iteration 44: weight=0.41912444123629167\n",
            "iteration 45: weight=0.4274411319920185\n",
            "iteration 46: weight=0.43569390280808973\n",
            "iteration 47: weight=0.44388219441484106\n",
            "iteration 48: weight=0.45200547739408625\n",
            "iteration 49: weight=0.46006325122787955\n",
            "iteration 50: weight=0.46805504337543613\n",
            "iteration 51: weight=0.47598040838047034\n",
            "iteration 52: weight=0.4838389270107088\n",
            "iteration 53: weight=0.49163020543088104\n",
            "iteration 54: weight=0.4993538744100774\n",
            "iteration 55: weight=0.5070095885639945\n",
            "iteration 56: weight=0.5145970256322605\n",
            "iteration 57: weight=0.5221158857907408\n",
            "iteration 58: weight=0.5295658909984718\n",
            "iteration 59: weight=0.5369467843786504\n",
            "iteration 60: weight=0.5442583296329151\n",
            "iteration 61: weight=0.5515003104880011\n",
            "iteration 62: weight=0.5586725301737111\n",
            "iteration 63: weight=0.5657748109310412\n",
            "iteration 64: weight=0.5728069935492085\n",
            "iteration 65: weight=0.5797689369302654\n",
            "iteration 66: weight=0.5866605176799308\n",
            "iteration 67: weight=0.593481629723238\n",
            "iteration 68: weight=0.6002321839435789\n",
            "iteration 69: weight=0.6069121078437126\n",
            "iteration 70: weight=0.6135213452273167\n",
            "iteration 71: weight=0.6200598558996636\n",
            "iteration 72: weight=0.6265276153860312\n",
            "iteration 73: weight=0.6329246146664799\n",
            "iteration 74: weight=0.6392508599256631\n",
            "iteration 75: weight=0.6455063723163745\n",
            "iteration 76: weight=0.6516911877355773\n",
            "iteration 77: weight=0.6578053566117045\n",
            "iteration 78: weight=0.6638489437020676\n",
            "iteration 79: weight=0.6698220278992584\n",
            "iteration 80: weight=0.6757247020454785\n",
            "iteration 81: weight=0.6815570727537846\n",
            "iteration 82: weight=0.6873192602352851\n",
            "iteration 83: weight=0.6930113981313788\n",
            "iteration 84: weight=0.6986336333501754\n",
            "iteration 85: weight=0.7041861259062895\n",
            "iteration 86: weight=0.7096690487632503\n",
            "iteration 87: weight=0.715082587677817\n",
            "iteration 88: weight=0.7204269410455401\n",
            "iteration 89: weight=0.7257023197469564\n",
            "iteration 90: weight=0.7309089469938495\n",
            "iteration 91: weight=0.7360470581750536\n",
            "iteration 92: weight=0.7411169007013244\n",
            "iteration 93: weight=0.7461187338488379\n",
            "iteration 94: weight=0.7510528286009228\n",
            "iteration 95: weight=0.7559194674876706\n",
            "iteration 96: weight=0.7607189444231036\n",
            "iteration 97: weight=0.7654515645396199\n",
            "iteration 98: weight=0.7701176440194669\n",
            "iteration 99: weight=0.7747175099230297\n",
            "iteration 100: weight=0.779251500013753\n",
            "iteration 101: weight=0.7837199625795447\n",
            "iteration 102: weight=0.7881232562505381\n",
            "iteration 103: weight=0.7924617498131199\n",
            "iteration 104: weight=0.7967358220201536\n",
            "iteration 105: weight=0.8009458613973575\n",
            "iteration 106: weight=0.8050922660458156\n",
            "iteration 107: weight=0.8091754434406261\n",
            "iteration 108: weight=0.8131958102257099\n",
            "iteration 109: weight=0.8171537920048246\n",
            "iteration 110: weight=0.8210498231288458\n",
            "iteration 111: weight=0.8248843464793953\n",
            "iteration 112: weight=0.8286578132489142\n",
            "iteration 113: weight=0.8323706827172909\n",
            "iteration 114: weight=0.8360234220251708\n",
            "iteration 115: weight=0.8396165059440863\n",
            "iteration 116: weight=0.8431504166435579\n",
            "iteration 117: weight=0.8466256434553276\n",
            "iteration 118: weight=0.8500426826348972\n",
            "iteration 119: weight=0.853402037120552\n",
            "iteration 120: weight=0.8567042162900583\n",
            "iteration 121: weight=0.8599497357152321\n",
            "iteration 122: weight=0.8631391169145791\n",
            "iteration 123: weight=0.8662728871042165\n",
            "iteration 124: weight=0.8693515789472867\n",
            "iteration 125: weight=0.8723757303020815\n",
            "iteration 126: weight=0.8753458839690943\n",
            "iteration 127: weight=0.8782625874372232\n",
            "iteration 128: weight=0.8811263926293483\n",
            "iteration 129: weight=0.8839378556475077\n",
            "iteration 130: weight=0.8866975365178965\n",
            "iteration 131: weight=0.8894059989359145\n",
            "iteration 132: weight=0.8920638100114848\n",
            "iteration 133: weight=0.8946715400148665\n",
            "iteration 134: weight=0.8972297621231812\n",
            "iteration 135: weight=0.899739052167872\n",
            "iteration 136: weight=0.9021999883833086\n",
            "iteration 137: weight=0.9046131511567507\n",
            "iteration 138: weight=0.9069791227798769\n",
            "iteration 139: weight=0.9092984872020836\n",
            "iteration 140: weight=0.9115718297857509\n",
            "iteration 141: weight=0.9137997370636707\n",
            "iteration 142: weight=0.9159827964988259\n",
            "iteration 143: weight=0.9181215962467025\n",
            "iteration 144: weight=0.9202167249203139\n",
            "iteration 145: weight=0.9222687713581065\n",
            "iteration 146: weight=0.9242783243949139\n",
            "iteration 147: weight=0.9262459726361172\n",
            "iteration 148: weight=0.9281723042351635\n",
            "iteration 149: weight=0.9300579066745887\n",
            "iteration 150: weight=0.9319033665506825\n",
            "iteration 151: weight=0.9337092693619281\n",
            "iteration 152: weight=0.9354761993013392\n",
            "iteration 153: weight=0.9372047390528148\n",
            "iteration 154: weight=0.9388954695916194\n",
            "iteration 155: weight=0.9405489699890938\n",
            "iteration 156: weight=0.9421658172216906\n",
            "iteration 157: weight=0.9437465859844256\n",
            "iteration 158: weight=0.9452918485088255\n",
            "iteration 159: weight=0.9468021743854461\n",
            "iteration 160: weight=0.9482781303910307\n",
            "iteration 161: weight=0.9497202803203675\n",
            "iteration 162: weight=0.9511291848229013\n",
            "iteration 163: weight=0.9525054012441457\n",
            "iteration 164: weight=0.9538494834719371\n",
            "iteration 165: weight=0.9551619817875636\n",
            "iteration 166: weight=0.9564434427217976\n",
            "iteration 167: weight=0.9576944089158519\n",
            "iteration 168: weight=0.9589154189872755\n",
            "iteration 169: weight=0.960107007400798\n",
            "iteration 170: weight=0.9612697043441257\n",
            "iteration 171: weight=0.9624040356086874\n",
            "iteration 172: weight=0.9635105224753213\n",
            "iteration 173: weight=0.9645896816048914\n",
            "iteration 174: weight=0.9656420249338132\n",
            "iteration 175: weight=0.9666680595744667\n",
            "iteration 176: weight=0.967668287720468\n",
            "iteration 177: weight=0.9686432065567677\n",
            "iteration 178: weight=0.9695933081745378\n",
            "iteration 179: weight=0.9705190794908065\n",
            "iteration 180: weight=0.9714210021727957\n",
            "iteration 181: weight=0.9722995525669116\n",
            "iteration 182: weight=0.9731552016323355\n",
            "iteration 183: weight=0.9739884148791591\n",
            "iteration 184: weight=0.974799652311004\n",
            "iteration 185: weight=0.9755893683720627\n",
            "iteration 186: weight=0.9763580118984965\n",
            "iteration 187: weight=0.9771060260741199\n",
            "iteration 188: weight=0.9778338483903031\n",
            "iteration 189: weight=0.9785419106100165\n",
            "iteration 190: weight=0.9792306387359447\n",
            "iteration 191: weight=0.9799004529825899\n",
            "iteration 192: weight=0.9805517677522859\n",
            "iteration 193: weight=0.9811849916150435\n",
            "iteration 194: weight=0.9818005272921408\n",
            "iteration 195: weight=0.9823987716433791\n",
            "iteration 196: weight=0.9829801156579137\n",
            "iteration 197: weight=0.9835449444485785\n",
            "iteration 198: weight=0.9840936372496135\n",
            "iteration 199: weight=0.9846265674177078\n",
            "iteration 200: weight=0.9851441024362706\n",
            "iteration 201: weight=0.985646603922839\n",
            "iteration 202: weight=0.9861344276395342\n",
            "iteration 203: weight=0.9866079235064753\n",
            "iteration 204: weight=0.9870674356180613\n",
            "iteration 205: weight=0.9875133022620295\n",
            "iteration 206: weight=0.9879458559412018\n",
            "iteration 207: weight=0.988365423397828\n",
            "iteration 208: weight=0.9887723256404363\n",
            "iteration 209: weight=0.9891668779731013\n",
            "iteration 210: weight=0.9895493900270416\n",
            "iteration 211: weight=0.9899201657944571\n",
            "iteration 212: weight=0.9902795036645191\n",
            "iteration 213: weight=0.9906276964614262\n",
            "iteration 214: weight=0.9909650314844387\n",
            "iteration 215: weight=0.9912917905498064\n",
            "iteration 216: weight=0.9916082500345064\n",
            "iteration 217: weight=0.9919146809217049\n",
            "iteration 218: weight=0.9922113488478624\n",
            "iteration 219: weight=0.9924985141514004\n",
            "iteration 220: weight=0.9927764319228479\n",
            "iteration 221: weight=0.9930453520563906\n",
            "iteration 222: weight=0.9933055193027432\n",
            "iteration 223: weight=0.9935571733232698\n",
            "iteration 224: weight=0.9938005487452738\n",
            "iteration 225: weight=0.9940358752183888\n",
            "iteration 226: weight=0.9942633774719917\n",
            "iteration 227: weight=0.9944832753735716\n",
            "iteration 228: weight=0.9946957839879826\n",
            "iteration 229: weight=0.9949011136375121\n",
            "iteration 230: weight=0.9950994699626986\n",
            "iteration 231: weight=0.9952910539838334\n",
            "iteration 232: weight=0.9954760621630828\n",
            "iteration 233: weight=0.9956546864671673\n",
            "iteration 234: weight=0.9958271144305386\n",
            "iteration 235: weight=0.995993529218995\n",
            "iteration 236: weight=0.9961541096936759\n",
            "iteration 237: weight=0.9963090304753823\n",
            "iteration 238: weight=0.9964584620091662\n",
            "iteration 239: weight=0.9966025706291372\n",
            "iteration 240: weight=0.9967415186234347\n",
            "iteration 241: weight=0.996875464299317\n",
            "iteration 242: weight=0.9970045620483159\n",
            "iteration 243: weight=0.9971289624114147\n",
            "iteration 244: weight=0.9972488121442001\n",
            "iteration 245: weight=0.9973642542819476\n",
            "iteration 246: weight=0.9974754282045957\n",
            "iteration 247: weight=0.9975824697015706\n",
            "iteration 248: weight=0.9976855110364211\n",
            "iteration 249: weight=0.9977846810112256\n",
            "iteration 250: weight=0.997880105030736\n",
            "iteration 251: weight=0.9979719051662236\n",
            "iteration 252: weight=0.9980602002189933\n",
            "iteration 253: weight=0.9981451057835343\n",
            "iteration 254: weight=0.998226734310277\n",
            "iteration 255: weight=0.9983051951679266\n",
            "iteration 256: weight=0.9983805947053465\n",
            "iteration 257: weight=0.9984530363129637\n",
            "iteration 258: weight=0.9985226204836718\n",
            "iteration 259: weight=0.9985894448732079\n",
            "iteration 260: weight=0.9986536043599804\n",
            "iteration 261: weight=0.9987151911043263\n",
            "iteration 262: weight=0.9987742946071777\n",
            "iteration 263: weight=0.9988310017681191\n",
            "iteration 264: weight=0.998885396942817\n",
            "iteration 265: weight=0.9989375619998053\n",
            "iteration 266: weight=0.9989875763766112\n",
            "iteration 267: weight=0.9990355171352061\n",
            "iteration 268: weight=0.9990814590167699\n",
            "iteration 269: weight=0.9991254744957537\n",
            "iteration 270: weight=0.9991676338332321\n",
            "iteration 271: weight=0.9992080051295326\n",
            "iteration 272: weight=0.9992466543761335\n",
            "iteration 273: weight=0.9992836455068221\n",
            "iteration 274: weight=0.999319040448105\n",
            "iteration 275: weight=0.999352899168863\n",
            "iteration 276: weight=0.9993852797292463\n",
            "iteration 277: weight=0.9994162383288032\n",
            "iteration 278: weight=0.99944582935384\n",
            "iteration 279: weight=0.9994741054240057\n",
            "iteration 280: weight=0.9995011174381015\n",
            "iteration 281: weight=0.9995269146191113\n",
            "iteration 282: weight=0.999551544558452\n",
            "iteration 283: weight=0.999575053259444\n",
            "iteration 284: weight=0.9995974851800001\n",
            "iteration 285: weight=0.9996188832745349\n",
            "iteration 286: weight=0.9996392890350945\n",
            "iteration 287: weight=0.9996587425317095\n",
            "iteration 288: weight=0.9996772824519727\n",
            "iteration 289: weight=0.9996949461398444\n",
            "iteration 290: weight=0.999711769633689\n",
            "iteration 291: weight=0.9997277877035461\n",
            "iteration 292: weight=0.9997430338876411\n",
            "iteration 293: weight=0.9997575405281391\n",
            "iteration 294: weight=0.9997713388061485\n",
            "iteration 295: weight=0.9997844587759777\n",
            "iteration 296: weight=0.9997969293986531\n",
            "iteration 297: weight=0.9998087785747034\n",
            "iteration 298: weight=0.9998200331762171\n",
            "iteration 299: weight=0.9998307190781793\n",
            "iteration 300: weight=0.9998408611890968\n",
            "iteration 301: weight=0.9998504834809169\n",
            "iteration 302: weight=0.9998596090182496\n",
            "iteration 303: weight=0.9998682599868994\n",
            "iteration 304: weight=0.9998764577217167\n",
            "iteration 305: weight=0.9998842227337755\n",
            "iteration 306: weight=0.9998915747368874\n",
            "iteration 307: weight=0.9998985326734601\n",
            "iteration 308: weight=0.9999051147397099\n",
            "iteration 309: weight=0.9999113384102364\n",
            "iteration 310: weight=0.9999172204619698\n",
            "iteration 311: weight=0.9999227769974995\n",
            "iteration 312: weight=0.9999280234677939\n",
            "iteration 313: weight=0.9999329746943202\n",
            "iteration 314: weight=0.9999376448905762\n",
            "iteration 315: weight=0.9999420476830406\n",
            "iteration 316: weight=0.999946196131554\n",
            "iteration 317: weight=0.9999501027491401\n",
            "iteration 318: weight=0.9999537795212766\n",
            "iteration 319: weight=0.999957237924626\n",
            "iteration 320: weight=0.9999604889452356\n",
            "iteration 321: weight=0.9999635430962188\n",
            "iteration 322: weight=0.9999664104349248\n",
            "iteration 323: weight=0.999969100579609\n",
            "iteration 324: weight=0.9999716227256126\n",
            "iteration 325: weight=0.9999739856610631\n",
            "iteration 326: weight=0.999976197782103\n",
            "iteration 327: weight=0.999978267107659\n",
            "iteration 328: weight=0.9999802012937604\n",
            "iteration 329: weight=0.9999820076474166\n",
            "iteration 330: weight=0.9999836931400633\n",
            "iteration 331: weight=0.9999852644205875\n",
            "iteration 332: weight=0.9999867278279402\n",
            "iteration 333: weight=0.9999880894033467\n",
            "iteration 334: weight=0.9999893549021244\n",
            "iteration 335: weight=0.9999905298051156\n",
            "iteration 336: weight=0.9999916193297473\n",
            "iteration 337: weight=0.9999926284407242\n",
            "iteration 338: weight=0.9999935618603659\n",
            "iteration 339: weight=0.9999944240785968\n",
            "iteration 340: weight=0.9999952193625967\n",
            "iteration 341: weight=0.9999959517661219\n",
            "iteration 342: weight=0.9999966251385041\n",
            "iteration 343: weight=0.9999972431333374\n",
            "iteration 344: weight=0.9999978092168593\n",
            "iteration 345: weight=0.9999983266760363\n",
            "iteration 346: weight=0.99999879862636\n",
            "iteration 347: weight=0.9999992280193639\n",
            "iteration 348: weight=0.9999996176498664\n",
            "iteration 349: weight=0.9999999701629496\n",
            "iteration 350: weight=1.0000002880606802\n",
            "iteration 351: weight=1.0000005737085804\n",
            "iteration 352: weight=1.0000008293418565\n",
            "iteration 353: weight=1.0000010570713915\n",
            "iteration 354: weight=1.0000012588895093\n",
            "iteration 355: weight=1.0000014366755168\n",
            "iteration 356: weight=1.0000015922010315\n",
            "iteration 357: weight=1.0000017271351007\n",
            "iteration 358: weight=1.0000018430491189\n",
            "iteration 359: weight=1.0000019414215489\n",
            "iteration 360: weight=1.0000020236424547\n",
            "iteration 361: weight=1.0000020910178502\n",
            "iteration 362: weight=1.0000021447738707\n",
            "iteration 363: weight=1.0000021860607735\n",
            "iteration 364: weight=1.0000022159567712\n",
            "iteration 365: weight=1.0000022354717062\n",
            "iteration 366: weight=1.000002245550569\n",
            "iteration 367: weight=1.0000022470768675\n",
            "iteration 368: weight=1.0000022408758509\n",
            "iteration 369: weight=1.000002227717595\n",
            "iteration 370: weight=1.0000022083199527\n",
            "iteration 371: weight=1.0000021833513726\n",
            "iteration 372: weight=1.0000021534335954\n",
            "iteration 373: weight=1.0000021191442274\n",
            "iteration 374: weight=1.0000020810191976\n",
            "iteration 375: weight=1.000002039555104\n",
            "iteration 376: weight=1.0000019952114498\n",
            "iteration 377: weight=1.0000019484127782\n",
            "iteration 378: weight=1.000001899550705\n",
            "iteration 379: weight=1.0000018489858558\n",
            "iteration 380: weight=1.0000017970497108\n",
            "iteration 381: weight=1.0000017440463607\n",
            "iteration 382: weight=1.000001690254176\n",
            "iteration 383: weight=1.0000016359273962\n",
            "iteration 384: weight=1.0000015812976388\n",
            "iteration 385: weight=1.0000015265753324\n",
            "iteration 386: weight=1.0000014719510786\n",
            "iteration 387: weight=1.0000014175969432\n",
            "iteration 388: weight=1.0000013636676808\n",
            "iteration 389: weight=1.0000013103018963\n",
            "iteration 390: weight=1.0000012576231443\n",
            "iteration 391: weight=1.0000012057409704\n",
            "iteration 392: weight=1.0000011547518959\n",
            "iteration 393: weight=1.0000011047403499\n",
            "iteration 394: weight=1.000001055779549\n",
            "iteration 395: weight=1.0000010079323283\n",
            "iteration 396: weight=1.0000009612519265\n",
            "iteration 397: weight=1.000000915782725\n",
            "iteration 398: weight=1.0000008715609447\n",
            "iteration 399: weight=1.000000828615303\n",
            "iteration 400: weight=1.0000007869676308\n",
            "iteration 401: weight=1.0000007466334533\n",
            "iteration 402: weight=1.000000707622535\n",
            "iteration 403: weight=1.0000006699393922\n",
            "iteration 404: weight=1.0000006335837726\n",
            "iteration 405: weight=1.0000005985511047\n",
            "iteration 406: weight=1.0000005648329189\n",
            "iteration 407: weight=1.000000532417241\n",
            "iteration 408: weight=1.0000005012889592\n",
            "iteration 409: weight=1.0000004714301673\n",
            "iteration 410: weight=1.000000442820484\n",
            "iteration 411: weight=1.0000004154373503\n",
            "iteration 412: weight=1.000000389256305\n",
            "iteration 413: weight=1.000000364251242\n",
            "iteration 414: weight=1.0000003403946478\n",
            "iteration 415: weight=1.0000003176578207\n",
            "iteration 416: weight=1.0000002960110743\n",
            "iteration 417: weight=1.0000002754239252\n",
            "iteration 418: weight=1.000000255865265\n",
            "iteration 419: weight=1.000000237303518\n",
            "iteration 420: weight=1.0000002197067872\n",
            "iteration 421: weight=1.0000002030429869\n",
            "iteration 422: weight=1.0000001872799629\n",
            "iteration 423: weight=1.000000172385603\n",
            "iteration 424: weight=1.0000001583279363\n",
            "iteration 425: weight=1.0000001450752236\n",
            "iteration 426: weight=1.000000132596038\n",
            "iteration 427: weight=1.000000120859337\n",
            "iteration 428: weight=1.000000109834528\n",
            "iteration 429: weight=1.0000000994915248\n",
            "iteration 430: weight=1.0000000898007981\n",
            "iteration 431: weight=1.0000000807334197\n",
            "iteration 432: weight=1.0000000722610998\n",
            "iteration 433: weight=1.0000000643562201\n",
            "iteration 434: weight=1.000000056991861\n",
            "iteration 435: weight=1.000000050141824\n",
            "iteration 436: weight=1.0000000437806507\n",
            "iteration 437: weight=1.0000000378836358\n",
            "iteration 438: weight=1.0000000324268392\n",
            "iteration 439: weight=1.000000027387092\n",
            "iteration 440: weight=1.0000000227420007\n",
            "iteration 441: weight=1.0000000184699493\n",
            "iteration 442: weight=1.000000014550097\n",
            "iteration 443: weight=1.0000000109623746\n",
            "iteration 444: weight=1.0000000076874795\n",
            "iteration 445: weight=1.0000000047068673\n",
            "iteration 446: weight=1.0000000020027424\n",
            "iteration 447: weight=0.999999999558047\n",
            "iteration 448: weight=0.9999999973564491\n",
            "iteration 449: weight=0.999999995382329\n",
            "iteration 450: weight=0.9999999936207644\n",
            "iteration 451: weight=0.9999999920575156\n",
            "iteration 452: weight=0.9999999906790085\n",
            "iteration 453: weight=0.9999999894723182\n",
            "iteration 454: weight=0.9999999884251516\n",
            "iteration 455: weight=0.9999999875258295\n",
            "iteration 456: weight=0.9999999867632684\n",
            "iteration 457: weight=0.9999999861269618\n",
            "iteration 458: weight=0.9999999856069622\n",
            "iteration 459: weight=0.9999999851938618\n",
            "iteration 460: weight=0.999999984878774\n",
            "iteration 461: weight=0.9999999846533145\n",
            "iteration 462: weight=0.9999999845095833\n",
            "iteration 463: weight=0.9999999844401452\n",
            "iteration 464: weight=0.9999999844380122\n",
            "iteration 465: weight=0.9999999844966253\n",
            "iteration 466: weight=0.9999999846098364\n",
            "iteration 467: weight=0.9999999847718911\n",
            "iteration 468: weight=0.9999999849774112\n",
            "iteration 469: weight=0.9999999852213779\n",
            "iteration 470: weight=0.9999999854991155\n",
            "iteration 471: weight=0.9999999858062749\n",
            "iteration 472: weight=0.9999999861388184\n",
            "iteration 473: weight=0.9999999864930037\n",
            "iteration 474: weight=0.9999999868653698\n",
            "iteration 475: weight=0.9999999872527222\n",
            "iteration 476: weight=0.9999999876521187\n",
            "iteration 477: weight=0.9999999880608562\n",
            "iteration 478: weight=0.9999999884764575\n",
            "iteration 479: weight=0.9999999888966589\n",
            "iteration 480: weight=0.9999999893193974\n",
            "iteration 481: weight=0.9999999897427995\n",
            "iteration 482: weight=0.9999999901651699\n",
            "iteration 483: weight=0.9999999905849806\n",
            "iteration 484: weight=0.99999999100086\n",
            "iteration 485: weight=0.9999999914115838\n",
            "iteration 486: weight=0.9999999918160648\n",
            "iteration 487: weight=0.9999999922133441\n",
            "iteration 488: weight=0.9999999926025821\n",
            "iteration 489: weight=0.9999999929830505\n",
            "iteration 490: weight=0.9999999933541243\n",
            "iteration 491: weight=0.9999999937152738\n",
            "iteration 492: weight=0.9999999940660583\n",
            "iteration 493: weight=0.9999999944061185\n",
            "iteration 494: weight=0.9999999947351703\n",
            "iteration 495: weight=0.999999995052999\n",
            "iteration 496: weight=0.9999999953594528\n",
            "iteration 497: weight=0.9999999956544383\n",
            "iteration 498: weight=0.9999999959379143\n",
            "iteration 499: weight=0.9999999962098878\n",
            "iteration 500: weight=0.9999999964704088\n",
            "iteration 501: weight=0.9999999967195665\n",
            "iteration 502: weight=0.9999999969574849\n",
            "iteration 503: weight=0.999999997184319\n",
            "iteration 504: weight=0.9999999974002517\n",
            "iteration 505: weight=0.99999999760549\n",
            "iteration 506: weight=0.9999999978002622\n",
            "iteration 507: weight=0.9999999979848148\n",
            "iteration 508: weight=0.9999999981594101\n",
            "iteration 509: weight=0.9999999983243237\n",
            "iteration 510: weight=0.999999998479842\n",
            "iteration 511: weight=0.9999999986262601\n",
            "iteration 512: weight=0.9999999987638801\n",
            "iteration 513: weight=0.9999999988930093\n",
            "iteration 514: weight=0.9999999990139581\n",
            "iteration 515: weight=0.999999999127039\n",
            "iteration 516: weight=0.999999999232565\n",
            "iteration 517: weight=0.9999999993308486\n",
            "iteration 518: weight=0.9999999994222004\n",
            "iteration 519: weight=0.9999999995069282\n",
            "iteration 520: weight=0.999999999585336\n",
            "iteration 521: weight=0.9999999996577236\n",
            "iteration 522: weight=0.9999999997243852\n",
            "iteration 523: weight=0.9999999997856095\n",
            "iteration 524: weight=0.9999999998416785\n",
            "iteration 525: weight=0.9999999998928676\n",
            "iteration 526: weight=0.9999999999394448\n",
            "iteration 527: weight=0.9999999999816706\n",
            "iteration 528: weight=1.0000000000197973\n",
            "iteration 529: weight=1.0000000000540696\n",
            "iteration 530: weight=1.0000000000847238\n",
            "iteration 531: weight=1.0000000001119875\n",
            "iteration 532: weight=1.0000000001360803\n",
            "iteration 533: weight=1.0000000001572127\n",
            "iteration 534: weight=1.000000000175587\n",
            "iteration 535: weight=1.0000000001913973\n",
            "iteration 536: weight=1.000000000204829\n",
            "iteration 537: weight=1.000000000216059\n",
            "iteration 538: weight=1.000000000225256\n",
            "iteration 539: weight=1.0000000002325808\n",
            "iteration 540: weight=1.0000000002381861\n",
            "iteration 541: weight=1.0000000002422167\n",
            "iteration 542: weight=1.0000000002448097\n",
            "iteration 543: weight=1.0000000002460954\n",
            "iteration 544: weight=1.000000000246196\n",
            "iteration 545: weight=1.000000000245227\n",
            "iteration 546: weight=1.0000000002432972\n",
            "iteration 547: weight=1.0000000002405087\n",
            "iteration 548: weight=1.0000000002369576\n",
            "iteration 549: weight=1.000000000232733\n",
            "iteration 550: weight=1.0000000002279188\n",
            "iteration 551: weight=1.000000000222593\n",
            "iteration 552: weight=1.0000000002168283\n",
            "iteration 553: weight=1.000000000210692\n",
            "iteration 554: weight=1.0000000002042462\n",
            "iteration 555: weight=1.0000000001975489\n",
            "iteration 556: weight=1.0000000001906528\n",
            "iteration 557: weight=1.000000000183607\n",
            "iteration 558: weight=1.0000000001764557\n",
            "iteration 559: weight=1.0000000001692402\n",
            "iteration 560: weight=1.0000000001619973\n",
            "iteration 561: weight=1.0000000001547606\n",
            "iteration 562: weight=1.0000000001475606\n",
            "iteration 563: weight=1.0000000001404243\n",
            "iteration 564: weight=1.0000000001333762\n",
            "iteration 565: weight=1.000000000126438\n",
            "iteration 566: weight=1.0000000001196285\n",
            "iteration 567: weight=1.0000000001129647\n",
            "iteration 568: weight=1.0000000001064608\n",
            "iteration 569: weight=1.0000000001001295\n",
            "iteration 570: weight=1.0000000000939808\n",
            "iteration 571: weight=1.0000000000880238\n",
            "iteration 572: weight=1.0000000000822653\n",
            "iteration 573: weight=1.0000000000767109\n",
            "iteration 574: weight=1.0000000000713647\n",
            "iteration 575: weight=1.0000000000662297\n",
            "iteration 576: weight=1.0000000000613074\n",
            "iteration 577: weight=1.0000000000565983\n",
            "iteration 578: weight=1.000000000052102\n",
            "iteration 579: weight=1.0000000000478177\n",
            "iteration 580: weight=1.000000000043743\n",
            "iteration 581: weight=1.0000000000398754\n",
            "iteration 582: weight=1.0000000000362115\n",
            "iteration 583: weight=1.0000000000327474\n",
            "iteration 584: weight=1.0000000000294786\n",
            "iteration 585: weight=1.0000000000264004\n",
            "iteration 586: weight=1.0000000000235079\n",
            "iteration 587: weight=1.0000000000207954\n",
            "iteration 588: weight=1.0000000000182574\n",
            "iteration 589: weight=1.000000000015888\n",
            "iteration 590: weight=1.000000000013681\n",
            "iteration 591: weight=1.0000000000116305\n",
            "iteration 592: weight=1.00000000000973\n",
            "iteration 593: weight=1.0000000000079734\n",
            "iteration 594: weight=1.0000000000063545\n",
            "iteration 595: weight=1.0000000000048668\n",
            "iteration 596: weight=1.000000000003504\n",
            "iteration 597: weight=1.0000000000022604\n",
            "iteration 598: weight=1.0000000000011295\n",
            "iteration 599: weight=1.0000000000001055\n",
            "iteration 600: weight=0.9999999999991822\n",
            "iteration 601: weight=0.9999999999983541\n",
            "iteration 602: weight=0.9999999999976156\n",
            "iteration 603: weight=0.9999999999969611\n",
            "iteration 604: weight=0.9999999999963854\n",
            "iteration 605: weight=0.9999999999958834\n",
            "iteration 606: weight=0.9999999999954501\n",
            "iteration 607: weight=0.9999999999950807\n",
            "iteration 608: weight=0.9999999999947706\n",
            "iteration 609: weight=0.9999999999945155\n",
            "iteration 610: weight=0.999999999994311\n",
            "iteration 611: weight=0.9999999999941531\n",
            "iteration 612: weight=0.9999999999940381\n",
            "iteration 613: weight=0.9999999999939622\n",
            "iteration 614: weight=0.9999999999939219\n",
            "iteration 615: weight=0.9999999999939139\n",
            "iteration 616: weight=0.9999999999939351\n",
            "iteration 617: weight=0.9999999999939825\n",
            "iteration 618: weight=0.9999999999940533\n",
            "iteration 619: weight=0.999999999994145\n",
            "iteration 620: weight=0.999999999994255\n",
            "iteration 621: weight=0.9999999999943812\n",
            "iteration 622: weight=0.9999999999945212\n",
            "iteration 623: weight=0.999999999994673\n",
            "iteration 624: weight=0.9999999999948349\n",
            "iteration 625: weight=0.9999999999950051\n",
            "iteration 626: weight=0.999999999995182\n",
            "iteration 627: weight=0.9999999999953642\n",
            "iteration 628: weight=0.9999999999955501\n",
            "iteration 629: weight=0.9999999999957387\n",
            "iteration 630: weight=0.9999999999959289\n",
            "iteration 631: weight=0.9999999999961195\n",
            "iteration 632: weight=0.9999999999963097\n",
            "iteration 633: weight=0.9999999999964986\n",
            "iteration 634: weight=0.9999999999966854\n",
            "iteration 635: weight=0.9999999999968695\n",
            "iteration 636: weight=0.9999999999970502\n",
            "iteration 637: weight=0.9999999999972272\n",
            "iteration 638: weight=0.9999999999973999\n",
            "iteration 639: weight=0.9999999999975678\n",
            "iteration 640: weight=0.9999999999977308\n",
            "iteration 641: weight=0.9999999999978886\n",
            "iteration 642: weight=0.9999999999980408\n",
            "iteration 643: weight=0.9999999999981873\n",
            "iteration 644: weight=0.9999999999983281\n",
            "iteration 645: weight=0.999999999998463\n",
            "iteration 646: weight=0.9999999999985919\n",
            "iteration 647: weight=0.9999999999987148\n",
            "iteration 648: weight=0.9999999999988317\n",
            "iteration 649: weight=0.9999999999989427\n",
            "iteration 650: weight=0.9999999999990479\n",
            "iteration 651: weight=0.9999999999991472\n",
            "iteration 652: weight=0.9999999999992408\n",
            "iteration 653: weight=0.9999999999993289\n",
            "iteration 654: weight=0.9999999999994115\n",
            "iteration 655: weight=0.9999999999994887\n",
            "iteration 656: weight=0.9999999999995608\n",
            "iteration 657: weight=0.9999999999996279\n",
            "iteration 658: weight=0.9999999999996901\n",
            "iteration 659: weight=0.9999999999997478\n",
            "iteration 660: weight=0.9999999999998009\n",
            "iteration 661: weight=0.9999999999998498\n",
            "iteration 662: weight=0.9999999999998945\n",
            "iteration 663: weight=0.9999999999999354\n",
            "iteration 664: weight=0.9999999999999725\n",
            "iteration 665: weight=1.000000000000006\n",
            "iteration 666: weight=1.0000000000000362\n",
            "iteration 667: weight=1.0000000000000633\n",
            "iteration 668: weight=1.0000000000000873\n",
            "iteration 669: weight=1.0000000000001086\n",
            "iteration 670: weight=1.0000000000001272\n",
            "iteration 671: weight=1.0000000000001434\n",
            "iteration 672: weight=1.0000000000001572\n",
            "iteration 673: weight=1.000000000000169\n",
            "iteration 674: weight=1.0000000000001787\n",
            "iteration 675: weight=1.0000000000001865\n",
            "iteration 676: weight=1.0000000000001927\n",
            "iteration 677: weight=1.0000000000001974\n",
            "iteration 678: weight=1.0000000000002005\n",
            "iteration 679: weight=1.0000000000002023\n",
            "iteration 680: weight=1.000000000000203\n",
            "iteration 681: weight=1.0000000000002025\n",
            "iteration 682: weight=1.0000000000002012\n",
            "iteration 683: weight=1.000000000000199\n",
            "iteration 684: weight=1.0000000000001958\n",
            "iteration 685: weight=1.000000000000192\n",
            "iteration 686: weight=1.0000000000001878\n",
            "iteration 687: weight=1.000000000000183\n",
            "iteration 688: weight=1.0000000000001776\n",
            "iteration 689: weight=1.000000000000172\n",
            "iteration 690: weight=1.000000000000166\n",
            "iteration 691: weight=1.0000000000001599\n",
            "iteration 692: weight=1.0000000000001534\n",
            "iteration 693: weight=1.000000000000147\n",
            "iteration 694: weight=1.0000000000001403\n",
            "iteration 695: weight=1.0000000000001337\n",
            "iteration 696: weight=1.000000000000127\n",
            "iteration 697: weight=1.0000000000001203\n",
            "iteration 698: weight=1.0000000000001137\n",
            "iteration 699: weight=1.000000000000107\n",
            "iteration 700: weight=1.0000000000001006\n",
            "iteration 701: weight=1.0000000000000944\n",
            "iteration 702: weight=1.0000000000000882\n",
            "iteration 703: weight=1.0000000000000822\n",
            "iteration 704: weight=1.0000000000000764\n",
            "iteration 705: weight=1.0000000000000708\n",
            "iteration 706: weight=1.0000000000000653\n",
            "iteration 707: weight=1.00000000000006\n",
            "iteration 708: weight=1.0000000000000548\n",
            "iteration 709: weight=1.00000000000005\n",
            "iteration 710: weight=1.0000000000000453\n",
            "iteration 711: weight=1.0000000000000409\n",
            "iteration 712: weight=1.0000000000000366\n",
            "iteration 713: weight=1.0000000000000326\n",
            "iteration 714: weight=1.0000000000000289\n",
            "iteration 715: weight=1.0000000000000253\n",
            "iteration 716: weight=1.000000000000022\n",
            "iteration 717: weight=1.0000000000000189\n",
            "iteration 718: weight=1.000000000000016\n",
            "iteration 719: weight=1.0000000000000133\n",
            "iteration 720: weight=1.0000000000000109\n",
            "iteration 721: weight=1.0000000000000087\n",
            "iteration 722: weight=1.0000000000000067\n",
            "iteration 723: weight=1.0000000000000047\n",
            "iteration 724: weight=1.0000000000000029\n",
            "iteration 725: weight=1.0000000000000013\n",
            "iteration 726: weight=0.9999999999999999\n",
            "iteration 727: weight=0.9999999999999986\n",
            "iteration 728: weight=0.9999999999999974\n",
            "iteration 729: weight=0.9999999999999964\n",
            "iteration 730: weight=0.9999999999999956\n",
            "iteration 731: weight=0.9999999999999948\n",
            "iteration 732: weight=0.9999999999999941\n",
            "iteration 733: weight=0.9999999999999936\n",
            "iteration 734: weight=0.999999999999993\n",
            "iteration 735: weight=0.9999999999999926\n",
            "iteration 736: weight=0.9999999999999922\n",
            "iteration 737: weight=0.999999999999992\n",
            "iteration 738: weight=0.9999999999999918\n",
            "iteration 739: weight=0.9999999999999917\n",
            "iteration 740: weight=0.9999999999999916\n",
            "converged after 741 iterations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Also implement momentum and RMSProp gradient descent and see the convergence rate (in terms of number of iterations)**"
      ],
      "metadata": {
        "id": "7XEVkHMxZUUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize w_0 and b_0 with random values using function\n",
        "import random\n",
        "w_0 = random. randint(0, 10)\n",
        "b_0 = random.uniform(0.001,0.01)\n",
        "adam = AdamOptim()\n",
        "# Initialize iteration with 1\n",
        "t = 1\n",
        "converged = False\n",
        "itr = []\n",
        "loss = []\n",
        "print(f\"Value for w_0 is {w_0}\\n\")\n",
        "print(f\"Value for b_0 is {b_0}\\n\")\n",
        "print()\n",
        "while not converged:\n",
        "    dw = grad_function(w_0)\n",
        "    db = grad_function(b_0)\n",
        "    w_0_old = w_0\n",
        "    w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n",
        "    if check_convergence(w_0, w_0_old):\n",
        "        print('converged after '+str(t)+' iterations')\n",
        "        break\n",
        "    else:\n",
        "        print('iteration '+str(t)+': weight='+str(w_0)+ ' loss= '+str(loss_function(dw)))\n",
        "        itr.append(t)\n",
        "        loss.append(loss_function(dw))\n",
        "        t+=1\n",
        "    # print(loss_function(dw))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlaYxxV5Vqn3",
        "outputId": "46b11eaf-ea08-4b7e-b2a7-a2931227054d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value for w_0 is 6\n",
            "\n",
            "Value for b_0 is 0.003339432870367982\n",
            "\n",
            "\n",
            "iteration 1: weight=5.99000000001 loss= 81\n",
            "iteration 2: weight=5.9800005268651475 loss= 80.64040000035921\n",
            "iteration 3: weight=5.970001931881262 loss= 80.281618882848\n",
            "iteration 4: weight=5.9600045654461535 loss= 79.92366908408886\n",
            "iteration 5: weight=5.950008776237629 loss= 79.56656289520214\n",
            "iteration 6: weight=5.940014910458446 loss= 79.2103124343677\n",
            "iteration 7: weight=5.9300233110939615 loss= 78.8549296203733\n",
            "iteration 8: weight=5.920034317197878 loss= 78.50042614734363\n",
            "iteration 9: weight=5.910048263211059 loss= 78.14681346082764\n",
            "iteration 10: weight=5.900065478317872 loss= 77.7941027354035\n",
            "iteration 11: weight=5.890086285843992 loss= 77.44230485393874\n",
            "iteration 12: weight=5.880111002698997 loss= 77.09143038862199\n",
            "iteration 13: weight=5.87013993886648 loss= 76.74148958385925\n",
            "iteration 14: weight=5.860173396943767 loss= 76.39249234110368\n",
            "iteration 15: weight=5.850211671732721 loss= 76.0444482056646\n",
            "iteration 16: weight=5.840255049882452 loss= 75.69736635551837\n",
            "iteration 17: weight=5.830303809584221 loss= 75.35125559212051\n",
            "iteration 18: weight=5.82035822031819 loss= 75.00612433319847\n",
            "iteration 19: weight=5.810418542651204 loss= 74.66198060748383\n",
            "iteration 20: weight=5.800485028084272 loss= 74.31883205132533\n",
            "iteration 21: weight=5.7905579189480045 loss= 73.97668590710792\n",
            "iteration 22: weight=5.780637448343889 loss= 73.63554902338973\n",
            "iteration 23: weight=5.770723840128964 loss= 73.29542785665632\n",
            "iteration 24: weight=5.760817308941204 loss= 72.95632847458353\n",
            "iteration 25: weight=5.750918060262704 loss= 72.61825656069185\n",
            "iteration 26: weight=5.741026290517641 loss= 72.28121742027051\n",
            "iteration 27: weight=5.73114218720186 loss= 71.94521598744728\n",
            "iteration 28: weight=5.721265929040908 loss= 71.61025683327738\n",
            "iteration 29: weight=5.711397686173327 loss= 71.2763441747264\n",
            "iteration 30: weight=5.701537620356072 loss= 70.9434818844242\n",
            "iteration 31: weight=5.691685885188977 loss= 70.61167350106948\n",
            "iteration 32: weight=5.6818426263552935 loss= 70.28092224036999\n",
            "iteration 33: weight=5.672007981875483 loss= 69.95123100640855\n",
            "iteration 34: weight=5.662182082371562 loss= 69.62260240333097\n",
            "iteration 35: weight=5.652365051339478 loss= 69.29503874725948\n",
            "iteration 36: weight=5.642557005427181 loss= 68.96854207834204\n",
            "iteration 37: weight=5.632758054716214 loss= 68.64311417285526\n",
            "iteration 38: weight=5.622968303004853 loss= 68.31875655528698\n",
            "iteration 39: weight=5.613187848091024 loss= 67.99547051033088\n",
            "iteration 40: weight=5.6034167820533725 loss= 67.67325709473468\n",
            "iteration 41: weight=5.593655191529091 loss= 67.35211714894902\n",
            "iteration 42: weight=5.583903157987254 loss= 67.03205130853233\n",
            "iteration 43: weight=5.574160757996582 loss= 66.71306001527306\n",
            "iteration 44: weight=5.564428063486733 loss= 66.39514352799713\n",
            "iteration 45: weight=5.554705142002342 loss= 66.07830193303404\n",
            "iteration 46: weight=5.544992056949196 loss= 65.76253515432093\n",
            "iteration 47: weight=5.535288867832034 loss= 65.44784296312835\n",
            "iteration 48: weight=5.525595630483595 loss= 65.13422498739656\n",
            "iteration 49: weight=5.515912397284642 loss= 64.82168072067446\n",
            "iteration 50: weight=5.506239217374771 loss= 64.51020953065792\n",
            "iteration 51: weight=5.496576136853922 loss= 64.19981066732649\n",
            "iteration 52: weight=5.48692319897456 loss= 63.89048327068089\n",
            "iteration 53: weight=5.477280444324579 loss= 63.58222637808617\n",
            "iteration 54: weight=5.467647911001025 loss= 63.275038931226895\n",
            "iteration 55: weight=5.458025634774797 loss= 62.968919782683194\n",
            "iteration 56: weight=5.448413649246507 loss= 62.66386770213774\n",
            "iteration 57: weight=5.438811985993733 loss= 62.35988138222446\n",
            "iteration 58: weight=5.42922067470992 loss= 62.05695944403158\n",
            "iteration 59: weight=5.419639743335193 loss= 61.75510044227151\n",
            "iteration 60: weight=5.410069218179394 loss= 61.4543028701311\n",
            "iteration 61: weight=5.4005091240376295 loss= 61.15456516381606\n",
            "iteration 62: weight=5.390959484298658 loss= 60.85588570680318\n",
            "iteration 63: weight=5.38142032104642 loss= 60.558262833814695\n",
            "iteration 64: weight=5.3718916551550535 loss= 60.26169483452837\n",
            "iteration 65: weight=5.362373506377688 loss= 59.966179957037355\n",
            "iteration 66: weight=5.352865893429352 loss= 59.67171641107311\n",
            "iteration 67: weight=5.343368834064305 loss= 59.37830237100464\n",
            "iteration 68: weight=5.33388234514808 loss= 59.08593597862725\n",
            "iteration 69: weight=5.324406442724564 loss= 58.79461534575256\n",
            "iteration 70: weight=5.314941142078375 loss= 58.50433855661263\n",
            "iteration 71: weight=5.305486457792826 loss= 58.21510367008902\n",
            "iteration 72: weight=5.296042403803749 loss= 57.92690872177837\n",
            "iteration 73: weight=5.2866089934494225 loss= 57.63975172590456\n",
            "iteration 74: weight=5.277186239516864 loss= 57.35363067708819\n",
            "iteration 75: weight=5.267774154284697 loss= 57.06854355198218\n",
            "iteration 76: weight=5.258372749562851 loss= 56.78448831078306\n",
            "iteration 77: weight=5.2489820367292666 loss= 56.501462898626514\n",
            "iteration 78: weight=5.239602026763835 loss= 56.219465246874876\n",
            "iteration 79: weight=5.230232730279755 loss= 55.93849327430474\n",
            "iteration 80: weight=5.220874157552473 loss= 55.65854488820142\n",
            "iteration 81: weight=5.211526318546391 loss= 55.37961798536729\n",
            "iteration 82: weight=5.202189222939498 loss= 55.10171045305012\n",
            "iteration 83: weight=5.192862880146056 loss= 54.82482016979745\n",
            "iteration 84: weight=5.183547299337516 loss= 54.548945006242505\n",
            "iteration 85: weight=5.174242489461751 loss= 54.274082825826824\n",
            "iteration 86: weight=5.164948459260769 loss= 54.000231485464326\n",
            "iteration 87: weight=5.155665217286989 loss= 53.72738883615154\n",
            "iteration 88: weight=5.146392771918209 loss= 53.455552723527724\n",
            "iteration 89: weight=5.137131131371349 loss= 53.184720988389444\n",
            "iteration 90: weight=5.127880303715073 loss= 52.914891467162505\n",
            "iteration 91: weight=5.118640296881374 loss= 52.64606199233508\n",
            "iteration 92: weight=5.109411118676191 loss= 52.378230392854874\n",
            "iteration 93: weight=5.1001927767891555 loss= 52.111394494493254\n",
            "iteration 94: weight=5.090985278802508 loss= 51.84555212017924\n",
            "iteration 95: weight=5.0817886321992765 loss= 51.580701090305304\n",
            "iteration 96: weight=5.0726028443707545 loss= 51.31683922300785\n",
            "iteration 97: weight=5.063427922623344 loss= 51.053964334424016\n",
            "iteration 98: weight=5.054263874184817 loss= 50.79207423892687\n",
            "iteration 99: weight=5.045110706210031 loss= 50.531166749341054\n",
            "iteration 100: weight=5.035968425786157 loss= 50.27123967713993\n",
            "iteration 101: weight=5.026837039937446 loss= 50.012290832626526\n",
            "iteration 102: weight=5.017716555629578 loss= 49.754318025098904\n",
            "iteration 103: weight=5.008606979773632 loss= 49.49731906300169\n",
            "iteration 104: weight=4.999508319229701 loss= 49.241291754065\n",
            "iteration 105: weight=4.99042058081018 loss= 48.98623390543155\n",
            "iteration 106: weight=4.981343771282761 loss= 48.73214332377309\n",
            "iteration 107: weight=4.972277897373161 loss= 48.47901781539711\n",
            "iteration 108: weight=4.963222965767588 loss= 48.226855186344714\n",
            "iteration 109: weight=4.954178983114994 loss= 47.97565324248018\n",
            "iteration 110: weight=4.945145956029106 loss= 47.72540978957332\n",
            "iteration 111: weight=4.936123891090275 loss= 47.4761226333748\n",
            "iteration 112: weight=4.927112794847142 loss= 47.22778957968548\n",
            "iteration 113: weight=4.918112673818153 loss= 46.98040843441996\n",
            "iteration 114: weight=4.909123534492916 loss= 46.73397700366513\n",
            "iteration 115: weight=4.900145383333434 loss= 46.48849309373388\n",
            "iteration 116: weight=4.891178226775208 loss= 46.24395451121465\n",
            "iteration 117: weight=4.882222071228238 loss= 46.00035906301699\n",
            "iteration 118: weight=4.873276923077905 loss= 45.75770455641373\n",
            "iteration 119: weight=4.864342788685778 loss= 45.515988799079764\n",
            "iteration 120: weight=4.85541967439032 loss= 45.275209599127976\n",
            "iteration 121: weight=4.8465075865075296 loss= 45.03536476514256\n",
            "iteration 122: weight=4.8376065313315015 loss= 44.7964521062098\n",
            "iteration 123: weight=4.828716515134926 loss= 44.55846943194679\n",
            "iteration 124: weight=4.819837544169529 loss= 44.321414552528026\n",
            "iteration 125: weight=4.810969624666457 loss= 44.085285278710266\n",
            "iteration 126: weight=4.802112762836618 loss= 43.85007942185577\n",
            "iteration 127: weight=4.793266964870967 loss= 43.61579479395393\n",
            "iteration 128: weight=4.784432236940763 loss= 43.38242920764172\n",
            "iteration 129: weight=4.775608585197779 loss= 43.149980476222815\n",
            "iteration 130: weight=4.766796015774486 loss= 42.91844641368559\n",
            "iteration 131: weight=4.757994534784197 loss= 42.68782483472022\n",
            "iteration 132: weight=4.749204148321194 loss= 42.458113554734794\n",
            "iteration 133: weight=4.74042486246082 loss= 42.229310389870626\n",
            "iteration 134: weight=4.731656683259553 loss= 42.0014131570169\n",
            "iteration 135: weight=4.722899616755057 loss= 41.77441967382454\n",
            "iteration 136: weight=4.714153668966214 loss= 41.548327758719566\n",
            "iteration 137: weight=4.70541884589314 loss= 41.32313523091589\n",
            "iteration 138: weight=4.696695153517185 loss= 41.09883991042765\n",
            "iteration 139: weight=4.687982597800914 loss= 40.87543961808103\n",
            "iteration 140: weight=4.679281184688085 loss= 40.65293217552587\n",
            "iteration 141: weight=4.6705909201036 loss= 40.431315405246686\n",
            "iteration 142: weight=4.661911809953464 loss= 40.21058713057357\n",
            "iteration 143: weight=4.653243860124719 loss= 39.990745175692766\n",
            "iteration 144: weight=4.644587076485374 loss= 39.77178736565695\n",
            "iteration 145: weight=4.635941464884333 loss= 39.553711526395325\n",
            "iteration 146: weight=4.627307031151304 loss= 39.33651548472358\n",
            "iteration 147: weight=4.618683781096715 loss= 39.12019706835353\n",
            "iteration 148: weight=4.610071720511613 loss= 38.90475410590282\n",
            "iteration 149: weight=4.60147085516756 loss= 38.69018442690425\n",
            "iteration 150: weight=4.592881190816532 loss= 38.47648586181518\n",
            "iteration 151: weight=4.584302733190803 loss= 38.263656242026755\n",
            "iteration 152: weight=4.575735488002834 loss= 38.05169339987304\n",
            "iteration 153: weight=4.567179460945147 loss= 37.84059516864012\n",
            "iteration 154: weight=4.558634657690213 loss= 37.63035938257505\n",
            "iteration 155: weight=4.550101083890319 loss= 37.42098387689491\n",
            "iteration 156: weight=4.541578745177447 loss= 37.21246648779561\n",
            "iteration 157: weight=4.533067647163139 loss= 37.00480505246085\n",
            "iteration 158: weight=4.524567795438369 loss= 36.79799740907096\n",
            "iteration 159: weight=4.516079195573407 loss= 36.59204139681166\n",
            "iteration 160: weight=4.507601853117688 loss= 36.38693485588291\n",
            "iteration 161: weight=4.4991357735996695 loss= 36.1826756275078\n",
            "iteration 162: weight=4.490680962526699 loss= 35.97926155394116\n",
            "iteration 163: weight=4.48223742538487 loss= 35.776690478478486\n",
            "iteration 164: weight=4.473805167638886 loss= 35.574960245464716\n",
            "iteration 165: weight=4.465384194731918 loss= 35.37406870030298\n",
            "iteration 166: weight=4.45697451208546 loss= 35.174013689463465\n",
            "iteration 167: weight=4.44857612509919 loss= 34.974793060492175\n",
            "iteration 168: weight=4.440189039150824 loss= 34.776404662019814\n",
            "iteration 169: weight=4.431813259595973 loss= 34.57884634377058\n",
            "iteration 170: weight=4.423448791768 loss= 34.38211595657106\n",
            "iteration 171: weight=4.415095640977872 loss= 34.18621135235912\n",
            "iteration 172: weight=4.406753812514016 loss= 33.99113038419276\n",
            "iteration 173: weight=4.398423311642177 loss= 33.79687090625907\n",
            "iteration 174: weight=4.3901041436052655 loss= 33.60343077388322\n",
            "iteration 175: weight=4.381796313623216 loss= 33.4108078435373\n",
            "iteration 176: weight=4.373499826892839 loss= 33.218999972849424\n",
            "iteration 177: weight=4.3652146885876775 loss= 33.02800502061271\n",
            "iteration 178: weight=4.356940903857854 loss= 32.83782084679433\n",
            "iteration 179: weight=4.34867847782993 loss= 32.64844531254453\n",
            "iteration 180: weight=4.340427415606755 loss= 32.45987628020578\n",
            "iteration 181: weight=4.3321877222673235 loss= 32.27211161332188\n",
            "iteration 182: weight=4.323959402866624 loss= 32.08514917664708\n",
            "iteration 183: weight=4.3157424624354945 loss= 31.898986836155274\n",
            "iteration 184: weight=4.3075369059804745 loss= 31.713622459049212\n",
            "iteration 185: weight=4.2993427384836576 loss= 31.529053913769662\n",
            "iteration 186: weight=4.291159964902545 loss= 31.345279070004736\n",
            "iteration 187: weight=4.2829885901699 loss= 31.162295798699102\n",
            "iteration 188: weight=4.274828619193598 loss= 30.980101972063395\n",
            "iteration 189: weight=4.266680056856481 loss= 30.798695463583396\n",
            "iteration 190: weight=4.258542908016212 loss= 30.618074148029528\n",
            "iteration 191: weight=4.250417177505125 loss= 30.438235901466157\n",
            "iteration 192: weight=4.2423028701300804 loss= 30.25917860126103\n",
            "iteration 193: weight=4.23419999067232 loss= 30.080900126094704\n",
            "iteration 194: weight=4.226108543887314 loss= 29.903398355970047\n",
            "iteration 195: weight=4.218028534504621 loss= 29.726671172221643\n",
            "iteration 196: weight=4.20995996722774 loss= 29.550716457525358\n",
            "iteration 197: weight=4.20190284673396 loss= 29.375532095907896\n",
            "iteration 198: weight=4.193857177674218 loss= 29.201115972756305\n",
            "iteration 199: weight=4.18582296467295 loss= 29.027465974827614\n",
            "iteration 200: weight=4.1778002123279485 loss= 28.854579990258383\n",
            "iteration 201: weight=4.169788925210209 loss= 28.682455908574426\n",
            "iteration 202: weight=4.161789107863793 loss= 28.51109162070034\n",
            "iteration 203: weight=4.153800764805676 loss= 28.34048501896931\n",
            "iteration 204: weight=4.145823900525602 loss= 28.170633997132764\n",
            "iteration 205: weight=4.13785851948594 loss= 28.001536450370036\n",
            "iteration 206: weight=4.1299046261215375 loss= 27.83319027529822\n",
            "iteration 207: weight=4.121962224839574 loss= 27.66559336998186\n",
            "iteration 208: weight=4.114031320019417 loss= 27.498743633942766\n",
            "iteration 209: weight=4.106111916012474 loss= 27.33263896816982\n",
            "iteration 210: weight=4.098204017142051 loss= 27.167277275128836\n",
            "iteration 211: weight=4.090307627703203 loss= 27.002656458772368\n",
            "iteration 212: weight=4.082422751962594 loss= 26.838774424549587\n",
            "iteration 213: weight=4.074549394158345 loss= 26.675629079416222\n",
            "iteration 214: weight=4.066687558499898 loss= 26.51321833184441\n",
            "iteration 215: weight=4.058837249167863 loss= 26.351540091832664\n",
            "iteration 216: weight=4.0509984703138775 loss= 26.19059227091582\n",
            "iteration 217: weight=4.043171226060463 loss= 26.030372782174968\n",
            "iteration 218: weight=4.035355520500876 loss= 25.87087954024751\n",
            "iteration 219: weight=4.02755135769897 loss= 25.712110461337073\n",
            "iteration 220: weight=4.019758741689044 loss= 25.55406346322362\n",
            "iteration 221: weight=4.011977676475705 loss= 25.396736465273413\n",
            "iteration 222: weight=4.00420816603372 loss= 25.24012738844912\n",
            "iteration 223: weight=3.9964502143078744 loss= 25.084234155319876\n",
            "iteration 224: weight=3.988703825212826 loss= 24.929054690071332\n",
            "iteration 225: weight=3.980969002632964 loss= 24.774586918515812\n",
            "iteration 226: weight=3.973245750422263 loss= 24.62082876810242\n",
            "iteration 227: weight=3.9655340724041417 loss= 24.46777816792713\n",
            "iteration 228: weight=3.957833972371319 loss= 24.31543304874301\n",
            "iteration 229: weight=3.9501454540856704 loss= 24.163791342970313\n",
            "iteration 230: weight=3.942468521278086 loss= 24.012850984706706\n",
            "iteration 231: weight=3.9348031776483268 loss= 23.862609909737436\n",
            "iteration 232: weight=3.9271494268648826 loss= 23.71306605554556\n",
            "iteration 233: weight=3.9195072725648297 loss= 23.564217361322108\n",
            "iteration 234: weight=3.911876718353688 loss= 23.416061767976405\n",
            "iteration 235: weight=3.9042577678052792 loss= 23.268597218146223\n",
            "iteration 236: weight=3.8966504244615843 loss= 23.121821656208095\n",
            "iteration 237: weight=3.889054691832602 loss= 22.975733028287568\n",
            "iteration 238: weight=3.8814705733962076 loss= 22.83032928226948\n",
            "iteration 239: weight=3.87389807259801 loss= 22.68560836780825\n",
            "iteration 240: weight=3.866337192851211 loss= 22.541568236338186\n",
            "iteration 241: weight=3.8587879375364653 loss= 22.398206841083802\n",
            "iteration 242: weight=3.851250310001737 loss= 22.255522137070123\n",
            "iteration 243: weight=3.84372431356216 loss= 22.11351208113306\n",
            "iteration 244: weight=3.836209951499898 loss= 21.972174631929672\n",
            "iteration 245: weight=3.8287072270640032 loss= 21.831507749948628\n",
            "iteration 246: weight=3.8212161434702745 loss= 21.691509397520477\n",
            "iteration 247: weight=3.81373670390112 loss= 21.552177538828055\n",
            "iteration 248: weight=3.806268911505416 loss= 21.413510139916877\n",
            "iteration 249: weight=3.798812769398365 loss= 21.2755051687055\n",
            "iteration 250: weight=3.79136828066136 loss= 21.138160594995917\n",
            "iteration 251: weight=3.7839354483418424 loss= 21.001474390483985\n",
            "iteration 252: weight=3.7765142754531635 loss= 20.86544452876981\n",
            "iteration 253: weight=3.7691047649744456 loss= 20.73006898536817\n",
            "iteration 254: weight=3.7617069198504427 loss= 20.595345737718937\n",
            "iteration 255: weight=3.754320742991403 loss= 20.461272765197506\n",
            "iteration 256: weight=3.746946237272929 loss= 20.327848049125244\n",
            "iteration 257: weight=3.739583405535842 loss= 20.1950695727799\n",
            "iteration 258: weight=3.73223225058604 loss= 20.062935321406076\n",
            "iteration 259: weight=3.724892775194365 loss= 19.931443282225672\n",
            "iteration 260: weight=3.717564982096461 loss= 19.80059144444833\n",
            "iteration 261: weight=3.71024887399264 loss= 19.67037779928191\n",
            "iteration 262: weight=3.702944453547744 loss= 19.540800339942933\n",
            "iteration 263: weight=3.695651723391007 loss= 19.41185706166707\n",
            "iteration 264: weight=3.688370686115921 loss= 19.283545961719597\n",
            "iteration 265: weight=3.6811013442800973 loss= 19.155865039405867\n",
            "iteration 266: weight=3.6738437004051323 loss= 19.02881229608179\n",
            "iteration 267: weight=3.666597756976471 loss= 18.902385735164316\n",
            "iteration 268: weight=3.6593635164432725 loss= 18.776583362141903\n",
            "iteration 269: weight=3.6521409812182726 loss= 18.65140318458502\n",
            "iteration 270: weight=3.6449301536776515 loss= 18.5268432121566\n",
            "iteration 271: weight=3.637731036160898 loss= 18.402901456622534\n",
            "iteration 272: weight=3.6305436309706747 loss= 18.279575931862183\n",
            "iteration 273: weight=3.6233679403726846 loss= 18.156864653878827\n",
            "iteration 274: weight=3.616203966595538 loss= 18.034765640810146\n",
            "iteration 275: weight=3.609051711830617 loss= 17.913276912938755\n",
            "iteration 276: weight=3.601911178231945 loss= 17.79239649270263\n",
            "iteration 277: weight=3.5947823679160518 loss= 17.672122404705615\n",
            "iteration 278: weight=3.5876652829618414 loss= 17.552452675727924\n",
            "iteration 279: weight=3.5805599254104603 loss= 17.433385334736585\n",
            "iteration 280: weight=3.5734662972651656 loss= 17.31491841289592\n",
            "iteration 281: weight=3.5663844004911933 loss= 17.197049943578065\n",
            "iteration 282: weight=3.5593142370156268 loss= 17.079777962373395\n",
            "iteration 283: weight=3.5522558087272658 loss= 16.96310050710101\n",
            "iteration 284: weight=3.5452091174764964 loss= 16.847015617819217\n",
            "iteration 285: weight=3.53817416507516 loss= 16.731521336835954\n",
            "iteration 286: weight=3.5311509532964243 loss= 16.6166157087193\n",
            "iteration 287: weight=3.524139483874653 loss= 16.50229678030789\n",
            "iteration 288: weight=3.517139758505277 loss= 16.38856260072139\n",
            "iteration 289: weight=3.5101517788446657 loss= 16.27541122137091\n",
            "iteration 290: weight=3.503175546509998 loss= 16.162840695969496\n",
            "iteration 291: weight=3.496211063079134 loss= 16.050849080542513\n",
            "iteration 292: weight=3.4892583300904887 loss= 15.939434433438107\n",
            "iteration 293: weight=3.482317349042903 loss= 15.828594815337599\n",
            "iteration 294: weight=3.475388121395518 loss= 15.718328289265934\n",
            "iteration 295: weight=3.4684706485676466 loss= 15.608632920602053\n",
            "iteration 296: weight=3.4615649319386503 loss= 15.499506777089326\n",
            "iteration 297: weight=3.454670972847811 loss= 15.390947928845927\n",
            "iteration 298: weight=3.447788772594205 loss= 15.28295444837523\n",
            "iteration 299: weight=3.4409183324365817 loss= 15.17552441057616\n",
            "iteration 300: weight=3.4340596535932346 loss= 15.068655892753606\n",
            "iteration 301: weight=3.4272127372418804 loss= 14.962346974628732\n",
            "iteration 302: weight=3.420377584519534 loss= 14.856595738349364\n",
            "iteration 303: weight=3.4135541965223846 loss= 14.751400268500317\n",
            "iteration 304: weight=3.406742574305674 loss= 14.646758652113714\n",
            "iteration 305: weight=3.3999427188835742 loss= 14.542668978679314\n",
            "iteration 306: weight=3.3931546312290646 loss= 14.439129340154835\n",
            "iteration 307: weight=3.386378312273811 loss= 14.336137830976224\n",
            "iteration 308: weight=3.379613762908044 loss= 14.233692548067964\n",
            "iteration 309: weight=3.372860983980439 loss= 14.131791590853345\n",
            "iteration 310: weight=3.3661199762979965 loss= 14.030433061264715\n",
            "iteration 311: weight=3.359390740625921 loss= 13.92961506375374\n",
            "iteration 312: weight=3.3526732776875026 loss= 13.829335705301641\n",
            "iteration 313: weight=3.3459675881639988 loss= 13.729593095429415\n",
            "iteration 314: weight=3.339273672694515 loss= 13.630385346208042\n",
            "iteration 315: weight=3.3325915318758885 loss= 13.53171057226868\n",
            "iteration 316: weight=3.3259211662625696 loss= 13.433566890812864\n",
            "iteration 317: weight=3.3192625763665053 loss= 13.335952421622649\n",
            "iteration 318: weight=3.3126157626570234 loss= 13.238865287070778\n",
            "iteration 319: weight=3.3059807255607168 loss= 13.142303612130812\n",
            "iteration 320: weight=3.299357465461328 loss= 13.04626552438725\n",
            "iteration 321: weight=3.2927459826996337 loss= 12.950749154045653\n",
            "iteration 322: weight=3.286146277573333 loss= 12.8557526339427\n",
            "iteration 323: weight=3.279558350336931 loss= 12.761274099556298\n",
            "iteration 324: weight=3.2729822012016276 loss= 12.6673116890156\n",
            "iteration 325: weight=3.2664178303352034 loss= 12.573863543111074\n",
            "iteration 326: weight=3.259865237861909 loss= 12.480927805304509\n",
            "iteration 327: weight=3.2533244238623524 loss= 12.388502621739013\n",
            "iteration 328: weight=3.2467953883733895 loss= 12.296586141249001\n",
            "iteration 329: weight=3.240278131388013 loss= 12.205176515370164\n",
            "iteration 330: weight=3.2337726528552415 loss= 12.114271898349417\n",
            "iteration 331: weight=3.2272789526800123 loss= 12.023870447154808\n",
            "iteration 332: weight=3.220797030723072 loss= 11.93397032148544\n",
            "iteration 333: weight=3.214326886800868 loss= 11.84456968378137\n",
            "iteration 334: weight=3.207868520685441 loss= 11.755666699233426\n",
            "iteration 335: weight=3.2014219321043185 loss= 11.667259535793104\n",
            "iteration 336: weight=3.1949871207404095 loss= 11.579346364182369\n",
            "iteration 337: weight=3.1885640862318967 loss= 11.491925357903453\n",
            "iteration 338: weight=3.1821528281721334 loss= 11.40499469324864\n",
            "iteration 339: weight=3.1757533461095377 loss= 11.318552549310029\n",
            "iteration 340: weight=3.1693656395474896 loss= 11.23259710798925\n",
            "iteration 341: weight=3.1629897079442273 loss= 11.147126554007198\n",
            "iteration 342: weight=3.1566255507127448 loss= 11.062139074913706\n",
            "iteration 343: weight=3.15027316722069 loss= 10.97763286109722\n",
            "iteration 344: weight=3.143932556790264 loss= 10.89360610579443\n",
            "iteration 345: weight=3.1376037186981187 loss= 10.8100570050999\n",
            "iteration 346: weight=3.131286652175259 loss= 10.72698375797563\n",
            "iteration 347: weight=3.1249813564069417 loss= 10.644384566260658\n",
            "iteration 348: weight=3.118687830532578 loss= 10.562257634680575\n",
            "iteration 349: weight=3.1124060736456345 loss= 10.480601170857055\n",
            "iteration 350: weight=3.106136084793537 loss= 10.399413385317324\n",
            "iteration 351: weight=3.0998778629775727 loss= 10.318692491503647\n",
            "iteration 352: weight=3.0936314071527957 loss= 10.23843670578274\n",
            "iteration 353: weight=3.08739671622793 loss= 10.158644247455198\n",
            "iteration 354: weight=3.0811737890652755 loss= 10.07931333876486\n",
            "iteration 355: weight=3.0749626244806154 loss= 10.000442204908163\n",
            "iteration 356: weight=3.068763221243121 loss= 9.922029074043474\n",
            "iteration 357: weight=3.0625755780752595 loss= 9.84407217730037\n",
            "iteration 358: weight=3.0563996936527045 loss= 9.766569748788925\n",
            "iteration 359: weight=3.0502355666042416 loss= 9.68952002560893\n",
            "iteration 360: weight=3.044083195511681 loss= 9.612921247859095\n",
            "iteration 361: weight=3.037942578909766 loss= 9.53677165864626\n",
            "iteration 362: weight=3.0318137152860847 loss= 9.461069504094487\n",
            "iteration 363: weight=3.0256966030809824 loss= 9.385813033354234\n",
            "iteration 364: weight=3.019591240687473 loss= 9.311000498611396\n",
            "iteration 365: weight=3.0134976264511537 loss= 9.236630155096377\n",
            "iteration 366: weight=3.007415758670118 loss= 9.162700261093105\n",
            "iteration 367: weight=3.001345635594871 loss= 9.089209077948027\n",
            "iteration 368: weight=2.995287255428246 loss= 9.016154870079069\n",
            "iteration 369: weight=2.9892406163253185 loss= 8.943535904984545\n",
            "iteration 370: weight=2.983205716393326 loss= 8.871350453252058\n",
            "iteration 371: weight=2.9771825536915837 loss= 8.799596788567356\n",
            "iteration 372: weight=2.971171126231406 loss= 8.728273187723154\n",
            "iteration 373: weight=2.965171431976022 loss= 8.657377930627932\n",
            "iteration 374: weight=2.9591834688405 loss= 8.586909300314666\n",
            "iteration 375: weight=2.9532072346916665 loss= 8.51686558294958\n",
            "iteration 376: weight=2.9472427273480273 loss= 8.4472450678408\n",
            "iteration 377: weight=2.9412899445796916 loss= 8.378046047447025\n",
            "iteration 378: weight=2.935348884108296 loss= 8.309266817386122\n",
            "iteration 379: weight=2.9294195436069277 loss= 8.240905676443724\n",
            "iteration 380: weight=2.923501920700049 loss= 8.17296092658175\n",
            "iteration 381: weight=2.9175960129634255 loss= 8.105430872946915\n",
            "iteration 382: weight=2.911701817924051 loss= 8.038313823879202\n",
            "iteration 383: weight=2.9058193330600766 loss= 7.971608090920283\n",
            "iteration 384: weight=2.8999485558007376 loss= 7.905311988821914\n",
            "iteration 385: weight=2.8940894835262845 loss= 7.839423835554284\n",
            "iteration 386: weight=2.8882421135679115 loss= 7.773941952314331\n",
            "iteration 387: weight=2.8824064432076897 loss= 7.708864663534008\n",
            "iteration 388: weight=2.8765824696784965 loss= 7.644190296888542\n",
            "iteration 389: weight=2.8707701901639506 loss= 7.579917183304595\n",
            "iteration 390: weight=2.8649696017983444 loss= 7.516043656968453\n",
            "iteration 391: weight=2.8591807016665785 loss= 7.4525680553341225\n",
            "iteration 392: weight=2.853403486804098 loss= 7.3894887191314105\n",
            "iteration 393: weight=2.847637954196828 loss= 7.3268039923739625\n",
            "iteration 394: weight=2.8418841007811104 loss= 7.264512222367248\n",
            "iteration 395: weight=2.8361419234436434 loss= 7.2026117597165165\n",
            "iteration 396: weight=2.8304114190214187 loss= 7.141100958334716\n",
            "iteration 397: weight=2.824692584301663 loss= 7.079978175450339\n",
            "iteration 398: weight=2.818985416021777 loss= 7.019241771615272\n",
            "iteration 399: weight=2.8132899108692793 loss= 6.958890110712561\n",
            "iteration 400: weight=2.8076060654817465 loss= 6.898921559964158\n",
            "iteration 401: weight=2.8019338764467587 loss= 6.839334489938615\n",
            "iteration 402: weight=2.796273340301843 loss= 6.780127274558737\n",
            "iteration 403: weight=2.7906244535344182 loss= 6.721298291109189\n",
            "iteration 404: weight=2.784987212581743 loss= 6.662845920244063\n",
            "iteration 405: weight=2.7793616138308614 loss= 6.604768545994389\n",
            "iteration 406: weight=2.773747653618552 loss= 6.547064555775625\n",
            "iteration 407: weight=2.7681453282312756 loss= 6.489732340395067\n",
            "iteration 408: weight=2.762554633905127 loss= 6.432770294059239\n",
            "iteration 409: weight=2.7569755668257847 loss= 6.376176814381235\n",
            "iteration 410: weight=2.7514081231284644 loss= 6.31995030238801\n",
            "iteration 411: weight=2.7458522988978697 loss= 6.264089162527624\n",
            "iteration 412: weight=2.7403080901681474 loss= 6.208591802676429\n",
            "iteration 413: weight=2.7347754929228425 loss= 6.153456634146229\n",
            "iteration 414: weight=2.729254503094853 loss= 6.098682071691394\n",
            "iteration 415: weight=2.7237451165663877 loss= 6.044266533515895\n",
            "iteration 416: weight=2.7182473291689235 loss= 5.990208441280327\n",
            "iteration 417: weight=2.7127611366831634 loss= 5.936506220108862\n",
            "iteration 418: weight=2.7072865348389983 loss= 5.883158298596154\n",
            "iteration 419: weight=2.701823519315465 loss= 5.830163108814224\n",
            "iteration 420: weight=2.6963720857407103 loss= 5.77751908631924\n",
            "iteration 421: weight=2.6909322296919522 loss= 5.72522467015831\n",
            "iteration 422: weight=2.6855039466954445 loss= 5.673278302876179\n",
            "iteration 423: weight=2.6800872322264406 loss= 5.6216784305219\n",
            "iteration 424: weight=2.67468208170916 loss= 5.570423502655444\n",
            "iteration 425: weight=2.6692884905167555 loss= 5.519511972354264\n",
            "iteration 426: weight=2.6639064539712796 loss= 5.4689422962198115\n",
            "iteration 427: weight=2.6585359673436537 loss= 5.4187129343839935\n",
            "iteration 428: weight=2.653177025853639 loss= 5.3688223505155825\n",
            "iteration 429: weight=2.647829624669806 loss= 5.319269011826579\n",
            "iteration 430: weight=2.6424937589095063 loss= 5.27005138907851\n",
            "iteration 431: weight=2.637169423638847 loss= 5.221167956588692\n",
            "iteration 432: weight=2.6318566138726633 loss= 5.172617192236432\n",
            "iteration 433: weight=2.6265553245744933 loss= 5.124397577469166\n",
            "iteration 434: weight=2.621265550656555 loss= 5.076507597308568\n",
            "iteration 435: weight=2.6159872869797236 loss= 5.028945740356592\n",
            "iteration 436: weight=2.6107205283535073 loss= 4.981710498801457\n",
            "iteration 437: weight=2.605465269536029 loss= 4.934800368423577\n",
            "iteration 438: weight=2.600221505234006 loss= 4.888213848601463\n",
            "iteration 439: weight=2.5949892301027297 loss= 4.841949442317528\n",
            "iteration 440: weight=2.5897684387460496 loss= 4.796005656163876\n",
            "iteration 441: weight=2.5845591257163565 loss= 4.750381000348009\n",
            "iteration 442: weight=2.5793612855145662 loss= 4.70507398869851\n",
            "iteration 443: weight=2.5741749125901054 loss= 4.660083138670627\n",
            "iteration 444: weight=2.5690000013408985 loss= 4.6154069713518435\n",
            "iteration 445: weight=2.563836546113355 loss= 4.571044011467363\n",
            "iteration 446: weight=2.5586845412023593 loss= 4.526992787385572\n",
            "iteration 447: weight=2.5535439808512588 loss= 4.4832518311234\n",
            "iteration 448: weight=2.5484148592518556 loss= 4.43981967835167\n",
            "iteration 449: weight=2.543297170544399 loss= 4.396694868400353\n",
            "iteration 450: weight=2.5381909088175783 loss= 4.353875944263795\n",
            "iteration 451: weight=2.533096068108517 loss= 4.311361452605876\n",
            "iteration 452: weight=2.528012642402768 loss= 4.269149943765111\n",
            "iteration 453: weight=2.522940625634309 loss= 4.227239971759683\n",
            "iteration 454: weight=2.5178800116855427 loss= 4.185630094292446\n",
            "iteration 455: weight=2.5128307943872925 loss= 4.144318872755841\n",
            "iteration 456: weight=2.507792967518804 loss= 4.103304872236777\n",
            "iteration 457: weight=2.502766524807745 loss= 4.062586661521429\n",
            "iteration 458: weight=2.4977514599302078 loss= 4.0221628131000084\n",
            "iteration 459: weight=2.492747766510712 loss= 3.9820319031714444\n",
            "iteration 460: weight=2.4877554381222087 loss= 3.9421925116480274\n",
            "iteration 461: weight=2.4827744682860855 loss= 3.902643222159986\n",
            "iteration 462: weight=2.477804850472174 loss= 3.8633826220599925\n",
            "iteration 463: weight=2.472846578098755 loss= 3.8244093024276413\n",
            "iteration 464: weight=2.46789964453257 loss= 3.7857218580738294\n",
            "iteration 465: weight=2.4629640430888284 loss= 3.7473188875451022\n",
            "iteration 466: weight=2.4580397670312197 loss= 3.7091989931279326\n",
            "iteration 467: weight=2.453126809571926 loss= 3.6713607808529343\n",
            "iteration 468: weight=2.4482251638716335 loss= 3.633802860499035\n",
            "iteration 469: weight=2.4433348230395477 loss= 3.5965238455975452\n",
            "iteration 470: weight=2.438455780133409 loss= 3.55952235343622\n",
            "iteration 471: weight=2.433588028159508 loss= 3.52279700506322\n",
            "iteration 472: weight=2.4287315600727055 loss= 3.486346425291032\n",
            "iteration 473: weight=2.423886368776449 loss= 3.4501692427003263\n",
            "iteration 474: weight=2.419052447122793 loss= 3.41426408964373\n",
            "iteration 475: weight=2.4142297879124217 loss= 3.3786296022495783\n",
            "iteration 476: weight=2.409418383894669 loss= 3.343264420425566\n",
            "iteration 477: weight=2.404618227767543 loss= 3.3081671878623666\n",
            "iteration 478: weight=2.399829312177751 loss= 3.2733365520371622\n",
            "iteration 479: weight=2.395051629720724 loss= 3.2387711642171393\n",
            "iteration 480: weight=2.390285172940644 loss= 3.204469679462896\n",
            "iteration 481: weight=2.385529934330472 loss= 3.1704307566318093\n",
            "iteration 482: weight=2.3807859063319774 loss= 3.136653058381321\n",
            "iteration 483: weight=2.3760530813357668 loss= 3.103135251172171\n",
            "iteration 484: weight=2.371331451681317 loss= 3.069876005271566\n",
            "iteration 485: weight=2.366621009657007 loss= 3.0368739947562844\n",
            "iteration 486: weight=2.361921747500152 loss= 3.004127897515721\n",
            "iteration 487: weight=2.357233657397037 loss= 2.9716363952548646\n",
            "iteration 488: weight=2.3525567314829545 loss= 2.939398173497204\n",
            "iteration 489: weight=2.34789096184224 loss= 2.9074119215875944\n",
            "iteration 490: weight=2.343236340508312 loss= 2.8756763326950363\n",
            "iteration 491: weight=2.338592859463709 loss= 2.8441901038153983\n",
            "iteration 492: weight=2.333960510640133 loss= 2.8129519357740795\n",
            "iteration 493: weight=2.3293392859184903 loss= 2.781960533228606\n",
            "iteration 494: weight=2.324729177128933 loss= 2.7512146046711656\n",
            "iteration 495: weight=2.320130176050904 loss= 2.720712862431067\n",
            "iteration 496: weight=2.315542274413183 loss= 2.6904540226771463\n",
            "iteration 497: weight=2.3109654638939325 loss= 2.66043680542011\n",
            "iteration 498: weight=2.3063997361207447 loss= 2.6306599345148047\n",
            "iteration 499: weight=2.301845082670691 loss= 2.6011221376624265\n",
            "iteration 500: weight=2.297301495070371 loss= 2.5718221464126687\n",
            "iteration 501: weight=2.292768964795964 loss= 2.5427586961657953\n",
            "iteration 502: weight=2.2882474832732824 loss= 2.5139305261746587\n",
            "iteration 503: weight=2.283737041877823 loss= 2.4853363795466548\n",
            "iteration 504: weight=2.279237631934822 loss= 2.456975003245601\n",
            "iteration 505: weight=2.2747492447193123 loss= 2.428845148093556\n",
            "iteration 506: weight=2.2702718714561785 loss= 2.4009455687725794\n",
            "iteration 507: weight=2.2658055033202156 loss= 2.373275023826414\n",
            "iteration 508: weight=2.261350131436188 loss= 2.345832275662115\n",
            "iteration 509: weight=2.25690574687889 loss= 2.3186160905516022\n",
            "iteration 510: weight=2.2524723406732075 loss= 2.291625238633161\n",
            "iteration 511: weight=2.2480499037941795 loss= 2.264858493912862\n",
            "iteration 512: weight=2.243638427167063 loss= 2.238314634265925\n",
            "iteration 513: weight=2.2392379016673973 loss= 2.211992441438012\n",
            "iteration 514: weight=2.2348483181210717 loss= 2.1858907010464668\n",
            "iteration 515: weight=2.23046966730439 loss= 2.160008202581471\n",
            "iteration 516: weight=2.22610193994414 loss= 2.134343739407144\n",
            "iteration 517: weight=2.221745126717665 loss= 2.1088961087625746\n",
            "iteration 518: weight=2.2173992182529316 loss= 2.0836641117627934\n",
            "iteration 519: weight=2.213064205128603 loss= 2.0586465533996696\n",
            "iteration 520: weight=2.208740077874112 loss= 2.0338422425427467\n",
            "iteration 521: weight=2.204426826969735 loss= 2.0092499919400106\n",
            "iteration 522: weight=2.2001244428466675 loss= 1.9848686182185968\n",
            "iteration 523: weight=2.1958329158870993 loss= 1.960696941885426\n",
            "iteration 524: weight=2.191552236424294 loss= 1.9367337873277721\n",
            "iteration 525: weight=2.187282394742667 loss= 1.9129779828137714\n",
            "iteration 526: weight=2.1830233810778648 loss= 1.889428360492862\n",
            "iteration 527: weight=2.1787751856168462 loss= 1.8660837563961525\n",
            "iteration 528: weight=2.1745377984979655 loss= 1.842943010436736\n",
            "iteration 529: weight=2.1703112098110537 loss= 1.8200049664099271\n",
            "iteration 530: weight=2.166095409597506 loss= 1.7972684719934344\n",
            "iteration 531: weight=2.1618903878503644 loss= 1.7747323787474771\n",
            "iteration 532: weight=2.157696134514406 loss= 1.7523955421148232\n",
            "iteration 533: weight=2.1535126394862303 loss= 1.7302568214207659\n",
            "iteration 534: weight=2.1493398926143485 loss= 1.7083150798730387\n",
            "iteration 535: weight=2.1451778836992736 loss= 1.6865691845616544\n",
            "iteration 536: weight=2.1410266024936115 loss= 1.665018006458694\n",
            "iteration 537: weight=2.1368860387021535 loss= 1.6436604204180103\n",
            "iteration 538: weight=2.132756181981969 loss= 1.6224953051748843\n",
            "iteration 539: weight=2.128637021942502 loss= 1.6015215433455952\n",
            "iteration 540: weight=2.1245285481456637 loss= 1.5807380214269502\n",
            "iteration 541: weight=2.1204307501059327 loss= 1.5601436297957223\n",
            "iteration 542: weight=2.1163436172904513 loss= 1.5397372627080417\n",
            "iteration 543: weight=2.1122671391191243 loss= 1.5195178182987128\n",
            "iteration 544: weight=2.1082013049647204 loss= 1.4994841985804683\n",
            "iteration 545: weight=2.1041461041529717 loss= 1.4796353094431547\n",
            "iteration 546: weight=2.1001015259626774 loss= 1.4599700606528527\n",
            "iteration 547: weight=2.0960675596258076 loss= 1.4404873658509363\n",
            "iteration 548: weight=2.0920441943276065 loss= 1.4211861425530632\n",
            "iteration 549: weight=2.0880314192066995 loss= 1.4020653121480988\n",
            "iteration 550: weight=2.0840292233551994 loss= 1.383123799896981\n",
            "iteration 551: weight=2.0800375958188138 loss= 1.3643605349315093\n",
            "iteration 552: weight=2.076056525596955 loss= 1.345774450253078\n",
            "iteration 553: weight=2.0720860016428504 loss= 1.3273644827313422\n",
            "iteration 554: weight=2.068126012863651 loss= 1.3091295731028136\n",
            "iteration 555: weight=2.064176548120547 loss= 1.291068665969397\n",
            "iteration 556: weight=2.0602375962288795 loss= 1.273180709796863\n",
            "iteration 557: weight=2.0563091459582545 loss= 1.2554646569132517\n",
            "iteration 558: weight=2.0523911860326596 loss= 1.2379194635072102\n",
            "iteration 559: weight=2.0484837051305793 loss= 1.220544089626273\n",
            "iteration 560: weight=2.0445866918851134 loss= 1.2033374991750732\n",
            "iteration 561: weight=2.040700134884095 loss= 1.1862986599134855\n",
            "iteration 562: weight=2.0368240226702117 loss= 1.1694265434547146\n",
            "iteration 563: weight=2.032958343741124 loss= 1.1527201252633121\n",
            "iteration 564: weight=2.0291030865495894 loss= 1.1361783846531273\n",
            "iteration 565: weight=2.025258239503584 loss= 1.1198003047852092\n",
            "iteration 566: weight=2.0214237909664274 loss= 1.1035848726656177\n",
            "iteration 567: weight=2.017599729256906 loss= 1.0875310791432025\n",
            "iteration 568: weight=2.013786042649401 loss= 1.0716379189072898\n",
            "iteration 569: weight=2.009982719374014 loss= 1.0559043904853285\n",
            "iteration 570: weight=2.0061897476166966 loss= 1.0403294962404575\n",
            "iteration 571: weight=2.0024071155193774 loss= 1.0249122423690196\n",
            "iteration 572: weight=1.998634811180094 loss= 1.0096516388980046\n",
            "iteration 573: weight=1.9948728226531232 loss= 0.9945466996824321\n",
            "iteration 574: weight=1.9911211379491138 loss= 0.979596442402678\n",
            "iteration 575: weight=1.9873797450352195 loss= 0.9647998885617302\n",
            "iteration 576: weight=1.9836486318352329 loss= 0.9501560634823822\n",
            "iteration 577: weight=1.9799277862297209 loss= 0.9356639963043705\n",
            "iteration 578: weight=1.976217196056161 loss= 0.9213227199814424\n",
            "iteration 579: weight=1.9725168491090785 loss= 0.9071312712783683\n",
            "iteration 580: weight=1.968826733140185 loss= 0.893088690767887\n",
            "iteration 581: weight=1.9651468358585167 loss= 0.8791940228275927\n",
            "iteration 582: weight=1.9614771449305755 loss= 0.8654463156367593\n",
            "iteration 583: weight=1.9578176479804699 loss= 0.8518446211731012\n",
            "iteration 584: weight=1.954168332590057 loss= 0.8383879952094775\n",
            "iteration 585: weight=1.9505291862990857 loss= 0.8250754973105305\n",
            "iteration 586: weight=1.9469001966053407 loss= 0.8119061908292649\n",
            "iteration 587: weight=1.943281350964788 loss= 0.7988791429035689\n",
            "iteration 588: weight=1.9396726367917199 loss= 0.7859934244526698\n",
            "iteration 589: weight=1.9360740414589033 loss= 0.7732481101735345\n",
            "iteration 590: weight=1.9324855522977267 loss= 0.7606422785372051\n",
            "iteration 591: weight=1.9289071565983493 loss= 0.7481750117850785\n",
            "iteration 592: weight=1.925338841609851 loss= 0.7358453959251237\n",
            "iteration 593: weight=1.9217805945403823 loss= 0.7236525207280393\n",
            "iteration 594: weight=1.9182324025573174 loss= 0.7115954797233535\n",
            "iteration 595: weight=1.9146942527874056 loss= 0.6996733701954638\n",
            "iteration 596: weight=1.9111661323169253 loss= 0.6878852931796184\n",
            "iteration 597: weight=1.9076480281918382 loss= 0.6762303534578376\n",
            "iteration 598: weight=1.9041399274179451 loss= 0.6647076595547747\n",
            "iteration 599: weight=1.9006418169610417 loss= 0.6533163237335278\n",
            "iteration 600: weight=1.897153683747076 loss= 0.6420554619913794\n",
            "iteration 601: weight=1.8936755146623059 loss= 0.6309241940554897\n",
            "iteration 602: weight=1.890207296553459 loss= 0.6199216433785257\n",
            "iteration 603: weight=1.8867490162278917 loss= 0.6090469371342366\n",
            "iteration 604: weight=1.88330066045375 loss= 0.598299206212968\n",
            "iteration 605: weight=1.8798622159601317 loss= 0.5876775852171243\n",
            "iteration 606: weight=1.8764336694372474 loss= 0.577181212456567\n",
            "iteration 607: weight=1.873015007536586 loss= 0.5668092299439635\n",
            "iteration 608: weight=1.8696062168710765 loss= 0.556560783390077\n",
            "iteration 609: weight=1.866207284015255 loss= 0.5464350221989971\n",
            "iteration 610: weight=1.8628181955054295 loss= 0.5364310994633188\n",
            "iteration 611: weight=1.859438937839847 loss= 0.5265481719592642\n",
            "iteration 612: weight=1.8560694974788607 loss= 0.5167854001417496\n",
            "iteration 613: weight=1.8527098608450991 loss= 0.5071419481393935\n",
            "iteration 614: weight=1.8493600143236344 loss= 0.4976169837494768\n",
            "iteration 615: weight=1.846019944262153 loss= 0.48820967843284\n",
            "iteration 616: weight=1.8426896369711263 loss= 0.47891920730873405\n",
            "iteration 617: weight=1.8393690787239823 loss= 0.4697447491496094\n",
            "iteration 618: weight=1.8360582557572782 loss= 0.46068548637585804\n",
            "iteration 619: weight=1.8327571542708734 loss= 0.45174060505049685\n",
            "iteration 620: weight=1.829465760428104 loss= 0.4429092948737994\n",
            "iteration 621: weight=1.8261840603559572 loss= 0.4341907491778749\n",
            "iteration 622: weight=1.822912040145247 loss= 0.42558416492119466\n",
            "iteration 623: weight=1.819649685850791 loss= 0.4170887426830623\n",
            "iteration 624: weight=1.816396983491587 loss= 0.40870368665803714\n",
            "iteration 625: weight=1.8131539190509913 loss= 0.40042820465030227\n",
            "iteration 626: weight=1.8099204784768972 loss= 0.3922615080679792\n",
            "iteration 627: weight=1.8066966476819146 loss= 0.3842028119173957\n",
            "iteration 628: weight=1.8034824125435502 loss= 0.3762513347972978\n",
            "iteration 629: weight=1.8002777589043883 loss= 0.36840629889301457\n",
            "iteration 630: weight=1.7970826725722722 loss= 0.3606669299705678\n",
            "iteration 631: weight=1.793897139320487 loss= 0.3530324573707353\n",
            "iteration 632: weight=1.7907211448879428 loss= 0.3455021140030632\n",
            "iteration 633: weight=1.7875546749793576 loss= 0.33807513633982467\n",
            "iteration 634: weight=1.784397715265443 loss= 0.3307507644099359\n",
            "iteration 635: weight=1.781250251383089 loss= 0.32352824179281603\n",
            "iteration 636: weight=1.7781122689355493 loss= 0.31640681561220285\n",
            "iteration 637: weight=1.7749837534926285 loss= 0.3093857365299173\n",
            "iteration 638: weight=1.7718646905908693 loss= 0.3024642587395787\n",
            "iteration 639: weight=1.7687550657337405 loss= 0.29564163996027615\n",
            "iteration 640: weight=1.7656548643918255 loss= 0.288917141430189\n",
            "iteration 641: weight=1.7625640720030114 loss= 0.282290027900157\n",
            "iteration 642: weight=1.7594826739726792 loss= 0.27575956762721\n",
            "iteration 643: weight=1.7564106556738948 loss= 0.2693250323680467\n",
            "iteration 644: weight=1.753348002447599 loss= 0.26298569737246646\n",
            "iteration 645: weight=1.7502946996028006 loss= 0.25674084137675424\n",
            "iteration 646: weight=1.7472507324167683 loss= 0.2505897465970248\n",
            "iteration 647: weight=1.7442160861352238 loss= 0.24453169872251346\n",
            "iteration 648: weight=1.7411907459725358 loss= 0.23856598690882835\n",
            "iteration 649: weight=1.738174697111914 loss= 0.232691903771153\n",
            "iteration 650: weight=1.7351679247056044 loss= 0.22690874537740813\n",
            "iteration 651: weight=1.7321704138750846 loss= 0.22121581124136336\n",
            "iteration 652: weight=1.72918214971126 loss= 0.21561240431571216\n",
            "iteration 653: weight=1.7262031172746604 loss= 0.21009783098509782\n",
            "iteration 654: weight=1.7232333015956374 loss= 0.2046714010590951\n",
            "iteration 655: weight=1.7202726876745622 loss= 0.19933242776515536\n",
            "iteration 656: weight=1.717321260482024 loss= 0.1940802277415008\n",
            "iteration 657: weight=1.714379004959029 loss= 0.18891412102998295\n",
            "iteration 658: weight=1.7114459060172 loss= 0.18383343106889383\n",
            "iteration 659: weight=1.7085219485389762 loss= 0.17883748468573835\n",
            "iteration 660: weight=1.705607117377814 loss= 0.17392561208996593\n",
            "iteration 661: weight=1.702701397358388 loss= 0.1690971468656568\n",
            "iteration 662: weight=1.6998047732767925 loss= 0.16435142596417252\n",
            "iteration 663: weight=1.696917229900744 loss= 0.15968778969676167\n",
            "iteration 664: weight=1.6940387519697837 loss= 0.15510558172712985\n",
            "iteration 665: weight=1.6911693241954806 loss= 0.1506041490639649\n",
            "iteration 666: weight=1.6883089312616348 loss= 0.14618284205342702\n",
            "iteration 667: weight=1.6854575578244824 loss= 0.14184101437159646\n",
            "iteration 668: weight=1.682615188512899 loss= 0.13757802301688482\n",
            "iteration 669: weight=1.6797818079286064 loss= 0.13339322830240663\n",
            "iteration 670: weight=1.6769574006463763 loss= 0.1292859938483133\n",
            "iteration 671: weight=1.6741419512142375 loss= 0.1252556865740886\n",
            "iteration 672: weight=1.6713354441536818 loss= 0.12130167669080749\n",
            "iteration 673: weight=1.6685378639598714 loss= 0.11742333769335778\n",
            "iteration 674: weight=1.6657491951018455 loss= 0.11362004635262446\n",
            "iteration 675: weight=1.6629694220227285 loss= 0.10989118270763854\n",
            "iteration 676: weight=1.6601985291399382 loss= 0.10623613005768862\n",
            "iteration 677: weight=1.657436500845394 loss= 0.10265427495439838\n",
            "iteration 678: weight=1.6546833215057264 loss= 0.09914500719376695\n",
            "iteration 679: weight=1.6519389754624858 loss= 0.09570771980817572\n",
            "iteration 680: weight=1.6492034470323529 loss= 0.09234180905835943\n",
            "iteration 681: weight=1.646476720507348 loss= 0.08904667442534464\n",
            "iteration 682: weight=1.6437587801550426 loss= 0.0858217186023511\n",
            "iteration 683: weight=1.6410496102187693 loss= 0.08266634748666357\n",
            "iteration 684: weight=1.6383491949178335 loss= 0.07957997017146701\n",
            "iteration 685: weight=1.6356575184477251 loss= 0.0765619989376507\n",
            "iteration 686: weight=1.6329745649803304 loss= 0.07361184924557951\n",
            "iteration 687: weight=1.6303003186641447 loss= 0.07072893972683247\n",
            "iteration 688: weight=1.627634763624484 loss= 0.06791269217591056\n",
            "iteration 689: weight=1.6249778839636984 loss= 0.06516253154191154\n",
            "iteration 690: weight=1.6223296637613864 loss= 0.06247788592017467\n",
            "iteration 691: weight=1.619690087074607 loss= 0.05985818654389541\n",
            "iteration 692: weight=1.617059137938094 loss= 0.05730286777570792\n",
            "iteration 693: weight=1.6144368003644713 loss= 0.05481136709923895\n",
            "iteration 694: weight=1.6118230583444657 loss= 0.05238312511063148\n",
            "iteration 695: weight=1.6092178958471226 loss= 0.0500175855100391\n",
            "iteration 696: weight=1.6066212968200209 loss= 0.047714195093091716\n",
            "iteration 697: weight=1.604033245189488 loss= 0.04547240374233197\n",
            "iteration 698: weight=1.6014537248608165 loss= 0.04329166441862453\n",
            "iteration 699: weight=1.598882719718478 loss= 0.04117143315253702\n",
            "iteration 700: weight=1.5963202136263412 loss= 0.03911116903569245\n",
            "iteration 701: weight=1.5937661904278868 loss= 0.037110334212095974\n",
            "iteration 702: weight=1.5912206339464248 loss= 0.03516839386943493\n",
            "iteration 703: weight=1.5886835279853104 loss= 0.03328481623035051\n",
            "iteration 704: weight=1.5861548563281624 loss= 0.03145907254368541\n",
            "iteration 705: weight=1.5836346027390789 loss= 0.029690637075705295\n",
            "iteration 706: weight=1.5811227509628552 loss= 0.027978987101294095\n",
            "iteration 707: weight=1.5786192847252016 loss= 0.026323602895125697\n",
            "iteration 708: weight=1.5761241877329613 loss= 0.02472396772280927\n",
            "iteration 709: weight=1.5736374436743275 loss= 0.023179567832012582\n",
            "iteration 710: weight=1.5711590362190622 loss= 0.0216898924435589\n",
            "iteration 711: weight=1.5686889490187144 loss= 0.02025443374250324\n",
            "iteration 712: weight=1.5662271657068383 loss= 0.018872686869182198\n",
            "iteration 713: weight=1.5637736698992122 loss= 0.017544149910244\n",
            "iteration 714: weight=1.5613284451940566 loss= 0.016268323889654823\n",
            "iteration 715: weight=1.5588914751722536 loss= 0.015044712759681556\n",
            "iteration 716: weight=1.5564627433975657 loss= 0.013872823391856581\n",
            "iteration 717: weight=1.554042233416855 loss= 0.012752165567917473\n",
            "iteration 718: weight=1.5516299287603024 loss= 0.011682251970727364\n",
            "iteration 719: weight=1.5492258129416265 loss= 0.010662598175175564\n",
            "iteration 720: weight=1.5468298694583036 loss= 0.009692722639055962\n",
            "iteration 721: weight=1.544442081791787 loss= 0.008772146693927008\n",
            "iteration 722: weight=1.5420624334077264 loss= 0.007900394535951483\n",
            "iteration 723: weight=1.539690907756188 loss= 0.00707699321671762\n",
            "iteration 724: weight=1.5373274882718735 loss= 0.0063014726340409055\n",
            "iteration 725: weight=1.5349721583743405 loss= 0.005573365522747453\n",
            "iteration 726: weight=1.5326249014682225 loss= 0.0048922074454398334\n",
            "iteration 727: weight=1.530285700943448 loss= 0.00425753678324492\n",
            "iteration 728: weight=1.5279545401754615 loss= 0.0036688947265439698\n",
            "iteration 729: weight=1.5256314025254425 loss= 0.0031258252656860552\n",
            "iteration 730: weight=1.523316271340526 loss= 0.0026278751816850576\n",
            "iteration 731: weight=1.5210091299540225 loss= 0.002174594036900235\n",
            "iteration 732: weight=1.5187099616856379 loss= 0.0017655341656999113\n",
            "iteration 733: weight=1.5164187498416937 loss= 0.0014002506651120683\n",
            "iteration 734: weight=1.5141354777153466 loss= 0.0010783013854565038\n",
            "iteration 735: weight=1.5118601285868094 loss= 0.000799246920964336\n",
            "iteration 736: weight=1.5095926857235702 loss= 0.0005626506003826304\n",
            "iteration 737: weight=1.5073331323806123 loss= 0.00036807847756481493\n",
            "iteration 738: weight=1.505081451800635 loss= 0.00021509932204644144\n",
            "iteration 739: weight=1.502837627214273 loss= 0.00010328460960873365\n",
            "iteration 740: weight=1.5006016418403156 loss= 3.2208512828812985e-05\n",
            "iteration 741: weight=1.4983734788859282 loss= 1.4478916161575484e-06\n",
            "iteration 742: weight=1.4961531215468704 loss= 1.0582283738069798e-05\n",
            "iteration 743: weight=1.4939405530077166 loss= 5.919389533259878e-05\n",
            "iteration 744: weight=1.4917357564420752 loss= 0.00014686759140913885\n",
            "iteration 745: weight=1.4895387150128088 loss= 0.0002731908863388144\n",
            "iteration 746: weight=1.4873494118722523 loss= 0.00043775393433298504\n",
            "iteration 747: weight=1.485167830162434 loss= 0.000640149519911648\n",
            "iteration 748: weight=1.4829939530152934 loss= 0.0008799730483616264\n",
            "iteration 749: weight=1.4808277635529012 loss= 0.00115682253618421\n",
            "iteration 750: weight=1.4786692448876784 loss= 0.0014702986015338038\n",
            "iteration 751: weight=1.4765183801226143 loss= 0.0018200044546473615\n",
            "iteration 752: weight=1.474375152351486 loss= 0.00220554588826416\n",
            "iteration 753: weight=1.4722395446590775 loss= 0.0026265312680382458\n",
            "iteration 754: weight=1.4701115401213969 loss= 0.0030825715229414463\n",
            "iteration 755: weight=1.4679911218058963 loss= 0.0035732801356594956\n",
            "iteration 756: weight=1.4658782727716886 loss= 0.004098273132979835\n",
            "iteration 757: weight=1.4637729760697664 loss= 0.004657169076173195\n",
            "iteration 758: weight=1.46167521474322 loss= 0.005249589051366854\n",
            "iteration 759: weight=1.459584971827454 loss= 0.005875156659913228\n",
            "iteration 760: weight=1.4575022303504066 loss= 0.006533498008750693\n",
            "iteration 761: weight=1.455426973332765 loss= 0.007224241700759637\n",
            "iteration 762: weight=1.4533591837881839 loss= 0.007947018825112173\n",
            "iteration 763: weight=1.4512988447235022 loss= 0.008701462947617644\n",
            "iteration 764: weight=1.4492459391389594 loss= 0.009487210101062238\n",
            "iteration 765: weight=1.447200450028412 loss= 0.010303898775544829\n",
            "iteration 766: weight=1.44516236037955 loss= 0.011151169908808933\n",
            "iteration 767: weight=1.4431316531741138 loss= 0.012028666876569338\n",
            "iteration 768: weight=1.4411083113881085 loss= 0.01293603548283706\n",
            "iteration 769: weight=1.4390923179920205 loss= 0.01387292395023998\n",
            "iteration 770: weight=1.4370836559510327 loss= 0.014838982910340581\n",
            "iteration 771: weight=1.4350823082252393 loss= 0.01583386539395204\n",
            "iteration 772: weight=1.4330882577698607 loss= 0.0168572268214513\n",
            "iteration 773: weight=1.4311014875354586 loss= 0.017908724993090375\n",
            "iteration 774: weight=1.4291219804681496 loss= 0.018988020079306223\n",
            "iteration 775: weight=1.42714971950982 loss= 0.020094774611029487\n",
            "iteration 776: weight=1.425184687598338 loss= 0.021228653469991587\n",
            "iteration 777: weight=1.4232268676677693 loss= 0.02238932387903314\n",
            "iteration 778: weight=1.4212762426485885 loss= 0.023576455392408824\n",
            "iteration 779: weight=1.419332795467893 loss= 0.024789719886095685\n",
            "iteration 780: weight=1.4173965090496143 loss= 0.026028791548099228\n",
            "iteration 781: weight=1.415467366314732 loss= 0.027293346868761836\n",
            "iteration 782: weight=1.413545350181484 loss= 0.028583064631070854\n",
            "iteration 783: weight=1.4116304435655795 loss= 0.029897625900968894\n",
            "iteration 784: weight=1.4097226293804088 loss= 0.031236714017664924\n",
            "iteration 785: weight=1.4078218905372553 loss= 0.03260001458394812\n",
            "iteration 786: weight=1.4059282099455055 loss= 0.03398721545650296\n",
            "iteration 787: weight=1.4040415705128597 loss= 0.035398006736227616\n",
            "iteration 788: weight=1.402161955145541 loss= 0.03683208075855393\n",
            "iteration 789: weight=1.400289346748506 loss= 0.03828913208377249\n",
            "iteration 790: weight=1.3984237282256524 loss= 0.0397688574873587\n",
            "iteration 791: weight=1.396565082480029 loss= 0.0412709559503045\n",
            "iteration 792: weight=1.3947133924140436 loss= 0.042795128649452785\n",
            "iteration 793: weight=1.3928686409296709 loss= 0.044341078947836676\n",
            "iteration 794: weight=1.39103081092866 loss= 0.04590851238502314\n",
            "iteration 795: weight=1.3891998853127419 loss= 0.0474971366674618\n",
            "iteration 796: weight=1.387375846983836 loss= 0.04910666165883826\n",
            "iteration 797: weight=1.3855586788442567 loss= 0.050736799370433294\n",
            "iteration 798: weight=1.383748363796919 loss= 0.05238726395148796\n",
            "iteration 799: weight=1.3819448847455444 loss= 0.05405777167957393\n",
            "iteration 800: weight=1.380148224594866 loss= 0.05574804095097119\n",
            "iteration 801: weight=1.3783583662508327 loss= 0.057457792271050745\n",
            "iteration 802: weight=1.3765752926208141 loss= 0.05918674824466619\n",
            "iteration 803: weight=1.3747989866138037 loss= 0.0609346335665506\n",
            "iteration 804: weight=1.3730294311406228 loss= 0.06270117501172201\n",
            "iteration 805: weight=1.371266609114123 loss= 0.06448610142589539\n",
            "iteration 806: weight=1.3695105034493886 loss= 0.066289143715904\n",
            "iteration 807: weight=1.3677610970639393 loss= 0.06811003484012812\n",
            "iteration 808: weight=1.3660183728779307 loss= 0.06994850979893152\n",
            "iteration 809: weight=1.3642823138143558 loss= 0.07180430562510887\n",
            "iteration 810: weight=1.3625529027992458 loss= 0.07367716137434\n",
            "iteration 811: weight=1.3608301227618698 loss= 0.07556681811565436\n",
            "iteration 812: weight=1.3591139566349348 loss= 0.07747301892190495\n",
            "iteration 813: weight=1.3574043873547843 loss= 0.07939550886025204\n",
            "iteration 814: weight=1.355701397861597 loss= 0.08133403498265757\n",
            "iteration 815: weight=1.3540049710995856 loss= 0.08328834631638848\n",
            "iteration 816: weight=1.3523150900171932 loss= 0.08525819385453137\n",
            "iteration 817: weight=1.3506317375672912 loss= 0.087243330546519\n",
            "iteration 818: weight=1.3489548967073757 loss= 0.08924351128866626\n",
            "iteration 819: weight=1.3472845503997635 loss= 0.09125849291471821\n",
            "iteration 820: weight=1.3456206816117875 loss= 0.09328803418640952\n",
            "iteration 821: weight=1.3439632733159916 loss= 0.09533189578403634\n",
            "iteration 822: weight=1.342312308490326 loss= 0.09738984029703968\n",
            "iteration 823: weight=1.3406677701183392 loss= 0.09946163221460058\n",
            "iteration 824: weight=1.3390296411893734 loss= 0.10154703791624975\n",
            "iteration 825: weight=1.337397904698756 loss= 0.10364582566248748\n",
            "iteration 826: weight=1.3357725436479928 loss= 0.10575776558541916\n",
            "iteration 827: weight=1.3341535410449583 loss= 0.10788262967940176\n",
            "iteration 828: weight=1.332540879904088 loss= 0.1100201917917053\n",
            "iteration 829: weight=1.3309345432465691 loss= 0.11217022761318818\n",
            "iteration 830: weight=1.3293345141005293 loss= 0.11433251466898486\n",
            "iteration 831: weight=1.327740775501227 loss= 0.11650683230920977\n",
            "iteration 832: weight=1.3261533104912397 loss= 0.1186929616996748\n",
            "iteration 833: weight=1.3245721021206533 loss= 0.12089068581262119\n",
            "iteration 834: weight=1.3229971334472481 loss= 0.12309978941746591\n",
            "iteration 835: weight=1.321428387536687 loss= 0.12532005907156518\n",
            "iteration 836: weight=1.3198658474627014 loss= 0.12755128311099062\n",
            "iteration 837: weight=1.3183094963072775 loss= 0.12979325164132294\n",
            "iteration 838: weight=1.3167593171608412 loss= 0.13204575652846096\n",
            "iteration 839: weight=1.3152152931224423 loss= 0.13430859138944484\n",
            "iteration 840: weight=1.3136774072999398 loss= 0.13658155158329954\n",
            "iteration 841: weight=1.3121456428101836 loss= 0.1388644342018901\n",
            "iteration 842: weight=1.3106199827791987 loss= 0.1411570380607965\n",
            "iteration 843: weight=1.3091004103423662 loss= 0.14345916369020406\n",
            "iteration 844: weight=1.307586908644606 loss= 0.14577061332581187\n",
            "iteration 845: weight=1.3060794608405564 loss= 0.14809119089975686\n",
            "iteration 846: weight=1.3045780500947555 loss= 0.15042070203155722\n",
            "iteration 847: weight=1.3030826595818195 loss= 0.1527589540190717\n",
            "iteration 848: weight=1.3015932724866228 loss= 0.1551057558294784\n",
            "iteration 849: weight=1.300109872004476 loss= 0.15746091809026996\n",
            "iteration 850: weight=1.2986324413413028 loss= 0.15982425308026804\n",
            "iteration 851: weight=1.2971609637138182 loss= 0.16219557472065538\n",
            "iteration 852: weight=1.2956954223497035 loss= 0.1645746985660279\n",
            "iteration 853: weight=1.294235800487783 loss= 0.16696144179546413\n",
            "iteration 854: weight=1.2927820813781985 loss= 0.16935562320361375\n",
            "iteration 855: weight=1.2913342482825836 loss= 0.17175706319180617\n",
            "iteration 856: weight=1.2898922844742375 loss= 0.1741655837591779\n",
            "iteration 857: weight=1.2884561732382982 loss= 0.17658100849381886\n",
            "iteration 858: weight=1.2870258978719138 loss= 0.17900316256393967\n",
            "iteration 859: weight=1.2856014416844153 loss= 0.18143187270905803\n",
            "iteration 860: weight=1.2841827879974868 loss= 0.18386696723120477\n",
            "iteration 861: weight=1.282769920145336 loss= 0.18630827598615096\n",
            "iteration 862: weight=1.2813628214748634 loss= 0.18875563037465481\n",
            "iteration 863: weight=1.2799614753458315 loss= 0.19120886333372988\n",
            "iteration 864: weight=1.2785658651310328 loss= 0.19366780932793248\n",
            "iteration 865: weight=1.277175974216457 loss= 0.19613230434067186\n",
            "iteration 866: weight=1.275791786001458 loss= 0.19860218586554013\n",
            "iteration 867: weight=1.27441328389892 loss= 0.20107729289766396\n",
            "iteration 868: weight=1.273040451335422 loss= 0.20355746592507717\n",
            "iteration 869: weight=1.2716732717514037 loss= 0.2060425469201157\n",
            "iteration 870: weight=1.270311728601328 loss= 0.20853237933083346\n",
            "iteration 871: weight=1.2689558053538452 loss= 0.21102680807244023\n",
            "iteration 872: weight=1.267605485491955 loss= 0.213525679518761\n",
            "iteration 873: weight=1.2662607525131677 loss= 0.21602884149371981\n",
            "iteration 874: weight=1.2649215899296664 loss= 0.21853614326284254\n",
            "iteration 875: weight=1.2635879812684658 loss= 0.22104743552478368\n",
            "iteration 876: weight=1.2622599100715728 loss= 0.22356257040287697\n",
            "iteration 877: weight=1.2609373598961444 loss= 0.22608140143670663\n",
            "iteration 878: weight=1.2596203143146467 loss= 0.22860378357370226\n",
            "iteration 879: weight=1.2583087569150113 loss= 0.231129573160757\n",
            "iteration 880: weight=1.2570026713007922 loss= 0.23365862793586833\n",
            "iteration 881: weight=1.2557020410913218 loss= 0.23619080701980333\n",
            "iteration 882: weight=1.2544068499218655 loss= 0.2387259709077849\n",
            "iteration 883: weight=1.253117081443776 loss= 0.24126398146120454\n",
            "iteration 884: weight=1.2518327193246477 loss= 0.24380470189935655\n",
            "iteration 885: weight=1.250553747248468 loss= 0.24634799679119634\n",
            "iteration 886: weight=1.2492801489157708 loss= 0.24889373204712473\n",
            "iteration 887: weight=1.2480119080437868 loss= 0.25144177491079234\n",
            "iteration 888: weight=1.2467490083665944 loss= 0.25399199395093186\n",
            "iteration 889: weight=1.2454914336352694 loss= 0.2565442590532131\n",
            "iteration 890: weight=1.2442391676180338 loss= 0.259098441412122\n",
            "iteration 891: weight=1.242992194100404 loss= 0.2616544135228649\n",
            "iteration 892: weight=1.241750496885339 loss= 0.26421204917329755\n",
            "iteration 893: weight=1.240514059793386 loss= 0.2667712234358772\n",
            "iteration 894: weight=1.2392828666628264 loss= 0.269331812659642\n",
            "iteration 895: weight=1.238056901349822 loss= 0.27189369446221423\n",
            "iteration 896: weight=1.2368361477285579 loss= 0.2744567477218276\n",
            "iteration 897: weight=1.235620589691387 loss= 0.2770208525693817\n",
            "iteration 898: weight=1.2344102111489728 loss= 0.27958589038051973\n",
            "iteration 899: weight=1.2332049960304305 loss= 0.282151743767733\n",
            "iteration 900: weight=1.2320049282834695 loss= 0.28471829657249037\n",
            "iteration 901: weight=1.2308099918745325 loss= 0.28728543385739325\n",
            "iteration 902: weight=1.2296201707889365 loss= 0.2898530418983569\n",
            "iteration 903: weight=1.22843544903101 loss= 0.2924210081768155\n",
            "iteration 904: weight=1.227255810624233 loss= 0.2949892213719565\n",
            "iteration 905: weight=1.2260812396113725 loss= 0.2975575713529769\n",
            "iteration 906: weight=1.22491172005462 loss= 0.30012594917136926\n",
            "iteration 907: weight=1.223747236035727 loss= 0.302694247053231\n",
            "iteration 908: weight=1.2225877716561397 loss= 0.3052623583916011\n",
            "iteration 909: weight=1.2214333110371332 loss= 0.30783017773882426\n",
            "iteration 910: weight=1.2202838383199446 loss= 0.3103976007989384\n",
            "iteration 911: weight=1.2191393376659059 loss= 0.3129645244200916\n",
            "iteration 912: weight=1.2179997932565751 loss= 0.31553084658698416\n",
            "iteration 913: weight=1.216865189293868 loss= 0.31809646641333744\n",
            "iteration 914: weight=1.215735510000187 loss= 0.32066128413438877\n",
            "iteration 915: weight=1.2146107396185515 loss= 0.3232252010994152\n",
            "iteration 916: weight=1.2134908624127256 loss= 0.3257881197642809\n",
            "iteration 917: weight=1.2123758626673466 loss= 0.3283499436840148\n",
            "iteration 918: weight=1.211265724688051 loss= 0.3309105775054122\n",
            "iteration 919: weight=1.210160432801601 loss= 0.3334699269596656\n",
            "iteration 920: weight=1.2090599713560093 loss= 0.33602789885502116\n",
            "iteration 921: weight=1.2079643247206644 loss= 0.3385844010694645\n",
            "iteration 922: weight=1.2068734772864536 loss= 0.34113934254343015\n",
            "iteration 923: weight=1.2057874134658855 loss= 0.34369263327254107\n",
            "iteration 924: weight=1.2047061176932128 loss= 0.3462441843003753\n",
            "iteration 925: weight=1.2036295744245529 loss= 0.34879390771125873\n",
            "iteration 926: weight=1.2025577681380082 loss= 0.35134171662308655\n",
            "iteration 927: weight=1.2014906833337862 loss= 0.35388752518017164\n",
            "iteration 928: weight=1.2004283045343178 loss= 0.35643124854611974\n",
            "iteration 929: weight=1.1993706162843754 loss= 0.3589728028967337\n",
            "iteration 930: weight=1.1983176031511895 loss= 0.3615121054129451\n",
            "iteration 931: weight=1.197269249724566 loss= 0.36404907427377275\n",
            "iteration 932: weight=1.1962255406170008 loss= 0.3665836286493086\n",
            "iteration 933: weight=1.1951864604637947 loss= 0.3691156886937337\n",
            "iteration 934: weight=1.1941519939231673 loss= 0.3716451755383591\n",
            "iteration 935: weight=1.19312212567637 loss= 0.37417201128469724\n",
            "iteration 936: weight=1.192096840427798 loss= 0.3766961189975586\n",
            "iteration 937: weight=1.1910761229051023 loss= 0.3792174226981794\n",
            "iteration 938: weight=1.1900599578592992 loss= 0.38173584735737376\n",
            "iteration 939: weight=1.189048330064881 loss= 0.38425131888871755\n",
            "iteration 940: weight=1.1880412243199245 loss= 0.3867637641417565\n",
            "iteration 941: weight=1.1870386254461989 loss= 0.3892731108952465\n",
            "iteration 942: weight=1.1860405182892735 loss= 0.3917792878504184\n",
            "iteration 943: weight=1.185046887718624 loss= 0.3942822246242721\n",
            "iteration 944: weight=1.1840577186277381 loss= 0.39678185174290015\n",
            "iteration 945: weight=1.1830729959342203 loss= 0.39927810063483804\n",
            "iteration 946: weight=1.1820927045798957 loss= 0.4017709036244429\n",
            "iteration 947: weight=1.1811168295309136 loss= 0.4042601939253019\n",
            "iteration 948: weight=1.1801453557778494 loss= 0.40674590563366575\n",
            "iteration 949: weight=1.1791782683358065 loss= 0.4092279737219141\n",
            "iteration 950: weight=1.1782155522445168 loss= 0.41170633403204704\n",
            "iteration 951: weight=1.1772571925684407 loss= 0.4141809232692052\n",
            "iteration 952: weight=1.1763031743968662 loss= 0.41665167899521816\n",
            "iteration 953: weight=1.1753534828440066 loss= 0.4191185396221826\n",
            "iteration 954: weight=1.174408103049099 loss= 0.4215814444060667\n",
            "iteration 955: weight=1.1734670201764998 loss= 0.42404033344034475\n",
            "iteration 956: weight=1.1725302194157814 loss= 0.42649514764965746\n",
            "iteration 957: weight=1.1715976859818262 loss= 0.42894582878350507\n",
            "iteration 958: weight=1.170669405114922 loss= 0.4313923194099648\n",
            "iteration 959: weight=1.1697453620808542 loss= 0.43383456290943745\n",
            "iteration 960: weight=1.1688255421709988 loss= 0.4362725034684243\n",
            "iteration 961: weight=1.1679099307024141 loss= 0.4387060860733316\n",
            "iteration 962: weight=1.1669985130179317 loss= 0.44113525650430163\n",
            "iteration 963: weight=1.166091274486246 loss= 0.4435599613290745\n",
            "iteration 964: weight=1.1651882005020042 loss= 0.44598014789687823\n",
            "iteration 965: weight=1.1642892764858943 loss= 0.4483957643323445\n",
            "iteration 966: weight=1.1633944878847329 loss= 0.45080675952945726\n",
            "iteration 967: weight=1.1625038201715516 loss= 0.453213083145525\n",
            "iteration 968: weight=1.161617258845684 loss= 0.4556146855951855\n",
            "iteration 969: weight=1.1607347894328492 loss= 0.4580115180444355\n",
            "iteration 970: weight=1.1598563974852383 loss= 0.46040353240469256\n",
            "iteration 971: weight=1.1589820685815957 loss= 0.46279068132688084\n",
            "iteration 972: weight=1.1581117883273038 loss= 0.46517291819554996\n",
            "iteration 973: weight=1.1572455423544639 loss= 0.4675501971230174\n",
            "iteration 974: weight=1.1563833163219772 loss= 0.4699224729435425\n",
            "iteration 975: weight=1.1555250959156258 loss= 0.47228970120752944\n",
            "iteration 976: weight=1.154670866848152 loss= 0.4746518381757552\n",
            "iteration 977: weight=1.1538206148593364 loss= 0.47700884081362704\n",
            "iteration 978: weight=1.1529743257160767 loss= 0.4793606667854716\n",
            "iteration 979: weight=1.1521319852124643 loss= 0.4817072744488464\n",
            "iteration 980: weight=1.1512935791698606 loss= 0.4840486228488846\n",
            "iteration 981: weight=1.1504590934369727 loss= 0.486384671712665\n",
            "iteration 982: weight=1.149628513889928 loss= 0.4887153814436118\n",
            "iteration 983: weight=1.1488018264323479 loss= 0.49104071311592135\n",
            "iteration 984: weight=1.1479790169954207 loss= 0.49336062846901885\n",
            "iteration 985: weight=1.147160071537975 loss= 0.49567508990204123\n",
            "iteration 986: weight=1.1463449760465503 loss= 0.49798406046834753\n",
            "iteration 987: weight=1.1455337165354673 loss= 0.5002875038700604\n",
            "iteration 988: weight=1.144726279046899 loss= 0.5025853844526338\n",
            "iteration 989: weight=1.1439226496509387 loss= 0.5048776671994474\n",
            "iteration 990: weight=1.143122814445669 loss= 0.5071643177264326\n",
            "iteration 991: weight=1.1423267595572293 loss= 0.5094453022767214\n",
            "iteration 992: weight=1.1415344711398818 loss= 0.5117205877153284\n",
            "iteration 993: weight=1.1407459353760787 loss= 0.5139901415238568\n",
            "iteration 994: weight=1.1399611384765262 loss= 0.5162539317952344\n",
            "iteration 995: weight=1.1391800666802498 loss= 0.5185119272284764\n",
            "iteration 996: weight=1.1384027062546571 loss= 0.5207640971234757\n",
            "iteration 997: weight=1.1376290434956011 loss= 0.5230104113758232\n",
            "iteration 998: weight=1.1368590647274421 loss= 0.5252508404716518\n",
            "iteration 999: weight=1.136092756303109 loss= 0.5274853554825123\n",
            "iteration 1000: weight=1.1353301046041595 loss= 0.529713928060274\n",
            "iteration 1001: weight=1.1345710960408408 loss= 0.531936530432053\n",
            "iteration 1002: weight=1.133815717052147 loss= 0.5341531353951695\n",
            "iteration 1003: weight=1.133063954105878 loss= 0.5363637163121331\n",
            "iteration 1004: weight=1.1323157936986974 loss= 0.5385682471056528\n",
            "iteration 1005: weight=1.1315712223561882 loss= 0.5407667022536754\n",
            "iteration 1006: weight=1.1308302266329093 loss= 0.5429590567844531\n",
            "iteration 1007: weight=1.13009279311245 loss= 0.5451452862716364\n",
            "iteration 1008: weight=1.1293589084074849 loss= 0.5473253668293947\n",
            "iteration 1009: weight=1.1286285591598268 loss= 0.5494992751075647\n",
            "iteration 1010: weight=1.12790173204048 loss= 0.5516669882868249\n",
            "iteration 1011: weight=1.1271784137496923 loss= 0.5538284840738988\n",
            "iteration 1012: weight=1.1264585910170057 loss= 0.5559837406967825\n",
            "iteration 1013: weight=1.1257422506013075 loss= 0.5581327369000023\n",
            "iteration 1014: weight=1.1250293792908799 loss= 0.560275451939898\n",
            "iteration 1015: weight=1.1243199639034487 loss= 0.5624118655799313\n",
            "iteration 1016: weight=1.1236139912862317 loss= 0.5645419580860245\n",
            "iteration 1017: weight=1.1229114483159865 loss= 0.5666657102219234\n",
            "iteration 1018: weight=1.122212321899057 loss= 0.5687831032445876\n",
            "iteration 1019: weight=1.1215165989714189 loss= 0.5708941188996071\n",
            "iteration 1020: weight=1.1208242664987258 loss= 0.5729987394166471\n",
            "iteration 1021: weight=1.1201353114763533 loss= 0.5750969475049172\n",
            "iteration 1022: weight=1.1194497209294425 loss= 0.5771887263486686\n",
            "iteration 1023: weight=1.1187674819129436 loss= 0.5792740596027168\n",
            "iteration 1024: weight=1.1180885815116577 loss= 0.581352931387991\n",
            "iteration 1025: weight=1.1174130068402786 loss= 0.583425326287111\n",
            "iteration 1026: weight=1.1167407450434335 loss= 0.585491229339987\n",
            "iteration 1027: weight=1.116071783295723 loss= 0.5875506260394499\n",
            "iteration 1028: weight=1.1154061088017608 loss= 0.5896035023269052\n",
            "iteration 1029: weight=1.1147437087962122 loss= 0.5916498445880121\n",
            "iteration 1030: weight=1.1140845705438316 loss= 0.5936896396483909\n",
            "iteration 1031: weight=1.1134286813395002 loss= 0.5957228747693556\n",
            "iteration 1032: weight=1.1127760285082622 loss= 0.5977495376436706\n",
            "iteration 1033: weight=1.1121265994053608 loss= 0.5997696163913367\n",
            "iteration 1034: weight=1.1114803814162728 loss= 0.6017830995553977\n",
            "iteration 1035: weight=1.1108373619567433 loss= 0.6037899760977794\n",
            "iteration 1036: weight=1.110197528472819 loss= 0.6057902353951472\n",
            "iteration 1037: weight=1.1095608684408813 loss= 0.6077838672347953\n",
            "iteration 1038: weight=1.1089273693676787 loss= 0.6097708618105552\n",
            "iteration 1039: weight=1.1082970187903574 loss= 0.6117512097187361\n",
            "iteration 1040: weight=1.1076698042764932 loss= 0.6137249019540867\n",
            "iteration 1041: weight=1.107045713424121 loss= 0.6156919299057806\n",
            "iteration 1042: weight=1.1064247338617639 loss= 0.6176522853534323\n",
            "iteration 1043: weight=1.1058068532484622 loss= 0.6196059604631337\n",
            "iteration 1044: weight=1.1051920592738016 loss= 0.6215529477835178\n",
            "iteration 1045: weight=1.1045803396579403 loss= 0.6234932402418456\n",
            "iteration 1046: weight=1.1039716821516352 loss= 0.6254268311401193\n",
            "iteration 1047: weight=1.1033660745362683 loss= 0.6273537141512218\n",
            "iteration 1048: weight=1.102763504623872 loss= 0.6292738833150762\n",
            "iteration 1049: weight=1.1021639602571531 loss= 0.6311873330348343\n",
            "iteration 1050: weight=1.1015674293095172 loss= 0.6330940580730882\n",
            "iteration 1051: weight=1.1009738996850915 loss= 0.6349940535481062\n",
            "iteration 1052: weight=1.1003833593187475 loss= 0.6368873149300937\n",
            "iteration 1053: weight=1.0997957961761229 loss= 0.638773838037477\n",
            "iteration 1054: weight=1.0992111982536423 loss= 0.6406536190332135\n",
            "iteration 1055: weight=1.0986295535785386 loss= 0.6425266544211248\n",
            "iteration 1056: weight=1.098050850208872 loss= 0.6443929410422528\n",
            "iteration 1057: weight=1.0974750762335492 loss= 0.6462524760712428\n",
            "iteration 1058: weight=1.0969022197723426 loss= 0.648105257012748\n",
            "iteration 1059: weight=1.0963322689759072 loss= 0.6499512816978592\n",
            "iteration 1060: weight=1.0957652120257984 loss= 0.6517905482805574\n",
            "iteration 1061: weight=1.0952010371344885 loss= 0.6536230552341911\n",
            "iteration 1062: weight=1.0946397325453825 loss= 0.6554488013479751\n",
            "iteration 1063: weight=1.094081286532833 loss= 0.6572677857235161\n",
            "iteration 1064: weight=1.0935256874021555 loss= 0.6590800077713598\n",
            "iteration 1065: weight=1.0929729234896415 loss= 0.6608854672075608\n",
            "iteration 1066: weight=1.0924229831625722 loss= 0.662684164050277\n",
            "iteration 1067: weight=1.0918758548192316 loss= 0.6644760986163875\n",
            "iteration 1068: weight=1.0913315268889174 loss= 0.6662612715181319\n",
            "iteration 1069: weight=1.0907899878319536 loss= 0.6680396836597744\n",
            "iteration 1070: weight=1.0902512261397006 loss= 0.6698113362342908\n",
            "iteration 1071: weight=1.0897152303345652 loss= 0.6715762307200751\n",
            "iteration 1072: weight=1.0891819889700105 loss= 0.6733343688776756\n",
            "iteration 1073: weight=1.0886514906305649 loss= 0.6750857527465463\n",
            "iteration 1074: weight=1.0881237239318295 loss= 0.676830384641825\n",
            "iteration 1075: weight=1.0875986775204867 loss= 0.678568267151135\n",
            "iteration 1076: weight=1.0870763400743066 loss= 0.6802994031314062\n",
            "iteration 1077: weight=1.0865567003021537 loss= 0.6820237957057187\n",
            "iteration 1078: weight=1.0860397469439926 loss= 0.6837414482601727\n",
            "iteration 1079: weight=1.0855254687708935 loss= 0.6854523644407746\n",
            "iteration 1080: weight=1.0850138545850359 loss= 0.6871565481503503\n",
            "iteration 1081: weight=1.0845048932197137 loss= 0.6888540035454791\n",
            "iteration 1082: weight=1.0839985735393376 loss= 0.6905447350334462\n",
            "iteration 1083: weight=1.0834948844394388 loss= 0.6922287472692237\n",
            "iteration 1084: weight=1.0829938148466705 loss= 0.6939060451524659\n",
            "iteration 1085: weight=1.0824953537188096 loss= 0.6955766338245318\n",
            "iteration 1086: weight=1.0819994900447578 loss= 0.6972405186655277\n",
            "iteration 1087: weight=1.0815062128445423 loss= 0.6988977052913701\n",
            "iteration 1088: weight=1.0810155111693152 loss= 0.7005481995508702\n",
            "iteration 1089: weight=1.0805273741013528 loss= 0.7021920075228412\n",
            "iteration 1090: weight=1.0800417907540547 loss= 0.7038291355132257\n",
            "iteration 1091: weight=1.0795587502719413 loss= 0.7054595900522447\n",
            "iteration 1092: weight=1.0790782418306517 loss= 0.7070833778915675\n",
            "iteration 1093: weight=1.0786002546369409 loss= 0.7087005060015013\n",
            "iteration 1094: weight=1.0781247779286753 loss= 0.7103109815682043\n",
            "iteration 1095: weight=1.0776518009748297 loss= 0.711914811990918\n",
            "iteration 1096: weight=1.0771813130754815 loss= 0.7135120048792194\n",
            "iteration 1097: weight=1.0767133035618062 loss= 0.715102568050296\n",
            "iteration 1098: weight=1.0762477617960708 loss= 0.7166865095262387\n",
            "iteration 1099: weight=1.0757846771716282 loss= 0.7182638375313581\n",
            "iteration 1100: weight=1.0753240391129095 loss= 0.7198345604895189\n",
            "iteration 1101: weight=1.0748658370754174 loss= 0.7213986870214943\n",
            "iteration 1102: weight=1.0744100605457167 loss= 0.7229562259423422\n",
            "iteration 1103: weight=1.0739566990414275 loss= 0.724507186258802\n",
            "iteration 1104: weight=1.0735057421112146 loss= 0.726051577166707\n",
            "iteration 1105: weight=1.0730571793347787 loss= 0.7275894080484231\n",
            "iteration 1106: weight=1.0726110003228455 loss= 0.7291206884703012\n",
            "iteration 1107: weight=1.0721671947171556 loss= 0.7306454281801551\n",
            "iteration 1108: weight=1.0717257521904529 loss= 0.7321636371047531\n",
            "iteration 1109: weight=1.071286662446473 loss= 0.7336753253473336\n",
            "iteration 1110: weight=1.0708499152199307 loss= 0.7351805031851376\n",
            "iteration 1111: weight=1.070415500276507 loss= 0.7366791810669628\n",
            "iteration 1112: weight=1.0699834074128365 loss= 0.738171369610735\n",
            "iteration 1113: weight=1.0695536264564927 loss= 0.7396570796010983\n",
            "iteration 1114: weight=1.0691261472659737 loss= 0.7411363219870266\n",
            "iteration 1115: weight=1.068700959730688 loss= 0.7426091078794537\n",
            "iteration 1116: weight=1.0682780537709384 loss= 0.7440754485489184\n",
            "iteration 1117: weight=1.0678574193379065 loss= 0.7455353554232349\n",
            "iteration 1118: weight=1.0674390464136359 loss= 0.7469888400851761\n",
            "iteration 1119: weight=1.067022925011016 loss= 0.7484359142701787\n",
            "iteration 1120: weight=1.0666090451737635 loss= 0.7498765898640654\n",
            "iteration 1121: weight=1.066197396976406 loss= 0.7513108789007877\n",
            "iteration 1122: weight=1.0657879705242626 loss= 0.7527387935601835\n",
            "iteration 1123: weight=1.0653807559534247 loss= 0.7541603461657548\n",
            "iteration 1124: weight=1.0649757434307379 loss= 0.7555755491824662\n",
            "iteration 1125: weight=1.064572923153781 loss= 0.7569844152145568\n",
            "iteration 1126: weight=1.0641722853508466 loss= 0.7583869570033722\n",
            "iteration 1127: weight=1.0637738202809197 loss= 0.7597831874252154\n",
            "iteration 1128: weight=1.063377518233657 loss= 0.7611731194892133\n",
            "iteration 1129: weight=1.0629833695293642 loss= 0.7625567663352024\n",
            "iteration 1130: weight=1.0625913645189755 loss= 0.7639341412316328\n",
            "iteration 1131: weight=1.062201493584029 loss= 0.765305257573487\n",
            "iteration 1132: weight=1.061813747136645 loss= 0.7666701288802202\n",
            "iteration 1133: weight=1.0614281156195018 loss= 0.7680287687937127\n",
            "iteration 1134: weight=1.061044589505812 loss= 0.7693811910762444\n",
            "iteration 1135: weight=1.060663159299298 loss= 0.7707274096084842\n",
            "iteration 1136: weight=1.0602838155341667 loss= 0.7720674383874957\n",
            "iteration 1137: weight=1.0599065487750845 loss= 0.7734012915247629\n",
            "iteration 1138: weight=1.0595313496171512 loss= 0.7747289832442283\n",
            "iteration 1139: weight=1.0591582086858744 loss= 0.7760505278803531\n",
            "iteration 1140: weight=1.058787116637142 loss= 0.7773659398761883\n",
            "iteration 1141: weight=1.0584180641571956 loss= 0.7786752337814674\n",
            "iteration 1142: weight=1.0580510419626026 loss= 0.7799784242507144\n",
            "iteration 1143: weight=1.0576860408002282 loss= 0.7812755260413651\n",
            "iteration 1144: weight=1.0573230514472078 loss= 0.7825665540119096\n",
            "iteration 1145: weight=1.056962064710917 loss= 0.7838515231200456\n",
            "iteration 1146: weight=1.0566030714289432 loss= 0.7851304484208547\n",
            "iteration 1147: weight=1.0562460624690553 loss= 0.7864033450649875\n",
            "iteration 1148: weight=1.0558910287291743 loss= 0.7876702282968704\n",
            "iteration 1149: weight=1.055537961137342 loss= 0.7889311134529244\n",
            "iteration 1150: weight=1.0551868506516908 loss= 0.7901860159598033\n",
            "iteration 1151: weight=1.054837688260412 loss= 0.7914349513326449\n",
            "iteration 1152: weight=1.0544904649817242 loss= 0.7926779351733364\n",
            "iteration 1153: weight=1.0541451718638408 loss= 0.7939149831688014\n",
            "iteration 1154: weight=1.0538017999849385 loss= 0.7951461110892961\n",
            "iteration 1155: weight=1.053460340453123 loss= 0.7963713347867233\n",
            "iteration 1156: weight=1.053120784406397 loss= 0.7975906701929629\n",
            "iteration 1157: weight=1.0527831230126254 loss= 0.7988041333182156\n",
            "iteration 1158: weight=1.052447347469502 loss= 0.8000117402493621\n",
            "iteration 1159: weight=1.0521134490045145 loss= 0.8012135071483388\n",
            "iteration 1160: weight=1.0517814188749102 loss= 0.8024094502505263\n",
            "iteration 1161: weight=1.0514512483676601 loss= 0.8035995858631549\n",
            "iteration 1162: weight=1.051122928799424 loss= 0.8047839303637221\n",
            "iteration 1163: weight=1.0507964515165138 loss= 0.8059625001984282\n",
            "iteration 1164: weight=1.0504718078948578 loss= 0.8071353118806229\n",
            "iteration 1165: weight=1.0501489893399638 loss= 0.8083023819892704\n",
            "iteration 1166: weight=1.0498279872868814 loss= 0.809463727167424\n",
            "iteration 1167: weight=1.0495087932001657 loss= 0.8106193641207208\n",
            "iteration 1168: weight=1.0491913985738384 loss= 0.8117693096158844\n",
            "iteration 1169: weight=1.0488757949313505 loss= 0.8129135804792474\n",
            "iteration 1170: weight=1.0485619738255432 loss= 0.8140521935952836\n",
            "iteration 1171: weight=1.0482499268386092 loss= 0.8151851659051583\n",
            "iteration 1172: weight=1.0479396455820535 loss= 0.8163125144052878\n",
            "iteration 1173: weight=1.047631121696654 loss= 0.8174342561459175\n",
            "iteration 1174: weight=1.0473243468524218 loss= 0.8185504082297096\n",
            "iteration 1175: weight=1.04701931274856 loss= 0.819660987810346\n",
            "iteration 1176: weight=1.0467160111134242 loss= 0.8207660120911473\n",
            "iteration 1177: weight=1.046414433704481 loss= 0.8218654983237017\n",
            "iteration 1178: weight=1.046114572308267 loss= 0.8229594638065069\n",
            "iteration 1179: weight=1.0458164187403471 loss= 0.8240479258836295\n",
            "iteration 1180: weight=1.0455199648452729 loss= 0.8251309019433748\n",
            "iteration 1181: weight=1.0452252024965396 loss= 0.826208409416968\n",
            "iteration 1182: weight=1.044932123596545 loss= 0.8272804657772537\n",
            "iteration 1183: weight=1.044640720076545 loss= 0.8283470885374009\n",
            "iteration 1184: weight=1.0443509838966119 loss= 0.8294082952496296\n",
            "iteration 1185: weight=1.0440629070455894 loss= 0.8304641035039427\n",
            "iteration 1186: weight=1.04377648154105 loss= 0.8315145309268756\n",
            "iteration 1187: weight=1.0434916994292505 loss= 0.8325595951802557\n",
            "iteration 1188: weight=1.0432085527850878 loss= 0.8335993139599752\n",
            "iteration 1189: weight=1.0429270337120538 loss= 0.8346337049947755\n",
            "iteration 1190: weight=1.0426471343421904 loss= 0.8356627860450482\n",
            "iteration 1191: weight=1.0423688468360446 loss= 0.8366865749016419\n",
            "iteration 1192: weight=1.0420921633826228 loss= 0.8377050893846865\n",
            "iteration 1193: weight=1.0418170761993446 loss= 0.8387183473424265\n",
            "iteration 1194: weight=1.0415435775319972 loss= 0.8397263666500687\n",
            "iteration 1195: weight=1.041271659654688 loss= 0.8407291652086394\n",
            "iteration 1196: weight=1.0410013148697992 loss= 0.8417267609438575\n",
            "iteration 1197: weight=1.0407325355079393 loss= 0.8427191718050129\n",
            "iteration 1198: weight=1.0404653139278968 loss= 0.8437064157638648\n",
            "iteration 1199: weight=1.0401996425165925 loss= 0.8446885108135459\n",
            "iteration 1200: weight=1.0399355136890314 loss= 0.8456654749674775\n",
            "iteration 1201: weight=1.0396729198882546 loss= 0.8466373262583018\n",
            "iteration 1202: weight=1.0394118535852912 loss= 0.847604082736821\n",
            "iteration 1203: weight=1.0391523072791091 loss= 0.848565762470949\n",
            "iteration 1204: weight=1.038894273496567 loss= 0.8495223835446746\n",
            "iteration 1205: weight=1.038637744792364 loss= 0.8504739640570352\n",
            "iteration 1206: weight=1.0383827137489916 loss= 0.8514205221211033\n",
            "iteration 1207: weight=1.0381291729766826 loss= 0.8523620758629817\n",
            "iteration 1208: weight=1.0378771151133623 loss= 0.8532986434208127\n",
            "iteration 1209: weight=1.0376265328245977 loss= 0.8542302429437942\n",
            "iteration 1210: weight=1.0373774188035472 loss= 0.8551568925912112\n",
            "iteration 1211: weight=1.03712976577091 loss= 0.8560786105314744\n",
            "iteration 1212: weight=1.0368835664748752 loss= 0.8569954149411706\n",
            "iteration 1213: weight=1.0366388136910707 loss= 0.8579073240041253\n",
            "iteration 1214: weight=1.0363955002225116 loss= 0.8588143559104732\n",
            "iteration 1215: weight=1.0361536188995486 loss= 0.859716528855741\n",
            "iteration 1216: weight=1.0359131625798164 loss= 0.8606138610399408\n",
            "iteration 1217: weight=1.0356741241481813 loss= 0.8615063706666719\n",
            "iteration 1218: weight=1.0354364965166891 loss= 0.8623940759422343\n",
            "iteration 1219: weight=1.0352002726245126 loss= 0.8632769950747528\n",
            "iteration 1220: weight=1.0349654454378991 loss= 0.8641551462733095\n",
            "iteration 1221: weight=1.0347320079501166 loss= 0.8650285477470863\n",
            "iteration 1222: weight=1.0344999531814012 loss= 0.8658972177045217\n",
            "iteration 1223: weight=1.0342692741789037 loss= 0.8667611743524707\n",
            "iteration 1224: weight=1.034039964016636 loss= 0.8676204358953807\n",
            "iteration 1225: weight=1.0338120157954171 loss= 0.8684750205344715\n",
            "iteration 1226: weight=1.0335854226428194 loss= 0.8693249464669296\n",
            "iteration 1227: weight=1.0333601777131138 loss= 0.8701702318851097\n",
            "iteration 1228: weight=1.033136274187216 loss= 0.8710108949757468\n",
            "iteration 1229: weight=1.0329137052726316 loss= 0.8718469539191771\n",
            "iteration 1230: weight=1.032692464203401 loss= 0.8726784268885682\n",
            "iteration 1231: weight=1.0324725442400449 loss= 0.8735053320491586\n",
            "iteration 1232: weight=1.0322539386695084 loss= 0.8743276875575072\n",
            "iteration 1233: weight=1.0320366408051065 loss= 0.8751455115607519\n",
            "iteration 1234: weight=1.0318206439864674 loss= 0.8759588221958757\n",
            "iteration 1235: weight=1.0316059415794783 loss= 0.8767676375889842\n",
            "iteration 1236: weight=1.0313925269762274 loss= 0.8775719758545885\n",
            "iteration 1237: weight=1.0311803935949495 loss= 0.878371855094903\n",
            "iteration 1238: weight=1.0309695348799688 loss= 0.8791672933991459\n",
            "iteration 1239: weight=1.0307599443016426 loss= 0.8799583088428513\n",
            "iteration 1240: weight=1.0305516153563048 loss= 0.8807449194871901\n",
            "iteration 1241: weight=1.030344541566209 loss= 0.8815271433782992\n",
            "iteration 1242: weight=1.030138716479471 loss= 0.8823049985466178\n",
            "iteration 1243: weight=1.0299341336700127 loss= 0.8830785030062355\n",
            "iteration 1244: weight=1.0297307867375038 loss= 0.8838476747542457\n",
            "iteration 1245: weight=1.0295286693073045 loss= 0.8846125317701083\n",
            "iteration 1246: weight=1.0293277750304082 loss= 0.8853730920150227\n",
            "iteration 1247: weight=1.0291280975833832 loss= 0.8861293734313043\n",
            "iteration 1248: weight=1.0289296306683153 loss= 0.8868813939417756\n",
            "iteration 1249: weight=1.0287323680127496 loss= 0.8876291714491592\n",
            "iteration 1250: weight=1.0285363033696313 loss= 0.8883727238354819\n",
            "iteration 1251: weight=1.0283414305172487 loss= 0.8891120689614893\n",
            "iteration 1252: weight=1.0281477432591737 loss= 0.8898472246660614\n",
            "iteration 1253: weight=1.0279552354242032 loss= 0.8905782087656426\n",
            "iteration 1254: weight=1.0277639008663007 loss= 0.8913050390536778\n",
            "iteration 1255: weight=1.0275737334645367 loss= 0.8920277333000525\n",
            "iteration 1256: weight=1.0273847271230303 loss= 0.8927463092505463\n",
            "iteration 1257: weight=1.0271968757708891 loss= 0.8934607846262901\n",
            "iteration 1258: weight=1.0270101733621506 loss= 0.8941711771232321\n",
            "iteration 1259: weight=1.0268246138757222 loss= 0.8948775044116113\n",
            "iteration 1260: weight=1.0266401913153216 loss= 0.8955797841354375\n",
            "iteration 1261: weight=1.0264568997094174 loss= 0.8962780339119814\n",
            "iteration 1262: weight=1.0262747331111688 loss= 0.8969722713312669\n",
            "iteration 1263: weight=1.0260936855983656 loss= 0.8976625139555774\n",
            "iteration 1264: weight=1.0259137512733683 loss= 0.8983487793189632\n",
            "iteration 1265: weight=1.0257349242630476 loss= 0.899031084926759\n",
            "iteration 1266: weight=1.0255571987187244 loss= 0.8997094482551089\n",
            "iteration 1267: weight=1.0253805688161088 loss= 0.9003838867504959\n",
            "iteration 1268: weight=1.02520502875524 loss= 0.9010544178292816\n",
            "iteration 1269: weight=1.0250305727604256 loss= 0.9017210588772495\n",
            "iteration 1270: weight=1.02485719508018 loss= 0.9023838272491573\n",
            "iteration 1271: weight=1.0246848899871643 loss= 0.9030427402682968\n",
            "iteration 1272: weight=1.0245136517781253 loss= 0.9036978152260563\n",
            "iteration 1273: weight=1.0243434747738338 loss= 0.9043490693814957\n",
            "iteration 1274: weight=1.024174353319024 loss= 0.9049965199609219\n",
            "iteration 1275: weight=1.0240062817823317 loss= 0.9056401841574762\n",
            "iteration 1276: weight=1.0238392545562331 loss= 0.9062800791307242\n",
            "iteration 1277: weight=1.0236732660569838 loss= 0.906916222006255\n",
            "iteration 1278: weight=1.023508310724556 loss= 0.9075486298752836\n",
            "iteration 1279: weight=1.023344383022578 loss= 0.9081773197942653\n",
            "iteration 1280: weight=1.0231814774382715 loss= 0.9088023087845076\n",
            "iteration 1281: weight=1.023019588482391 loss= 0.9094236138317985\n",
            "iteration 1282: weight=1.0228587106891605 loss= 0.9100412518860308\n",
            "iteration 1283: weight=1.0226988386162124 loss= 0.9106552398608412\n",
            "iteration 1284: weight=1.022539966844525 loss= 0.91126559463325\n",
            "iteration 1285: weight=1.0223820899783602 loss= 0.9118723330433093\n",
            "iteration 1286: weight=1.022225202645202 loss= 0.9124754718937568\n",
            "iteration 1287: weight=1.022069299495693 loss= 0.9130750279496735\n",
            "iteration 1288: weight=1.0219143752035735 loss= 0.9136710179381503\n",
            "iteration 1289: weight=1.0217604244656178 loss= 0.9142634585479579\n",
            "iteration 1290: weight=1.021607442001572 loss= 0.9148523664292243\n",
            "iteration 1291: weight=1.0214554225540917 loss= 0.9154377581931175\n",
            "iteration 1292: weight=1.0213043608886796 loss= 0.9160196504115317\n",
            "iteration 1293: weight=1.0211542517936216 loss= 0.916598059616782\n",
            "iteration 1294: weight=1.0210050900799252 loss= 0.9171730023013053\n",
            "iteration 1295: weight=1.0208568705812564 loss= 0.9177444949173621\n",
            "iteration 1296: weight=1.0207095881538766 loss= 0.9183125538767475\n",
            "iteration 1297: weight=1.0205632376765794 loss= 0.9188771955505064\n",
            "iteration 1298: weight=1.0204178140506286 loss= 0.9194384362686563\n",
            "iteration 1299: weight=1.020273312199694 loss= 0.9199962923199099\n",
            "iteration 1300: weight=1.0201297270697893 loss= 0.9205507799514088\n",
            "iteration 1301: weight=1.0199870536292082 loss= 0.9211019153684595\n",
            "iteration 1302: weight=1.0198452868684613 loss= 0.9216497147342747\n",
            "iteration 1303: weight=1.0197044218002136 loss= 0.9221941941697208\n",
            "iteration 1304: weight=1.0195644534592203 loss= 0.9227353697530686\n",
            "iteration 1305: weight=1.0194253769022643 loss= 0.9232732575197508\n",
            "iteration 1306: weight=1.0192871872080922 loss= 0.9238078734621229\n",
            "iteration 1307: weight=1.0191498794773513 loss= 0.9243392335292312\n",
            "iteration 1308: weight=1.019013448832526 loss= 0.9248673536265833\n",
            "iteration 1309: weight=1.0188778904178744 loss= 0.9253922496159244\n",
            "iteration 1310: weight=1.018743199399365 loss= 0.9259139373150197\n",
            "iteration 1311: weight=1.0186093709646131 loss= 0.9264324324974376\n",
            "iteration 1312: weight=1.018476400322817 loss= 0.9269477508923418\n",
            "iteration 1313: weight=1.0183442827046947 loss= 0.9274599081842878\n",
            "iteration 1314: weight=1.01821301336242 loss= 0.9279689200130203\n",
            "iteration 1315: weight=1.01808258756956 loss= 0.9284748019732784\n",
            "iteration 1316: weight=1.0179530006210096 loss= 0.928977569614603\n",
            "iteration 1317: weight=1.0178242478329294 loss= 0.9294772384411536\n",
            "iteration 1318: weight=1.0176963245426813 loss= 0.9299738239115214\n",
            "iteration 1319: weight=1.0175692261087654 loss= 0.9304673414385543\n",
            "iteration 1320: weight=1.0174429479107554 loss= 0.9309578063891819\n",
            "iteration 1321: weight=1.0173174853492355 loss= 0.9314452340842477\n",
            "iteration 1322: weight=1.017192833845737 loss= 0.931929639798342\n",
            "iteration 1323: weight=1.0170689888426736 loss= 0.9324110387596405\n",
            "iteration 1324: weight=1.0169459458032784 loss= 0.9328894461497509\n",
            "iteration 1325: weight=1.0168237002115401 loss= 0.9333648771035572\n",
            "iteration 1326: weight=1.016702247572139 loss= 0.9338373467090706\n",
            "iteration 1327: weight=1.0165815834103833 loss= 0.9343068700072881\n",
            "iteration 1328: weight=1.0164617032721452 loss= 0.9347734619920488\n",
            "iteration 1329: weight=1.0163426027237978 loss= 0.9352371376098997\n",
            "iteration 1330: weight=1.0162242773521504 loss= 0.9356979117599602\n",
            "iteration 1331: weight=1.0161067227643854 loss= 0.9361557992937966\n",
            "iteration 1332: weight=1.0159899345879946 loss= 0.9366108150152934\n",
            "iteration 1333: weight=1.0158739084707147 loss= 0.937062973680535\n",
            "iteration 1334: weight=1.0157586400804643 loss= 0.9375122899976878\n",
            "iteration 1335: weight=1.0156441251052801 loss= 0.937958778626885\n",
            "iteration 1336: weight=1.015530359253253 loss= 0.9384024541801181\n",
            "iteration 1337: weight=1.015417338252464 loss= 0.9388433312211285\n",
            "iteration 1338: weight=1.0153050578509215 loss= 0.9392814242653074\n",
            "iteration 1339: weight=1.0151935138164965 loss= 0.9397167477795941\n",
            "iteration 1340: weight=1.0150827019368598 loss= 0.9401493161823824\n",
            "iteration 1341: weight=1.014972618019418 loss= 0.9405791438434254\n",
            "iteration 1342: weight=1.0148632578912498 loss= 0.9410062450837493\n",
            "iteration 1343: weight=1.0147546173990423 loss= 0.9414306341755682\n",
            "iteration 1344: weight=1.0146466924090278 loss= 0.9418523253421994\n",
            "iteration 1345: weight=1.0145394788069202 loss= 0.9422713327579874\n",
            "iteration 1346: weight=1.0144329724978507 loss= 0.9426876705482268\n",
            "iteration 1347: weight=1.0143271694063054 loss= 0.9431013527890921\n",
            "iteration 1348: weight=1.014222065476061 loss= 0.9435123935075662\n",
            "iteration 1349: weight=1.0141176566701215 loss= 0.9439208066813776\n",
            "iteration 1350: weight=1.0140139389706555 loss= 0.9443266062389355\n",
            "iteration 1351: weight=1.0139109083789313 loss= 0.944729806059271\n",
            "iteration 1352: weight=1.013808560915255 loss= 0.945130419971983\n",
            "iteration 1353: weight=1.0137068926189061 loss= 0.9455284617571814\n",
            "iteration 1354: weight=1.0136058995480752 loss= 0.9459239451454404\n",
            "iteration 1355: weight=1.0135055777797997 loss= 0.9463168838177484\n",
            "iteration 1356: weight=1.0134059234099013 loss= 0.9467072914054663\n",
            "iteration 1357: weight=1.0133069325529227 loss= 0.9470951814902833\n",
            "iteration 1358: weight=1.013208601342064 loss= 0.9474805676041813\n",
            "iteration 1359: weight=1.0131109259291202 loss= 0.9478634632293982\n",
            "iteration 1360: weight=1.0130139024844178 loss= 0.9482438817983946\n",
            "iteration 1361: weight=1.0129175271967517 loss= 0.9486218366938245\n",
            "iteration 1362: weight=1.0128217962733224 loss= 0.9489973412485087\n",
            "iteration 1363: weight=1.0127267059396732 loss= 0.9493704087454089\n",
            "iteration 1364: weight=1.0126322524396267 loss= 0.949741052417607\n",
            "iteration 1365: weight=1.012538432035223 loss= 0.9501092854482869\n",
            "iteration 1366: weight=1.0124452410066558 loss= 0.9504751209707153\n",
            "iteration 1367: weight=1.0123526756522103 loss= 0.9508385720682319\n",
            "iteration 1368: weight=1.0122607322882007 loss= 0.9511996517742336\n",
            "iteration 1369: weight=1.0121694072489067 loss= 0.951558373072169\n",
            "iteration 1370: weight=1.012078696886512 loss= 0.9519147488955322\n",
            "iteration 1371: weight=1.011988597571041 loss= 0.9522687921278566\n",
            "iteration 1372: weight=1.0118991056902966 loss= 0.9526205156027175\n",
            "iteration 1373: weight=1.011810217649798 loss= 0.9529699321037289\n",
            "iteration 1374: weight=1.011721929872718 loss= 0.9533170543645502\n",
            "iteration 1375: weight=1.0116342387998207 loss= 0.9536618950688915\n",
            "iteration 1376: weight=1.0115471408893997 loss= 0.9540044668505221\n",
            "iteration 1377: weight=1.0114606326172155 loss= 0.9543447822932799\n",
            "iteration 1378: weight=1.0113747104764337 loss= 0.954682853931085\n",
            "iteration 1379: weight=1.0112893709775626 loss= 0.955018694247956\n",
            "iteration 1380: weight=1.0112046106483916 loss= 0.9553523156780259\n",
            "iteration 1381: weight=1.0111204260339295 loss= 0.9556837306055617\n",
            "iteration 1382: weight=1.0110368136963417 loss= 0.9560129513649864\n",
            "iteration 1383: weight=1.0109537702148892 loss= 0.9563399902409043\n",
            "iteration 1384: weight=1.0108712921858671 loss= 0.9566648594681256\n",
            "iteration 1385: weight=1.0107893762225424 loss= 0.9569875712316934\n",
            "iteration 1386: weight=1.0107080189550923 loss= 0.9573081376669167\n",
            "iteration 1387: weight=1.0106272170305437 loss= 0.9576265708594012\n",
            "iteration 1388: weight=1.0105469671127107 loss= 0.9579428828450823\n",
            "iteration 1389: weight=1.0104672658821343 loss= 0.9582570856102635\n",
            "iteration 1390: weight=1.0103881100360197 loss= 0.958569191091652\n",
            "iteration 1391: weight=1.0103094962881767 loss= 0.9588792111764031\n",
            "iteration 1392: weight=1.0102314213689576 loss= 0.959187157702157\n",
            "iteration 1393: weight=1.0101538820251965 loss= 0.9594930424570862\n",
            "iteration 1394: weight=1.0100768750201483 loss= 0.9597968771799403\n",
            "iteration 1395: weight=1.0100003971334277 loss= 0.9600986735600935\n",
            "iteration 1396: weight=1.0099244451609488 loss= 0.9603984432375943\n",
            "iteration 1397: weight=1.009849015914864 loss= 0.9606961978032155\n",
            "iteration 1398: weight=1.0097741062235035 loss= 0.9609919487985086\n",
            "iteration 1399: weight=1.0096997129313148 loss= 0.9612857077158593\n",
            "iteration 1400: weight=1.0096258328988021 loss= 0.9615774859985405\n",
            "iteration 1401: weight=1.0095524630024664 loss= 0.9618672950407741\n",
            "iteration 1402: weight=1.0094796001347444 loss= 0.9621551461877884\n",
            "iteration 1403: weight=1.0094072412039492 loss= 0.9624410507358812\n",
            "iteration 1404: weight=1.0093353831342096 loss= 0.9627250199324803\n",
            "iteration 1405: weight=1.0092640228654104 loss= 0.9630070649762117\n",
            "iteration 1406: weight=1.009193157353132 loss= 0.9632871970169619\n",
            "iteration 1407: weight=1.0091227835685916 loss= 0.9635654271559494\n",
            "iteration 1408: weight=1.0090528984985823 loss= 0.9638417664457909\n",
            "iteration 1409: weight=1.0089834991454136 loss= 0.9641162258905734\n",
            "iteration 1410: weight=1.0089145825268528 loss= 0.9643888164459281\n",
            "iteration 1411: weight=1.0088461456760647 loss= 0.9646595490191009\n",
            "iteration 1412: weight=1.008778185641552 loss= 0.9649284344690299\n",
            "iteration 1413: weight=1.0087106994870974 loss= 0.9651954836064218\n",
            "iteration 1414: weight=1.0086436842917024 loss= 0.9654607071938287\n",
            "iteration 1415: weight=1.0085771371495302 loss= 0.965724115945729\n",
            "iteration 1416: weight=1.0085110551698455 loss= 0.9659857205286064\n",
            "iteration 1417: weight=1.008445435476956 loss= 0.9662455315610345\n",
            "iteration 1418: weight=1.0083802752101536 loss= 0.9665035596137573\n",
            "iteration 1419: weight=1.0083155715236556 loss= 0.9667598152097773\n",
            "iteration 1420: weight=1.0082513215865463 loss= 0.9670143088244378\n",
            "iteration 1421: weight=1.0081875225827186 loss= 0.9672670508855132\n",
            "iteration 1422: weight=1.0081241717108151 loss= 0.9675180517732958\n",
            "iteration 1423: weight=1.0080612661841708 loss= 0.9677673218206867\n",
            "iteration 1424: weight=1.0079988032307539 loss= 0.968014871313285\n",
            "iteration 1425: weight=1.0079367800931083 loss= 0.9682607104894818\n",
            "iteration 1426: weight=1.0078751940282955 loss= 0.9685048495405524\n",
            "iteration 1427: weight=1.007814042307837 loss= 0.9687472986107512\n",
            "iteration 1428: weight=1.0077533222176562 loss= 0.9689880677974066\n",
            "iteration 1429: weight=1.007693031058021 loss= 0.9692271671510183\n",
            "iteration 1430: weight=1.0076331661434859 loss= 0.9694646066753547\n",
            "iteration 1431: weight=1.0075737248028351 loss= 0.9697003963275528\n",
            "iteration 1432: weight=1.0075147043790251 loss= 0.9699345460182158\n",
            "iteration 1433: weight=1.0074561022291273 loss= 0.970167065611516\n",
            "iteration 1434: weight=1.0073979157242707 loss= 0.9703979649252956\n",
            "iteration 1435: weight=1.0073401422495856 loss= 0.9706272537311708\n",
            "iteration 1436: weight=1.0072827792041459 loss= 0.9708549417546344\n",
            "iteration 1437: weight=1.0072258240009133 loss= 0.9710810386751618\n",
            "iteration 1438: weight=1.00716927406668 loss= 0.9713055541263155\n",
            "iteration 1439: weight=1.0071131268420122 loss= 0.9715284976958527\n",
            "iteration 1440: weight=1.0070573797811937 loss= 0.9717498789258336\n",
            "iteration 1441: weight=1.00700203035217 loss= 0.9719697073127292\n",
            "iteration 1442: weight=1.0069470760364923 loss= 0.9721879923075306\n",
            "iteration 1443: weight=1.00689251432926 loss= 0.9724047433158582\n",
            "iteration 1444: weight=1.0068383427390664 loss= 0.9726199696980763\n",
            "iteration 1445: weight=1.006784558787942 loss= 0.9728336807694024\n",
            "iteration 1446: weight=1.0067311600112994 loss= 0.97304588580002\n",
            "iteration 1447: weight=1.0066781439578767 loss= 0.9732565940151934\n",
            "iteration 1448: weight=1.0066255081896833 loss= 0.9734658145953817\n",
            "iteration 1449: weight=1.0065732502819436 loss= 0.9736735566763529\n",
            "iteration 1450: weight=1.0065213678230418 loss= 0.9738798293493021\n",
            "iteration 1451: weight=1.0064698584144676 loss= 0.9740846416609664\n",
            "iteration 1452: weight=1.0064187196707604 loss= 0.9742880026137427\n",
            "iteration 1453: weight=1.0063679492194546 loss= 0.9744899211658058\n",
            "iteration 1454: weight=1.006317544701025 loss= 0.9746904062312278\n",
            "iteration 1455: weight=1.0062675037688324 loss= 0.9748894666800975\n",
            "iteration 1456: weight=1.0062178240890687 loss= 0.9750871113386396\n",
            "iteration 1457: weight=1.0061685033407024 loss= 0.9752833489893357\n",
            "iteration 1458: weight=1.0061195392154247 loss= 0.9754781883710474\n",
            "iteration 1459: weight=1.0060709294175956 loss= 0.9756716381791375\n",
            "iteration 1460: weight=1.0060226716641891 loss= 0.9758637070655914\n",
            "iteration 1461: weight=1.00597476368474 loss= 0.976054403639142\n",
            "iteration 1462: weight=1.00592720322129 loss= 0.9762437364653938\n",
            "iteration 1463: weight=1.005879988028334 loss= 0.9764317140669457\n",
            "iteration 1464: weight=1.0058331158727662 loss= 0.9766183449235174\n",
            "iteration 1465: weight=1.0057865845338276 loss= 0.9768036374720754\n",
            "iteration 1466: weight=1.005740391803052 loss= 0.976987600106958\n",
            "iteration 1467: weight=1.0056945354842133 loss= 0.9771702411800018\n",
            "iteration 1468: weight=1.0056490133932723 loss= 0.9773515690006708\n",
            "iteration 1469: weight=1.0056038233583238 loss= 0.9775315918361803\n",
            "iteration 1470: weight=1.0055589632195443 loss= 0.9777103179116301\n",
            "iteration 1471: weight=1.0055144308291386 loss= 0.9778877554101277\n",
            "iteration 1472: weight=1.005470224051288 loss= 0.978063912472923\n",
            "iteration 1473: weight=1.005426340762098 loss= 0.978238797199533\n",
            "iteration 1474: weight=1.0053827788495455 loss= 0.9784124176478736\n",
            "iteration 1475: weight=1.005339536213427 loss= 0.9785847818343903\n",
            "iteration 1476: weight=1.005296610765307 loss= 0.9787558977341901\n",
            "iteration 1477: weight=1.0052540004284656 loss= 0.9789257732811688\n",
            "iteration 1478: weight=1.0052117031378471 loss= 0.979094416368147\n",
            "iteration 1479: weight=1.0051697168400087 loss= 0.9792618348469997\n",
            "iteration 1480: weight=1.0051280394930686 loss= 0.9794280365287888\n",
            "iteration 1481: weight=1.005086669066655 loss= 0.9795930291838955\n",
            "iteration 1482: weight=1.0050456035418545 loss= 0.9797568205421551\n",
            "iteration 1483: weight=1.005004840911162 loss= 0.9799194182929885\n",
            "iteration 1484: weight=1.0049643791784288 loss= 0.980080830085536\n",
            "iteration 1485: weight=1.0049242163588126 loss= 0.9802410635287935\n",
            "iteration 1486: weight=1.0048843504787264 loss= 0.9804001261917433\n",
            "iteration 1487: weight=1.0048447795757882 loss= 0.9805580256034906\n",
            "iteration 1488: weight=1.0048055016987707 loss= 0.9807147692533991\n",
            "iteration 1489: weight=1.0047665149075506 loss= 0.9808703645912248\n",
            "iteration 1490: weight=1.0047278172730596 loss= 0.981024819027253\n",
            "iteration 1491: weight=1.0046894068772334 loss= 0.9811781399324314\n",
            "iteration 1492: weight=1.0046512818129623 loss= 0.9813303346385074\n",
            "iteration 1493: weight=1.0046134401840414 loss= 0.9814814104381653\n",
            "iteration 1494: weight=1.0045758801051212 loss= 0.9816313745851613\n",
            "iteration 1495: weight=1.0045385997016585 loss= 0.9817802342944609\n",
            "iteration 1496: weight=1.0045015971098663 loss= 0.9819279967423736\n",
            "iteration 1497: weight=1.0044648704766654 loss= 0.982074669066693\n",
            "iteration 1498: weight=1.0044284179596352 loss= 0.9822202583668318\n",
            "iteration 1499: weight=1.004392237726965 loss= 0.98236477170396\n",
            "iteration 1500: weight=1.0043563279574046 loss= 0.9825082161011409\n",
            "iteration 1501: weight=1.0043206868402168 loss= 0.9826505985434715\n",
            "iteration 1502: weight=1.004285312575128 loss= 0.9827919259782176\n",
            "iteration 1503: weight=1.0042502033722807 loss= 0.9829322053149538\n",
            "iteration 1504: weight=1.0042153574521844 loss= 0.9830714434257003\n",
            "iteration 1505: weight=1.0041807730456687 loss= 0.9832096471450612\n",
            "iteration 1506: weight=1.0041464483938345 loss= 0.9833468232703629\n",
            "iteration 1507: weight=1.0041123817480067 loss= 0.983482978561793\n",
            "iteration 1508: weight=1.0040785713696865 loss= 0.9836181197425388\n",
            "iteration 1509: weight=1.0040450155305038 loss= 0.9837522534989246\n",
            "iteration 1510: weight=1.0040117125121704 loss= 0.9838853864805527\n",
            "iteration 1511: weight=1.0039786606064318 loss= 0.9840175253004397\n",
            "iteration 1512: weight=1.0039458581150214 loss= 0.9841486765351575\n",
            "iteration 1513: weight=1.0039133033496126 loss= 0.9842788467249701\n",
            "iteration 1514: weight=1.0038809946317726 loss= 0.984408042373974\n",
            "iteration 1515: weight=1.0038489302929157 loss= 0.984536269950237\n",
            "iteration 1516: weight=1.0038171086742562 loss= 0.9846635358859362\n",
            "iteration 1517: weight=1.0037855281267631 loss= 0.9847898465774995\n",
            "iteration 1518: weight=1.003754187011113 loss= 0.9849152083857415\n",
            "iteration 1519: weight=1.0037230836976447 loss= 0.9850396276360054\n",
            "iteration 1520: weight=1.003692216566313 loss= 0.9851631106182998\n",
            "iteration 1521: weight=1.0036615840066423 loss= 0.9852856635874385\n",
            "iteration 1522: weight=1.0036311844176824 loss= 0.9854072927631816\n",
            "iteration 1523: weight=1.0036010162079618 loss= 0.9855280043303712\n",
            "iteration 1524: weight=1.0035710777954427 loss= 0.9856478044390727\n",
            "iteration 1525: weight=1.0035413676074758 loss= 0.9857666992047134\n",
            "iteration 1526: weight=1.003511884080755 loss= 0.9858846947082219\n",
            "iteration 1527: weight=1.0034826256612734 loss= 0.9860017969961663\n",
            "iteration 1528: weight=1.0034535908042772 loss= 0.9861180120808924\n",
            "iteration 1529: weight=1.0034247779742218 loss= 0.9862333459406647\n",
            "iteration 1530: weight=1.0033961856447275 loss= 0.9863478045198036\n",
            "iteration 1531: weight=1.0033678122985343 loss= 0.9864613937288237\n",
            "iteration 1532: weight=1.0033396564274586 loss= 0.9865741194445753\n",
            "iteration 1533: weight=1.0033117165323484 loss= 0.9866859875103795\n",
            "iteration 1534: weight=1.00328399112304 loss= 0.9867970037361689\n",
            "iteration 1535: weight=1.0032564787183136 loss= 0.9869071738986248\n",
            "iteration 1536: weight=1.0032291778458502 loss= 0.987016503741317\n",
            "iteration 1537: weight=1.0032020870421874 loss= 0.9871249989748397\n",
            "iteration 1538: weight=1.003175204852677 loss= 0.9872326652769534\n",
            "iteration 1539: weight=1.0031485298314406 loss= 0.987339508292718\n",
            "iteration 1540: weight=1.003122060541328 loss= 0.9874455336346353\n",
            "iteration 1541: weight=1.0030957955538726 loss= 0.9875507468827828\n",
            "iteration 1542: weight=1.0030697334492498 loss= 0.987655153584955\n",
            "iteration 1543: weight=1.0030438728162336 loss= 0.9877587592567987\n",
            "iteration 1544: weight=1.003018212252155 loss= 0.987861569381951\n",
            "iteration 1545: weight=1.0029927503628586 loss= 0.9879635894121765\n",
            "iteration 1546: weight=1.0029674857626611 loss= 0.9880648247675031\n",
            "iteration 1547: weight=1.002942417074309 loss= 0.9881652808363618\n",
            "iteration 1548: weight=1.0029175429289365 loss= 0.9882649629757204\n",
            "iteration 1549: weight=1.0028928619660236 loss= 0.9883638765112226\n",
            "iteration 1550: weight=1.002868372833355 loss= 0.9884620267373236\n",
            "iteration 1551: weight=1.002844074186978 loss= 0.9885594189174247\n",
            "iteration 1552: weight=1.0028199646911615 loss= 0.9886560582840123\n",
            "iteration 1553: weight=1.0027960430183547 loss= 0.9887519500387915\n",
            "iteration 1554: weight=1.0027723078491457 loss= 0.9888470993528232\n",
            "iteration 1555: weight=1.002748757872221 loss= 0.9889415113666589\n",
            "iteration 1556: weight=1.0027253917843242 loss= 0.9890351911904766\n",
            "iteration 1557: weight=1.002702208290216 loss= 0.9891281439042156\n",
            "iteration 1558: weight=1.002679206102633 loss= 0.9892203745577111\n",
            "iteration 1559: weight=1.002656383942248 loss= 0.9893118881708293\n",
            "iteration 1560: weight=1.0026337405376287 loss= 0.9894026897336026\n",
            "iteration 1561: weight=1.002611274625199 loss= 0.9894927842063636\n",
            "iteration 1562: weight=1.0025889849491978 loss= 0.989582176519877\n",
            "iteration 1563: weight=1.0025668702616402 loss= 0.9896708715754775\n",
            "iteration 1564: weight=1.002544929322277 loss= 0.9897588742451997\n",
            "iteration 1565: weight=1.0025231608985556 loss= 0.9898461893719135\n",
            "iteration 1566: weight=1.0025015637655805 loss= 0.9899328217694574\n",
            "iteration 1567: weight=1.0024801367060743 loss= 0.9900187762227709\n",
            "iteration 1568: weight=1.0024588785103379 loss= 0.9901040574880262\n",
            "iteration 1569: weight=1.0024377879762123 loss= 0.9901886702927629\n",
            "iteration 1570: weight=1.0024168639090396 loss= 0.9902726193360186\n",
            "iteration 1571: weight=1.0023961051216241 loss= 0.9903559092884608\n",
            "iteration 1572: weight=1.002375510434194 loss= 0.9904385447925189\n",
            "iteration 1573: weight=1.0023550786743625 loss= 0.9905205304625158\n",
            "iteration 1574: weight=1.00233480867709 loss= 0.9906018708847999\n",
            "iteration 1575: weight=1.0023146992846463 loss= 0.9906825706178743\n",
            "iteration 1576: weight=1.0022947493465717 loss= 0.990762634192528\n",
            "iteration 1577: weight=1.00227495771964 loss= 0.9908420661119676\n",
            "iteration 1578: weight=1.0022553232678202 loss= 0.9909208708519449\n",
            "iteration 1579: weight=1.0022358448622395 loss= 0.9909990528608887\n",
            "iteration 1580: weight=1.0022165213811454 loss= 0.9910766165600341\n",
            "iteration 1581: weight=1.0021973517098686 loss= 0.9911535663435509\n",
            "iteration 1582: weight=1.002178334740786 loss= 0.9912299065786732\n",
            "iteration 1583: weight=1.0021594693732834 loss= 0.9913056416058277\n",
            "iteration 1584: weight=1.002140754513719 loss= 0.9913807757387632\n",
            "iteration 1585: weight=1.0021221890753866 loss= 0.9914553132646757\n",
            "iteration 1586: weight=1.0021037719784784 loss= 0.9915292584443403\n",
            "iteration 1587: weight=1.0020855021500497 loss= 0.9916026155122359\n",
            "iteration 1588: weight=1.0020673785239815 loss= 0.9916753886766728\n",
            "iteration 1589: weight=1.002049400040945 loss= 0.9917475821199195\n",
            "iteration 1590: weight=1.0020315656483656 loss= 0.9918191999983312\n",
            "iteration 1591: weight=1.0020138743003864 loss= 0.9918902464424721\n",
            "iteration 1592: weight=1.001996324957833 loss= 0.9919607255572456\n",
            "iteration 1593: weight=1.0019789165881778 loss= 0.9920306414220171\n",
            "iteration 1594: weight=1.0019616481655045 loss= 0.9920999980907406\n",
            "iteration 1595: weight=1.0019445186704725 loss= 0.9921687995920829\n",
            "iteration 1596: weight=1.0019275270902823 loss= 0.9922370499295492\n",
            "iteration 1597: weight=1.0019106724186397 loss= 0.9923047530816058\n",
            "iteration 1598: weight=1.0018939536557214 loss= 0.9923719130018065\n",
            "iteration 1599: weight=1.0018773698081402 loss= 0.9924385336189144\n",
            "iteration 1600: weight=1.0018609198889101 loss= 0.9925046188370251\n",
            "iteration 1601: weight=1.001844602917412 loss= 0.9925701725356912\n",
            "iteration 1602: weight=1.0018284179193588 loss= 0.9926351985700439\n",
            "iteration 1603: weight=1.0018123639267622 loss= 0.992699700770916\n",
            "iteration 1604: weight=1.0017964399778976 loss= 0.9927636829449633\n",
            "iteration 1605: weight=1.0017806451172706 loss= 0.9928271488747864\n",
            "iteration 1606: weight=1.0017649783955833 loss= 0.9928901023190522\n",
            "iteration 1607: weight=1.0017494388697001 loss= 0.9929525470126143\n",
            "iteration 1608: weight=1.0017340256026146 loss= 0.9930144866666347\n",
            "iteration 1609: weight=1.001718737663416 loss= 0.9930759249687037\n",
            "iteration 1610: weight=1.0017035741272557 loss= 0.9931368655829586\n",
            "iteration 1611: weight=1.0016885340753143 loss= 0.9931973121502052\n",
            "iteration 1612: weight=1.0016736165947688 loss= 0.9932572682880366\n",
            "iteration 1613: weight=1.0016588207787591 loss= 0.9933167375909498\n",
            "iteration 1614: weight=1.0016441457263559 loss= 0.9933757236304677\n",
            "iteration 1615: weight=1.0016295905425274 loss= 0.9934342299552545\n",
            "iteration 1616: weight=1.0016151543381078 loss= 0.9934922600912355\n",
            "iteration 1617: weight=1.0016008362297637 loss= 0.9935498175417126\n",
            "iteration 1618: weight=1.001586635339963 loss= 0.9936069057874833\n",
            "iteration 1619: weight=1.001572550796942 loss= 0.9936635282869563\n",
            "iteration 1620: weight=1.0015585817346742 loss= 0.993719688476268\n",
            "iteration 1621: weight=1.0015447272928377 loss= 0.993775389769398\n",
            "iteration 1622: weight=1.0015309866167843 loss= 0.9938306355582861\n",
            "iteration 1623: weight=1.0015173588575073 loss= 0.9938854292129459\n",
            "iteration 1624: weight=1.0015038431716101 loss= 0.9939397740815809\n",
            "iteration 1625: weight=1.0014904387212753 loss= 0.9939936734906987\n",
            "iteration 1626: weight=1.0014771446742337 loss= 0.9940471307452263\n",
            "iteration 1627: weight=1.0014639602037323 loss= 0.9941001491286199\n",
            "iteration 1628: weight=1.0014508844885044 loss= 0.9941527319029833\n",
            "iteration 1629: weight=1.0014379167127385 loss= 0.9942048823091784\n",
            "iteration 1630: weight=1.0014250560660476 loss= 0.994256603566937\n",
            "iteration 1631: weight=1.0014123017434384 loss= 0.9943078988749752\n",
            "iteration 1632: weight=1.0013996529452818 loss= 0.9943587714111044\n",
            "iteration 1633: weight=1.0013871088772817 loss= 0.9944092243323419\n",
            "iteration 1634: weight=1.0013746687504452 loss= 0.9944592607750231\n",
            "iteration 1635: weight=1.0013623317810532 loss= 0.9945088838549129\n",
            "iteration 1636: weight=1.0013500971906295 loss= 0.994558096667314\n",
            "iteration 1637: weight=1.0013379642059117 loss= 0.9946069022871785\n",
            "iteration 1638: weight=1.0013259320588217 loss= 0.9946553037692184\n",
            "iteration 1639: weight=1.001313999986436 loss= 0.9947033041480117\n",
            "iteration 1640: weight=1.001302167230956 loss= 0.9947509064381134\n",
            "iteration 1641: weight=1.0012904330396797 loss= 0.994798113634165\n",
            "iteration 1642: weight=1.0012787966649717 loss= 0.9948449287110008\n",
            "iteration 1643: weight=1.0012672573642345 loss= 0.9948913546237548\n",
            "iteration 1644: weight=1.00125581439988 loss= 0.9949373943079708\n",
            "iteration 1645: weight=1.0012444670393004 loss= 0.9949830506797078\n",
            "iteration 1646: weight=1.00123321455484 loss= 0.9950283266356461\n",
            "iteration 1647: weight=1.001222056223766 loss= 0.9950732250531933\n",
            "iteration 1648: weight=1.001210991328241 loss= 0.9951177487905924\n",
            "iteration 1649: weight=1.0012000191552946 loss= 0.9951619006870244\n",
            "iteration 1650: weight=1.0011891389967948 loss= 0.9952056835627141\n",
            "iteration 1651: weight=1.0011783501494207 loss= 0.9952491002190355\n",
            "iteration 1652: weight=1.0011676519146342 loss= 0.9952921534386158\n",
            "iteration 1653: weight=1.0011570435986525 loss= 0.9953348459854382\n",
            "iteration 1654: weight=1.0011465245124205 loss= 0.9953771806049466\n",
            "iteration 1655: weight=1.001136093971583 loss= 0.9954191600241484\n",
            "iteration 1656: weight=1.0011257512964582 loss= 0.9954607869517171\n",
            "iteration 1657: weight=1.0011154958120096 loss= 0.9955020640780932\n",
            "iteration 1658: weight=1.0011053268478194 loss= 0.9955429940755879\n",
            "iteration 1659: weight=1.0010952437380611 loss= 0.9955835795984845\n",
            "iteration 1660: weight=1.0010852458214734 loss= 0.9956238232831385\n",
            "iteration 1661: weight=1.0010753324413326 loss= 0.9956637277480785\n",
            "iteration 1662: weight=1.0010655029454265 loss= 0.9957032955941071\n",
            "iteration 1663: weight=1.001055756686028 loss= 0.9957425294044008\n",
            "iteration 1664: weight=1.0010460930198686 loss= 0.995781431744608\n",
            "iteration 1665: weight=1.0010365113081117 loss= 0.9958200051629506\n",
            "iteration 1666: weight=1.001027010916327 loss= 0.9958582521903208\n",
            "iteration 1667: weight=1.0010175912144648 loss= 0.9958961753403808\n",
            "iteration 1668: weight=1.001008251576829 loss= 0.99593377710966\n",
            "iteration 1669: weight=1.000998991382053 loss= 0.9959710599776526\n",
            "iteration 1670: weight=1.000989810013072 loss= 0.9960080264069139\n",
            "iteration 1671: weight=1.0009807068570993 loss= 0.9960446788431596\n",
            "iteration 1672: weight=1.0009716813056 loss= 0.9960810197153611\n",
            "iteration 1673: weight=1.0009627327542656 loss= 0.9961170514358386\n",
            "iteration 1674: weight=1.0009538606029895 loss= 0.996152776400362\n",
            "iteration 1675: weight=1.0009450642558413 loss= 0.9961881969882417\n",
            "iteration 1676: weight=1.000936343121042 loss= 0.9962233155624255\n",
            "iteration 1677: weight=1.0009276966109397 loss= 0.996258134469593\n",
            "iteration 1678: weight=1.0009191241419841 loss= 0.9962926560402489\n",
            "iteration 1679: weight=1.0009106251347024 loss= 0.996326882588817\n",
            "iteration 1680: weight=1.0009021990136748 loss= 0.9963608164137342\n",
            "iteration 1681: weight=1.0008938452075102 loss= 0.9963944597975418\n",
            "iteration 1682: weight=1.0008855631488216 loss= 0.996427815006979\n",
            "iteration 1683: weight=1.000877352274202 loss= 0.9964608842930758\n",
            "iteration 1684: weight=1.0008692120242013 loss= 0.996493669891244\n",
            "iteration 1685: weight=1.000861141843301 loss= 0.9965261740213668\n",
            "iteration 1686: weight=1.0008531411798918 loss= 0.9965583988878927\n",
            "iteration 1687: weight=1.0008452094862486 loss= 0.9965903466799242\n",
            "iteration 1688: weight=1.000837346218508 loss= 0.9966220195713084\n",
            "iteration 1689: weight=1.0008295508366445 loss= 0.9966534197207264\n",
            "iteration 1690: weight=1.000821822804447 loss= 0.9966845492717844\n",
            "iteration 1691: weight=1.0008141615894957 loss= 0.9967154103530997\n",
            "iteration 1692: weight=1.000806566663139 loss= 0.9967460050783926\n",
            "iteration 1693: weight=1.0007990375004707 loss= 0.9967763355465726\n",
            "iteration 1694: weight=1.0007915735803068 loss= 0.9968064038418257\n",
            "iteration 1695: weight=1.000784174385163 loss= 0.996836212033705\n",
            "iteration 1696: weight=1.000776839401232 loss= 0.996865762177213\n",
            "iteration 1697: weight=1.0007695681183606 loss= 0.9968950563128933\n",
            "iteration 1698: weight=1.000762360030028 loss= 0.9969240964669128\n",
            "iteration 1699: weight=1.0007552146333223 loss= 0.9969528846511497\n",
            "iteration 1700: weight=1.0007481314289197 loss= 0.9969814228632804\n",
            "iteration 1701: weight=1.0007411099210615 loss= 0.9970097130868609\n",
            "iteration 1702: weight=1.0007341496175322 loss= 0.9970377572914145\n",
            "iteration 1703: weight=1.0007272500296378 loss= 0.9970655574325151\n",
            "iteration 1704: weight=1.0007204106721839 loss= 0.9970931154518712\n",
            "iteration 1705: weight=1.0007136310634537 loss= 0.997120433277411\n",
            "iteration 1706: weight=1.0007069107251874 loss= 0.9971475128233642\n",
            "iteration 1707: weight=1.0007002491825598 loss= 0.9971743559903441\n",
            "iteration 1708: weight=1.0006936459641593 loss= 0.9972009646654313\n",
            "iteration 1709: weight=1.0006871006019666 loss= 0.997227340722257\n",
            "iteration 1710: weight=1.0006806126313337 loss= 0.9972534860210823\n",
            "iteration 1711: weight=1.0006741815909623 loss= 0.997279402408881\n",
            "iteration 1712: weight=1.000667807022884 loss= 0.997305091719421\n",
            "iteration 1713: weight=1.0006614884724385 loss= 0.9973305557733431\n",
            "iteration 1714: weight=1.0006552254882533 loss= 0.9973557963782426\n",
            "iteration 1715: weight=1.0006490176222225 loss= 0.9973808153287488\n",
            "iteration 1716: weight=1.0006428644294874 loss= 0.9974056144066059\n",
            "iteration 1717: weight=1.000636765468415 loss= 0.9974301953807494\n",
            "iteration 1718: weight=1.0006307203005782 loss= 0.9974545600073872\n",
            "iteration 1719: weight=1.000624728490736 loss= 0.9974787100300775\n",
            "iteration 1720: weight=1.0006187896068122 loss= 0.9975026471798049\n",
            "iteration 1721: weight=1.000612903219877 loss= 0.9975263731750613\n",
            "iteration 1722: weight=1.0006070689041255 loss= 0.9975498897219199\n",
            "iteration 1723: weight=1.0006012862368592 loss= 0.9975731985141153\n",
            "iteration 1724: weight=1.0005955547984655 loss= 0.9975963012331178\n",
            "iteration 1725: weight=1.0005898741723986 loss= 0.9976191995482099\n",
            "iteration 1726: weight=1.0005842439451598 loss= 0.9976418951165625\n",
            "iteration 1727: weight=1.0005786637062777 loss= 0.9976643895833107\n",
            "iteration 1728: weight=1.00057313304829 loss= 0.9976866845816291\n",
            "iteration 1729: weight=1.0005676515667234 loss= 0.9977087817328038\n",
            "iteration 1730: weight=1.0005622188600747 loss= 0.997730682646311\n",
            "iteration 1731: weight=1.0005568345297917 loss= 0.9977523889198878\n",
            "iteration 1732: weight=1.000551498180255 loss= 0.9977739021396074\n",
            "iteration 1733: weight=1.0005462094187583 loss= 0.9977952238799512\n",
            "iteration 1734: weight=1.0005409678554902 loss= 0.9978163557038835\n",
            "iteration 1735: weight=1.0005357731035156 loss= 0.997837299162922\n",
            "iteration 1736: weight=1.000530624778757 loss= 0.9978580557972114\n",
            "iteration 1737: weight=1.0005255224999765 loss= 0.9978786271355949\n",
            "iteration 1738: weight=1.0005204658887568 loss= 0.9978990146956859\n",
            "iteration 1739: weight=1.0005154545694837 loss= 0.9979192199839384\n",
            "iteration 1740: weight=1.0005104881693274 loss= 0.9979392444957182\n",
            "iteration 1741: weight=1.0005055663182254 loss= 0.9979590897153744\n",
            "iteration 1742: weight=1.0005006886488632 loss= 0.9979787571163069\n",
            "iteration 1743: weight=1.0004958547966578 loss= 0.9979982481610395\n",
            "iteration 1744: weight=1.0004910643997391 loss= 0.9980175643012862\n",
            "iteration 1745: weight=1.0004863170989327 loss= 0.9980367069780223\n",
            "iteration 1746: weight=1.0004816125377423 loss= 0.9980556776215522\n",
            "iteration 1747: weight=1.0004769503623319 loss= 0.998074477651577\n",
            "iteration 1748: weight=1.0004723302215088 loss= 0.9980931084772651\n",
            "iteration 1749: weight=1.0004677517667067 loss= 0.9981115714973173\n",
            "iteration 1750: weight=1.0004632146519672 loss= 0.9981298681000343\n",
            "iteration 1751: weight=1.0004587185339247 loss= 0.9981479996633864\n",
            "iteration 1752: weight=1.0004542630717874 loss= 0.9981659675550748\n",
            "iteration 1753: weight=1.0004498479273218 loss= 0.9981837731326039\n",
            "iteration 1754: weight=1.0004454727648353 loss= 0.9982014177433435\n",
            "iteration 1755: weight=1.0004411372511597 loss= 0.9982189027245956\n",
            "iteration 1756: weight=1.0004368410556344 loss= 0.9982362294036587\n",
            "iteration 1757: weight=1.0004325838500898 loss= 0.9982533990978941\n",
            "iteration 1758: weight=1.0004283653088315 loss= 0.9982704131147901\n",
            "iteration 1759: weight=1.000424185108623 loss= 0.9982872727520252\n",
            "iteration 1760: weight=1.0004200429286703 loss= 0.9983039792975332\n",
            "iteration 1761: weight=1.0004159384506048 loss= 0.9983205340295667\n",
            "iteration 1762: weight=1.0004118713584682 loss= 0.9983369382167594\n",
            "iteration 1763: weight=1.0004078413386959 loss= 0.9983531931181909\n",
            "iteration 1764: weight=1.0004038480801012 loss= 0.9983692999834467\n",
            "iteration 1765: weight=1.0003998912738594 loss= 0.9983852600526825\n",
            "iteration 1766: weight=1.0003959706134924 loss= 0.9984010745566859\n",
            "iteration 1767: weight=1.0003920857948527 loss= 0.9984167447169374\n",
            "iteration 1768: weight=1.0003882365161083 loss= 0.9984322717456712\n",
            "iteration 1769: weight=1.0003844224777263 loss= 0.9984476568459365\n",
            "iteration 1770: weight=1.0003806433824585 loss= 0.9984629012116604\n",
            "iteration 1771: weight=1.0003768989353254 loss= 0.9984780060277045\n",
            "iteration 1772: weight=1.0003731888436016 loss= 0.9984929724699281\n",
            "iteration 1773: weight=1.0003695128168002 loss= 0.9985078017052456\n",
            "iteration 1774: weight=1.0003658705666578 loss= 0.9985224948916863\n",
            "iteration 1775: weight=1.0003622618071197 loss= 0.9985370531784549\n",
            "iteration 1776: weight=1.0003586862543252 loss= 0.9985514777059886\n",
            "iteration 1777: weight=1.000355143626592 loss= 0.9985657696060153\n",
            "iteration 1778: weight=1.0003516336444025 loss= 0.9985799300016138\n",
            "iteration 1779: weight=1.0003481560303884 loss= 0.9985939600072695\n",
            "iteration 1780: weight=1.0003447105093166 loss= 0.9986078607289324\n",
            "iteration 1781: weight=1.0003412968080745 loss= 0.9986216332640745\n",
            "iteration 1782: weight=1.0003379146556557 loss= 0.9986352787017468\n",
            "iteration 1783: weight=1.0003345637831458 loss= 0.9986487981226353\n",
            "iteration 1784: weight=1.0003312439237078 loss= 0.9986621925991168\n",
            "iteration 1785: weight=1.0003279548125683 loss= 0.998675463195317\n",
            "iteration 1786: weight=1.0003246961870034 loss= 0.9986886109671631\n",
            "iteration 1787: weight=1.0003214677863246 loss= 0.998701636962442\n",
            "iteration 1788: weight=1.000318269351865 loss= 0.9987145422208522\n",
            "iteration 1789: weight=1.0003151006269657 loss= 0.998727327774061\n",
            "iteration 1790: weight=1.000311961356961 loss= 0.9987399946457576\n",
            "iteration 1791: weight=1.0003088512891665 loss= 0.9987525438517085\n",
            "iteration 1792: weight=1.0003057701728637 loss= 0.9987649763998094\n",
            "iteration 1793: weight=1.000302717759288 loss= 0.9987772932901396\n",
            "iteration 1794: weight=1.000299693801614 loss= 0.9987894955150151\n",
            "iteration 1795: weight=1.0002966980549433 loss= 0.998801584059043\n",
            "iteration 1796: weight=1.00029373027629 loss= 0.9988135598991702\n",
            "iteration 1797: weight=1.0002907902245688 loss= 0.998825424004741\n",
            "iteration 1798: weight=1.0002878776605808 loss= 0.9988371773375436\n",
            "iteration 1799: weight=1.000284992347001 loss= 0.9988488208518667\n",
            "iteration 1800: weight=1.0002821340483652 loss= 0.9988603554945475\n",
            "iteration 1801: weight=1.0002793025310572 loss= 0.9988717822050243\n",
            "iteration 1802: weight=1.0002764975632958 loss= 0.9988831019153864\n",
            "iteration 1803: weight=1.0002737189151223 loss= 0.9988943155504266\n",
            "iteration 1804: weight=1.0002709663583873 loss= 0.9989054240276888\n",
            "iteration 1805: weight=1.0002682396667388 loss= 0.9989164282575203\n",
            "iteration 1806: weight=1.0002655386156094 loss= 0.9989273291431199\n",
            "iteration 1807: weight=1.0002628629822032 loss= 0.9989381275805879\n",
            "iteration 1808: weight=1.000260212545485 loss= 0.9989488244589767\n",
            "iteration 1809: weight=1.0002575870861659 loss= 0.9989594206603356\n",
            "iteration 1810: weight=1.000254986386693 loss= 0.9989699170597645\n",
            "iteration 1811: weight=1.0002524102312362 loss= 0.9989803145254578\n",
            "iteration 1812: weight=1.0002498584056763 loss= 0.9989906139187544\n",
            "iteration 1813: weight=1.0002473306975928 loss= 0.9990008160941865\n",
            "iteration 1814: weight=1.0002448268962523 loss= 0.9990109218995249\n",
            "iteration 1815: weight=1.0002423467925967 loss= 0.9990209321758274\n",
            "iteration 1816: weight=1.0002398901792309 loss= 0.9990308477574846\n",
            "iteration 1817: weight=1.0002374568504113 loss= 0.9990406694722689\n",
            "iteration 1818: weight=1.0002350466020347 loss= 0.999050398141378\n",
            "iteration 1819: weight=1.0002326592316255 loss= 0.9990600345794819\n",
            "iteration 1820: weight=1.0002302945383252 loss= 0.9990695795947702\n",
            "iteration 1821: weight=1.0002279523228808 loss= 0.9990790339889966\n",
            "iteration 1822: weight=1.0002256323876328 loss= 0.999088398557523\n",
            "iteration 1823: weight=1.0002233345365048 loss= 0.999097674089366\n",
            "iteration 1824: weight=1.0002210585749913 loss= 0.9991068613672416\n",
            "iteration 1825: weight=1.0002188043101472 loss= 0.999115961167609\n",
            "iteration 1826: weight=1.0002165715505764 loss= 0.9991249742607158\n",
            "iteration 1827: weight=1.0002143601064206 loss= 0.9991339014106404\n",
            "iteration 1828: weight=1.0002121697893491 loss= 0.9991427433753384\n",
            "iteration 1829: weight=1.0002100004125467 loss= 0.9991515009066816\n",
            "iteration 1830: weight=1.0002078517907038 loss= 0.9991601747505062\n",
            "iteration 1831: weight=1.000205723740005 loss= 0.9991687656466524\n",
            "iteration 1832: weight=1.0002036160781191 loss= 0.9991772743290086\n",
            "iteration 1833: weight=1.0002015286241877 loss= 0.9991857015255525\n",
            "iteration 1834: weight=1.000199461198815 loss= 0.9991940479583948\n",
            "iteration 1835: weight=1.000197413624057 loss= 0.9992023143438195\n",
            "iteration 1836: weight=1.0001953857234118 loss= 0.9992105013923278\n",
            "iteration 1837: weight=1.0001933773218081 loss= 0.9992186098086764\n",
            "iteration 1838: weight=1.0001913882455957 loss= 0.9992266402919219\n",
            "iteration 1839: weight=1.0001894183225346 loss= 0.9992345935354593\n",
            "iteration 1840: weight=1.0001874673817857 loss= 0.9992424702270652\n",
            "iteration 1841: weight=1.0001855352538997 loss= 0.9992502710489343\n",
            "iteration 1842: weight=1.0001836217708078 loss= 0.9992579966777227\n",
            "iteration 1843: weight=1.000181726765811 loss= 0.9992656477845878\n",
            "iteration 1844: weight=1.0001798500735708 loss= 0.9992732250352253\n",
            "iteration 1845: weight=1.0001779915300983 loss= 0.9992807290899126\n",
            "iteration 1846: weight=1.000176150972746 loss= 0.9992881606035457\n",
            "iteration 1847: weight=1.0001743282401965 loss= 0.9992955202256767\n",
            "iteration 1848: weight=1.0001725231724532 loss= 0.9993028086005554\n",
            "iteration 1849: weight=1.0001707356108314 loss= 0.9993100263671674\n",
            "iteration 1850: weight=1.0001689653979478 loss= 0.9993171741592698\n",
            "iteration 1851: weight=1.0001672123777112 loss= 0.9993242526054318\n",
            "iteration 1852: weight=1.0001654763953136 loss= 0.9993312623290721\n",
            "iteration 1853: weight=1.00016375729722 loss= 0.9993382039484952\n",
            "iteration 1854: weight=1.0001620549311594 loss= 0.9993450780769297\n",
            "iteration 1855: weight=1.0001603691461156 loss= 0.9993518853225652\n",
            "iteration 1856: weight=1.0001586997923178 loss= 0.9993586262885896\n",
            "iteration 1857: weight=1.0001570467212315 loss= 0.9993653015732253\n",
            "iteration 1858: weight=1.0001554097855494 loss= 0.9993719117697646\n",
            "iteration 1859: weight=1.0001537888391825 loss= 0.9993784574666081\n",
            "iteration 1860: weight=1.0001521837372507 loss= 0.9993849392472983\n",
            "iteration 1861: weight=1.000150594336074 loss= 0.9993913576905569\n",
            "iteration 1862: weight=1.000149020493164 loss= 0.9993977133703202\n",
            "iteration 1863: weight=1.0001474620672146 loss= 0.9994040068557735\n",
            "iteration 1864: weight=1.0001459189180932 loss= 0.9994102387113867\n",
            "iteration 1865: weight=1.0001443909068326 loss= 0.9994164094969497\n",
            "iteration 1866: weight=1.0001428778956216 loss= 0.9994225197676055\n",
            "iteration 1867: weight=1.000141379747797 loss= 0.9994285700738857\n",
            "iteration 1868: weight=1.0001398963278343 loss= 0.9994345609617447\n",
            "iteration 1869: weight=1.00013842750134 loss= 0.999440492972593\n",
            "iteration 1870: weight=1.000136973135043 loss= 0.9994463666433322\n",
            "iteration 1871: weight=1.0001355330967854 loss= 0.9994521825063872\n",
            "iteration 1872: weight=1.0001341072555152 loss= 0.9994579410897397\n",
            "iteration 1873: weight=1.0001326954812777 loss= 0.9994636429169631\n",
            "iteration 1874: weight=1.000131297645207 loss= 0.9994692885072523\n",
            "iteration 1875: weight=1.000129913619518 loss= 0.9994748783754585\n",
            "iteration 1876: weight=1.0001285432774987 loss= 0.9994804130321218\n",
            "iteration 1877: weight=1.000127186493501 loss= 0.9994858929835021\n",
            "iteration 1878: weight=1.000125843142934 loss= 0.9994913187316122\n",
            "iteration 1879: weight=1.0001245131022551 loss= 0.9994966907742502\n",
            "iteration 1880: weight=1.0001231962489627 loss= 0.99950200960503\n",
            "iteration 1881: weight=1.000121892461588 loss= 0.9995072757134121\n",
            "iteration 1882: weight=1.000120601619687 loss= 0.9995124895847367\n",
            "iteration 1883: weight=1.0001193236038335 loss= 0.9995176517002544\n",
            "iteration 1884: weight=1.0001180582956104 loss= 0.9995227625371558\n",
            "iteration 1885: weight=1.000116805577603 loss= 0.9995278225686031\n",
            "iteration 1886: weight=1.0001155653333906 loss= 0.9995328322637596\n",
            "iteration 1887: weight=1.0001143374475394 loss= 0.9995377920878226\n",
            "iteration 1888: weight=1.0001131218055948 loss= 0.99954270250205\n",
            "iteration 1889: weight=1.000111918294074 loss= 0.9995475639637926\n",
            "iteration 1890: weight=1.000110726800459 loss= 0.9995523769265219\n",
            "iteration 1891: weight=1.0001095472131882 loss= 0.9995571418398614\n",
            "iteration 1892: weight=1.0001083794216503 loss= 0.9995618591496147\n",
            "iteration 1893: weight=1.000107223316176 loss= 0.9995665292977949\n",
            "iteration 1894: weight=1.0001060787880318 loss= 0.9995711527226538\n",
            "iteration 1895: weight=1.000104945729412 loss= 0.9995757298587098\n",
            "iteration 1896: weight=1.0001038240334317 loss= 0.9995802611367764\n",
            "iteration 1897: weight=1.00010271359412 loss= 0.9995847469839929\n",
            "iteration 1898: weight=1.0001016143064132 loss= 0.9995891878238494\n",
            "iteration 1899: weight=1.000100526066147 loss= 0.9995935840762162\n",
            "iteration 1900: weight=1.0000994487700507 loss= 0.9995979361573716\n",
            "iteration 1901: weight=1.0000983823157392 loss= 0.9996022444800284\n",
            "iteration 1902: weight=1.0000973266017066 loss= 0.9996065094533633\n",
            "iteration 1903: weight=1.0000962815273198 loss= 0.9996107314830432\n",
            "iteration 1904: weight=1.0000952469928113 loss= 0.9996149109712509\n",
            "iteration 1905: weight=1.0000942228992724 loss= 0.9996190483167134\n",
            "iteration 1906: weight=1.000093209148647 loss= 0.9996231439147294\n",
            "iteration 1907: weight=1.0000922056437245 loss= 0.9996271981571937\n",
            "iteration 1908: weight=1.0000912122881336 loss= 0.999631211432625\n",
            "iteration 1909: weight=1.0000902289863358 loss= 0.9996351841261917\n",
            "iteration 1910: weight=1.0000892556436187 loss= 0.9996391166197366\n",
            "iteration 1911: weight=1.0000882921660896 loss= 0.9996430092918048\n",
            "iteration 1912: weight=1.000087338460669 loss= 0.9996468625176679\n",
            "iteration 1913: weight=1.0000863944350844 loss= 0.9996506766693508\n",
            "iteration 1914: weight=1.0000854599978644 loss= 0.9996544521156561\n",
            "iteration 1915: weight=1.0000845350583316 loss= 0.9996581892221875\n",
            "iteration 1916: weight=1.000083619526597 loss= 0.9996618883513779\n",
            "iteration 1917: weight=1.0000827133135535 loss= 0.999665549862513\n",
            "iteration 1918: weight=1.0000818163308702 loss= 0.999669174111755\n",
            "iteration 1919: weight=1.0000809284909857 loss= 0.9996727614521671\n",
            "iteration 1920: weight=1.0000800497071027 loss= 0.9996763122337397\n",
            "iteration 1921: weight=1.0000791798931812 loss= 0.9996798268034115\n",
            "iteration 1922: weight=1.0000783189639333 loss= 0.9996833055050972\n",
            "iteration 1923: weight=1.000077466834817 loss= 0.9996867486797073\n",
            "iteration 1924: weight=1.00007662342203 loss= 0.9996901566651744\n",
            "iteration 1925: weight=1.0000757886425042 loss= 0.9996935297964754\n",
            "iteration 1926: weight=1.0000749624139003 loss= 0.9996968684056564\n",
            "iteration 1927: weight=1.0000741446546009 loss= 0.999700172821853\n",
            "iteration 1928: weight=1.0000733352837057 loss= 0.9997034433713158\n",
            "iteration 1929: weight=1.0000725342210257 loss= 0.9997066803774326\n",
            "iteration 1930: weight=1.0000717413870772 loss= 0.99970988416075\n",
            "iteration 1931: weight=1.0000709567030766 loss= 0.9997130550389977\n",
            "iteration 1932: weight=1.0000701800909342 loss= 0.9997161933271086\n",
            "iteration 1933: weight=1.0000694114732496 loss= 0.9997192993372439\n",
            "iteration 1934: weight=1.0000686507733054 loss= 0.9997223733788122\n",
            "iteration 1935: weight=1.0000678979150623 loss= 0.999725415758493\n",
            "iteration 1936: weight=1.0000671528231535 loss= 0.9997284267802581\n",
            "iteration 1937: weight=1.0000664154228787 loss= 0.9997314067453926\n",
            "iteration 1938: weight=1.0000656856402 loss= 0.9997343559525189\n",
            "iteration 1939: weight=1.0000649634017353 loss= 0.9997372746976136\n",
            "iteration 1940: weight=1.0000642486347544 loss= 0.9997401632740329\n",
            "iteration 1941: weight=1.0000635412671726 loss= 0.9997430219725306\n",
            "iteration 1942: weight=1.0000628412275459 loss= 0.9997458510812803\n",
            "iteration 1943: weight=1.000062148445066 loss= 0.9997486508858962\n",
            "iteration 1944: weight=1.0000614628495552 loss= 0.9997514216694529\n",
            "iteration 1945: weight=1.0000607843714613 loss= 0.9997541637125066\n",
            "iteration 1946: weight=1.000060112941852 loss= 0.9997568772931141\n",
            "iteration 1947: weight=1.0000594484924108 loss= 0.999759562686855\n",
            "iteration 1948: weight=1.000058790955431 loss= 0.9997622201668498\n",
            "iteration 1949: weight=1.0000581402638122 loss= 0.9997648500037816\n",
            "iteration 1950: weight=1.000057496351054 loss= 0.9997674524659121\n",
            "iteration 1951: weight=1.0000568591512515 loss= 0.9997700278191055\n",
            "iteration 1952: weight=1.000056228599091 loss= 0.9997725763268462\n",
            "iteration 1953: weight=1.000055604629845 loss= 0.9997750982502572\n",
            "iteration 1954: weight=1.0000549871793667 loss= 0.9997775938481195\n",
            "iteration 1955: weight=1.000054376184086 loss= 0.9997800633768928\n",
            "iteration 1956: weight=1.0000537715810052 loss= 0.9997825070907331\n",
            "iteration 1957: weight=1.0000531733076932 loss= 0.999784925241511\n",
            "iteration 1958: weight=1.0000525813022814 loss= 0.9997873180788299\n",
            "iteration 1959: weight=1.0000519955034597 loss= 0.9997896858500479\n",
            "iteration 1960: weight=1.000051415850471 loss= 0.9997920288002907\n",
            "iteration 1961: weight=1.000050842283107 loss= 0.9997943471724748\n",
            "iteration 1962: weight=1.000050274741704 loss= 0.9997966412073234\n",
            "iteration 1963: weight=1.0000497131671382 loss= 0.9997989111433828\n",
            "iteration 1964: weight=1.0000491575008212 loss= 0.9998011572170431\n",
            "iteration 1965: weight=1.0000486076846955 loss= 0.9998033796625546\n",
            "iteration 1966: weight=1.0000480636612306 loss= 0.999805578712046\n",
            "iteration 1967: weight=1.0000475253734182 loss= 0.9998077545955399\n",
            "iteration 1968: weight=1.0000469927647682 loss= 0.9998099075409715\n",
            "iteration 1969: weight=1.0000464657793042 loss= 0.9998120377742071\n",
            "iteration 1970: weight=1.0000459443615592 loss= 0.9998141455190577\n",
            "iteration 1971: weight=1.0000454284565718 loss= 0.9998162309973004\n",
            "iteration 1972: weight=1.0000449180098816 loss= 0.9998182944286913\n",
            "iteration 1973: weight=1.0000444129675252 loss= 0.9998203360309841\n",
            "iteration 1974: weight=1.0000439132760321 loss= 0.9998223560199458\n",
            "iteration 1975: weight=1.0000434188824205 loss= 0.9998243546093747\n",
            "iteration 1976: weight=1.0000429297341935 loss= 0.9998263320111153\n",
            "iteration 1977: weight=1.0000424457793349 loss= 0.9998282884350743\n",
            "iteration 1978: weight=1.0000419669663052 loss= 0.9998302240892373\n",
            "iteration 1979: weight=1.0000414932440373 loss= 0.9998321391796842\n",
            "iteration 1980: weight=1.0000410245619336 loss= 0.9998340339106078\n",
            "iteration 1981: weight=1.0000405608698606 loss= 0.9998359084843245\n",
            "iteration 1982: weight=1.0000401021181462 loss= 0.9998377631012942\n",
            "iteration 1983: weight=1.0000396482575753 loss= 0.9998395979601347\n",
            "iteration 1984: weight=1.0000391992393862 loss= 0.9998414132576361\n",
            "iteration 1985: weight=1.000038755015267 loss= 0.9998432091887766\n",
            "iteration 1986: weight=1.0000383155373507 loss= 0.999844985946737\n",
            "iteration 1987: weight=1.000037880758213 loss= 0.9998467437229189\n",
            "iteration 1988: weight=1.0000374506308682 loss= 0.999848482706955\n",
            "iteration 1989: weight=1.0000370251087642 loss= 0.999850203086726\n",
            "iteration 1990: weight=1.0000366041457807 loss= 0.9998519050483777\n",
            "iteration 1991: weight=1.000036187696224 loss= 0.9998535887763313\n",
            "iteration 1992: weight=1.0000357757148248 loss= 0.9998552544533013\n",
            "iteration 1993: weight=1.0000353681567336 loss= 0.9998569022603078\n",
            "iteration 1994: weight=1.0000349649775173 loss= 0.9998585323766919\n",
            "iteration 1995: weight=1.0000345661331564 loss= 0.9998601449801293\n",
            "iteration 1996: weight=1.0000341715800405 loss= 0.9998617402466444\n",
            "iteration 1997: weight=1.0000337812749651 loss= 0.9998633183506256\n",
            "iteration 1998: weight=1.000033395175129 loss= 0.9998648794648377\n",
            "iteration 1999: weight=1.0000330132381297 loss= 0.9998664237604349\n",
            "iteration 2000: weight=1.0000326354219606 loss= 0.9998679514069769\n",
            "iteration 2001: weight=1.000032261685008 loss= 0.9998694625724405\n",
            "iteration 2002: weight=1.0000318919860467 loss= 0.9998709574232336\n",
            "iteration 2003: weight=1.0000315262842376 loss= 0.9998724361242083\n",
            "iteration 2004: weight=1.0000311645391242 loss= 0.9998738988386758\n",
            "iteration 2005: weight=1.000030806710629 loss= 0.9998753457284171\n",
            "iteration 2006: weight=1.0000304527590507 loss= 0.9998767769536979\n",
            "iteration 2007: weight=1.0000301026450606 loss= 0.9998781926732795\n",
            "iteration 2008: weight=1.0000297563297 loss= 0.9998795930444344\n",
            "iteration 2009: weight=1.0000294137743762 loss= 0.9998809782229565\n",
            "iteration 2010: weight=1.0000290749408598 loss= 0.9998823483631759\n",
            "iteration 2011: weight=1.0000287397912817 loss= 0.9998837036179695\n",
            "iteration 2012: weight=1.00002840828813 loss= 0.9998850441387755\n",
            "iteration 2013: weight=1.0000280803942467 loss= 0.9998863700756029\n",
            "iteration 2014: weight=1.0000277560728243 loss= 0.9998876815770472\n",
            "iteration 2015: weight=1.000027435287404 loss= 0.9998889787903009\n",
            "iteration 2016: weight=1.0000271180018712 loss= 0.9998902618611644\n",
            "iteration 2017: weight=1.0000268041804536 loss= 0.9998915309340595\n",
            "iteration 2018: weight=1.000026493787718 loss= 0.9998927861520418\n",
            "iteration 2019: weight=1.0000261867885667 loss= 0.9998940276568116\n",
            "iteration 2020: weight=1.0000258831482358 loss= 0.9998952555887248\n",
            "iteration 2021: weight=1.0000255828322913 loss= 0.9998964700868062\n",
            "iteration 2022: weight=1.0000252858066265 loss= 0.9998976712887602\n",
            "iteration 2023: weight=1.0000249920374595 loss= 0.999898859330982\n",
            "iteration 2024: weight=1.00002470149133 loss= 0.9999000343485696\n",
            "iteration 2025: weight=1.0000244141350962 loss= 0.9999011964753349\n",
            "iteration 2026: weight=1.0000241299359331 loss= 0.999902345843815\n",
            "iteration 2027: weight=1.000023848861329 loss= 0.9999034825852826\n",
            "iteration 2028: weight=1.0000235708790821 loss= 0.9999046068297572\n",
            "iteration 2029: weight=1.0000232959572994 loss= 0.9999057187060169\n",
            "iteration 2030: weight=1.0000230240643928 loss= 0.9999068183416088\n",
            "iteration 2031: weight=1.0000227551690766 loss= 0.9999079058628589\n",
            "iteration 2032: weight=1.0000224892403653 loss= 0.9999089813948846\n",
            "iteration 2033: weight=1.0000222262475706 loss= 0.9999100450616025\n",
            "iteration 2034: weight=1.0000219661602987 loss= 0.9999110969857418\n",
            "iteration 2035: weight=1.0000217089484482 loss= 0.999912137288854\n",
            "iteration 2036: weight=1.0000214545822068 loss= 0.9999131660913209\n",
            "iteration 2037: weight=1.0000212030320497 loss= 0.9999141835123692\n",
            "iteration 2038: weight=1.000020954268736 loss= 0.9999151896700756\n",
            "iteration 2039: weight=1.000020708263307 loss= 0.9999161846813818\n",
            "iteration 2040: weight=1.0000204649870836 loss= 0.9999171686621006\n",
            "iteration 2041: weight=1.0000202244116636 loss= 0.9999181417269283\n",
            "iteration 2042: weight=1.000019986508919 loss= 0.9999191039894529\n",
            "iteration 2043: weight=1.000019751250994 loss= 0.9999200555621662\n",
            "iteration 2044: weight=1.0000195186103031 loss= 0.9999209965564714\n",
            "iteration 2045: weight=1.0000192885595274 loss= 0.999921927082692\n",
            "iteration 2046: weight=1.0000190610716129 loss= 0.9999228472500846\n",
            "iteration 2047: weight=1.0000188361197686 loss= 0.9999237571668463\n",
            "iteration 2048: weight=1.0000186136774636 loss= 0.9999246569401232\n",
            "iteration 2049: weight=1.0000183937184248 loss= 0.9999255466760216\n",
            "iteration 2050: weight=1.0000181762166347 loss= 0.9999264264796162\n",
            "iteration 2051: weight=1.0000179611463293 loss= 0.9999272964549606\n",
            "iteration 2052: weight=1.0000177484819956 loss= 0.9999281567050937\n",
            "iteration 2053: weight=1.0000175381983696 loss= 0.9999290073320518\n",
            "iteration 2054: weight=1.0000173302704336 loss= 0.9999298484368753\n",
            "iteration 2055: weight=1.0000171246734144 loss= 0.9999306801196187\n",
            "iteration 2056: weight=1.0000169213827812 loss= 0.9999315024793601\n",
            "iteration 2057: weight=1.000016720374243 loss= 0.999932315614208\n",
            "iteration 2058: weight=1.0000165216237469 loss= 0.9999331196213119\n",
            "iteration 2059: weight=1.0000163251074756 loss= 0.9999339145968686\n",
            "iteration 2060: weight=1.0000161308018456 loss= 0.999934700636134\n",
            "iteration 2061: weight=1.0000159386835046 loss= 0.9999354778334287\n",
            "iteration 2062: weight=1.0000157487293302 loss= 0.9999362462821482\n",
            "iteration 2063: weight=1.000015560916427 loss= 0.9999370060747691\n",
            "iteration 2064: weight=1.000015375222125 loss= 0.9999377573028606\n",
            "iteration 2065: weight=1.000015191623978 loss= 0.9999385000570897\n",
            "iteration 2066: weight=1.00001501009976 loss= 0.9999392344272302\n",
            "iteration 2067: weight=1.0000148306274654 loss= 0.9999399605021723\n",
            "iteration 2068: weight=1.0000146531853051 loss= 0.9999406783699283\n",
            "iteration 2069: weight=1.0000144777517057 loss= 0.9999413881176428\n",
            "iteration 2070: weight=1.000014304305307 loss= 0.9999420898315984\n",
            "iteration 2071: weight=1.00001413282496 loss= 0.999942783597225\n",
            "iteration 2072: weight=1.000013963289726 loss= 0.9999434694991066\n",
            "iteration 2073: weight=1.000013795678873 loss= 0.9999441476209897\n",
            "iteration 2074: weight=1.0000136299718754 loss= 0.9999448180457908\n",
            "iteration 2075: weight=1.0000134661484106 loss= 0.9999454808556031\n",
            "iteration 2076: weight=1.0000133041883588 loss= 0.9999461361317062\n",
            "iteration 2077: weight=1.0000131440717996 loss= 0.9999467839545707\n",
            "iteration 2078: weight=1.0000129857790117 loss= 0.999947424403868\n",
            "iteration 2079: weight=1.0000128292904695 loss= 0.9999480575584752\n",
            "iteration 2080: weight=1.0000126745868425 loss= 0.9999486834964847\n",
            "iteration 2081: weight=1.0000125216489932 loss= 0.9999493022952106\n",
            "iteration 2082: weight=1.0000123704579749 loss= 0.9999499140311938\n",
            "iteration 2083: weight=1.0000122209950302 loss= 0.9999505187802135\n",
            "iteration 2084: weight=1.0000120732415898 loss= 0.9999511166172902\n",
            "iteration 2085: weight=1.00001192717927 loss= 0.9999517076166934\n",
            "iteration 2086: weight=1.0000117827898711 loss= 0.9999522918519503\n",
            "iteration 2087: weight=1.0000116400553765 loss= 0.999952869395852\n",
            "iteration 2088: weight=1.0000114989579496 loss= 0.9999534403204577\n",
            "iteration 2089: weight=1.0000113594799336 loss= 0.9999540046971057\n",
            "iteration 2090: weight=1.000011221603849 loss= 0.9999545625964168\n",
            "iteration 2091: weight=1.000011085312392 loss= 0.9999551140883014\n",
            "iteration 2092: weight=1.0000109505884331 loss= 0.9999556592419685\n",
            "iteration 2093: weight=1.0000108174150155 loss= 0.999956198125929\n",
            "iteration 2094: weight=1.0000106857753532 loss= 0.9999567308080038\n",
            "iteration 2095: weight=1.0000105556528298 loss= 0.9999572573553303\n",
            "iteration 2096: weight=1.0000104270309966 loss= 0.999957777834368\n",
            "iteration 2097: weight=1.0000102998935714 loss= 0.9999582923109054\n",
            "iteration 2098: weight=1.0000101742244363 loss= 0.9999588008500657\n",
            "iteration 2099: weight=1.000010050007637 loss= 0.9999593035163142\n",
            "iteration 2100: weight=1.0000099272273808 loss= 0.9999598003734627\n",
            "iteration 2101: weight=1.0000098058680351 loss= 0.9999602914846761\n",
            "iteration 2102: weight=1.0000096859141259 loss= 0.9999607769124798\n",
            "iteration 2103: weight=1.0000095673503364 loss= 0.9999612567187642\n",
            "iteration 2104: weight=1.0000094501615058 loss= 0.9999617309647911\n",
            "iteration 2105: weight=1.000009334332627 loss= 0.9999621997111992\n",
            "iteration 2106: weight=1.0000092198488462 loss= 0.9999626630180108\n",
            "iteration 2107: weight=1.0000091066954606 loss= 0.9999631209446378\n",
            "iteration 2108: weight=1.0000089948579178 loss= 0.9999635735498852\n",
            "iteration 2109: weight=1.0000088843218136 loss= 0.9999640208919586\n",
            "iteration 2110: weight=1.000008775072891 loss= 0.9999644630284704\n",
            "iteration 2111: weight=1.0000086670970387 loss= 0.9999649000164437\n",
            "iteration 2112: weight=1.0000085603802902 loss= 0.9999653319123194\n",
            "iteration 2113: weight=1.0000084549088213 loss= 0.9999657587719597\n",
            "iteration 2114: weight=1.00000835066895 loss= 0.9999661806506568\n",
            "iteration 2115: weight=1.0000082476471348 loss= 0.9999665976031342\n",
            "iteration 2116: weight=1.0000081458299726 loss= 0.9999670096835555\n",
            "iteration 2117: weight=1.0000080452041984 loss= 0.9999674169455279\n",
            "iteration 2118: weight=1.0000079457566835 loss= 0.9999678194421077\n",
            "iteration 2119: weight=1.000007847474434 loss= 0.9999682172258062\n",
            "iteration 2120: weight=1.0000077503445906 loss= 0.9999686103485952\n",
            "iteration 2121: weight=1.0000076543544254 loss= 0.9999689988619092\n",
            "iteration 2122: weight=1.000007559491343 loss= 0.9999693828166549\n",
            "iteration 2123: weight=1.0000074657428768 loss= 0.999969762263212\n",
            "iteration 2124: weight=1.00000737309669 loss= 0.999970137251442\n",
            "iteration 2125: weight=1.000007281540573 loss= 0.9999705078306899\n",
            "iteration 2126: weight=1.0000071910624422 loss= 0.9999708740497916\n",
            "iteration 2127: weight=1.0000071016503396 loss= 0.9999712359570768\n",
            "iteration 2128: weight=1.0000070132924308 loss= 0.9999715936003755\n",
            "iteration 2129: weight=1.0000069259770046 loss= 0.9999719470270219\n",
            "iteration 2130: weight=1.0000068396924708 loss= 0.9999722962838583\n",
            "iteration 2131: weight=1.00000675442736 loss= 0.9999726414172422\n",
            "iteration 2132: weight=1.000006670170322 loss= 0.9999729824730488\n",
            "iteration 2133: weight=1.0000065869101247 loss= 0.9999733194966763\n",
            "iteration 2134: weight=1.0000065046356528 loss= 0.9999736525330508\n",
            "iteration 2135: weight=1.000006423335907 loss= 0.9999739816266301\n",
            "iteration 2136: weight=1.0000063430000028 loss= 0.9999743068214092\n",
            "iteration 2137: weight=1.0000062636171692 loss= 0.9999746281609234\n",
            "iteration 2138: weight=1.0000061851767474 loss= 0.999974945688255\n",
            "iteration 2139: weight=1.000006107668191 loss= 0.9999752594460358\n",
            "iteration 2140: weight=1.000006031081063 loss= 0.9999755694764505\n",
            "iteration 2141: weight=1.000005955405036 loss= 0.9999758758212437\n",
            "iteration 2142: weight=1.000005880629891 loss= 0.9999761785217234\n",
            "iteration 2143: weight=1.0000058067455162 loss= 0.9999764776187631\n",
            "iteration 2144: weight=1.0000057337419057 loss= 0.9999767731528084\n",
            "iteration 2145: weight=1.000005661609159 loss= 0.9999770651638803\n",
            "iteration 2146: weight=1.0000055903374796 loss= 0.9999773536915791\n",
            "iteration 2147: weight=1.0000055199171738 loss= 0.999977638775089\n",
            "iteration 2148: weight=1.0000054503386502 loss= 0.9999779204531828\n",
            "iteration 2149: weight=1.0000053815924188 loss= 0.999978198764224\n",
            "iteration 2150: weight=1.0000053136690892 loss= 0.9999784737461709\n",
            "iteration 2151: weight=1.0000052465593703 loss= 0.9999787454365836\n",
            "iteration 2152: weight=1.0000051802540693 loss= 0.9999790138726244\n",
            "iteration 2153: weight=1.0000051147440905 loss= 0.9999792790910631\n",
            "iteration 2154: weight=1.0000050500204345 loss= 0.9999795411282805\n",
            "iteration 2155: weight=1.000004986074197 loss= 0.9999798000202729\n",
            "iteration 2156: weight=1.0000049228965684 loss= 0.9999800558026555\n",
            "iteration 2157: weight=1.000004860478832 loss= 0.9999803085106661\n",
            "iteration 2158: weight=1.0000047988123644 loss= 0.9999805581791686\n",
            "iteration 2159: weight=1.000004737888633 loss= 0.999980804842657\n",
            "iteration 2160: weight=1.0000046776991964 loss= 0.9999810485352585\n",
            "iteration 2161: weight=1.0000046182357032 loss= 0.9999812892907377\n",
            "iteration 2162: weight=1.0000045594898903 loss= 0.9999815271424997\n",
            "iteration 2163: weight=1.0000045014535832 loss= 0.9999817621235946\n",
            "iteration 2164: weight=1.0000044441186946 loss= 0.9999819942667195\n",
            "iteration 2165: weight=1.000004387477223 loss= 0.9999822236042224\n",
            "iteration 2166: weight=1.000004331521253 loss= 0.9999824501681075\n",
            "iteration 2167: weight=1.0000042762429533 loss= 0.9999826739900363\n",
            "iteration 2168: weight=1.0000042216345768 loss= 0.9999828951013319\n",
            "iteration 2169: weight=1.000004167688459 loss= 0.9999831135329815\n",
            "iteration 2170: weight=1.000004114397018 loss= 0.9999833293156425\n",
            "iteration 2171: weight=1.0000040617527528 loss= 0.9999835424796412\n",
            "iteration 2172: weight=1.000004009748243 loss= 0.9999837530549801\n",
            "iteration 2173: weight=1.0000039583761482 loss= 0.9999839610713404\n",
            "iteration 2174: weight=1.0000039076292064 loss= 0.9999841665580822\n",
            "iteration 2175: weight=1.0000038575002341 loss= 0.9999843695442525\n",
            "iteration 2176: weight=1.0000038079821252 loss= 0.9999845700585848\n",
            "iteration 2177: weight=1.0000037590678499 loss= 0.9999847681295022\n",
            "iteration 2178: weight=1.0000037107504545 loss= 0.9999849637851228\n",
            "iteration 2179: weight=1.00000366302306 loss= 0.9999851570532609\n",
            "iteration 2180: weight=1.000003615878862 loss= 0.999985347961431\n",
            "iteration 2181: weight=1.00000356931113 loss= 0.9999855365368499\n",
            "iteration 2182: weight=1.0000035233132054 loss= 0.9999857228064399\n",
            "iteration 2183: weight=1.0000034778785023 loss= 0.9999859067968333\n",
            "iteration 2184: weight=1.0000034330005059 loss= 0.9999860885343733\n",
            "iteration 2185: weight=1.0000033886727724 loss= 0.9999862680451185\n",
            "iteration 2186: weight=1.0000033448889276 loss= 0.9999864453548428\n",
            "iteration 2187: weight=1.0000033016426668 loss= 0.9999866204890426\n",
            "iteration 2188: weight=1.0000032589277537 loss= 0.9999867934729361\n",
            "iteration 2189: weight=1.0000032167380197 loss= 0.9999869643314678\n",
            "iteration 2190: weight=1.0000031750673637 loss= 0.9999871330893106\n",
            "iteration 2191: weight=1.0000031339097508 loss= 0.9999872997708694\n",
            "iteration 2192: weight=1.000003093259212 loss= 0.9999874644002823\n",
            "iteration 2193: weight=1.0000030531098436 loss= 0.9999876270014246\n",
            "iteration 2194: weight=1.000003013455806 loss= 0.9999877875979115\n",
            "iteration 2195: weight=1.0000029742913237 loss= 0.9999879462130996\n",
            "iteration 2196: weight=1.000002935610684 loss= 0.9999881028700909\n",
            "iteration 2197: weight=1.0000028974082376 loss= 0.9999882575917348\n",
            "iteration 2198: weight=1.000002859678396 loss= 0.9999884104006297\n",
            "iteration 2199: weight=1.0000028224156332 loss= 0.9999885613191267\n",
            "iteration 2200: weight=1.0000027856144826 loss= 0.9999887103693312\n",
            "iteration 2201: weight=1.0000027492695385 loss= 0.9999888575731081\n",
            "iteration 2202: weight=1.000002713375454 loss= 0.99998900295208\n",
            "iteration 2203: weight=1.0000026779269418 loss= 0.9999891465276333\n",
            "iteration 2204: weight=1.0000026429187718 loss= 0.9999892883209182\n",
            "iteration 2205: weight=1.0000026083457723 loss= 0.9999894283528528\n",
            "iteration 2206: weight=1.0000025742028282 loss= 0.9999895666441249\n",
            "iteration 2207: weight=1.0000025404848814 loss= 0.9999897032151932\n",
            "iteration 2208: weight=1.000002507186929 loss= 0.9999898380862908\n",
            "iteration 2209: weight=1.0000024743040237 loss= 0.9999899712774282\n",
            "iteration 2210: weight=1.0000024418312727 loss= 0.9999901028083941\n",
            "iteration 2211: weight=1.0000024097638376 loss= 0.9999902326987592\n",
            "iteration 2212: weight=1.0000023780969336 loss= 0.9999903609678775\n",
            "iteration 2213: weight=1.0000023468258288 loss= 0.999990487634887\n",
            "iteration 2214: weight=1.0000023159458438 loss= 0.9999906127187153\n",
            "iteration 2215: weight=1.0000022854523514 loss= 0.9999907362380793\n",
            "iteration 2216: weight=1.0000022553407755 loss= 0.9999908582114877\n",
            "iteration 2217: weight=1.000002225606591 loss= 0.9999909786572444\n",
            "iteration 2218: weight=1.0000021962453232 loss= 0.9999910975934492\n",
            "iteration 2219: weight=1.0000021672525472 loss= 0.9999912150380014\n",
            "iteration 2220: weight=1.0000021386238875 loss= 0.9999913310085992\n",
            "iteration 2221: weight=1.0000021103550174 loss= 0.9999914455227447\n",
            "iteration 2222: weight=1.0000020824416584 loss= 0.9999915585977448\n",
            "iteration 2223: weight=1.0000020548795798 loss= 0.9999916702507124\n",
            "iteration 2224: weight=1.0000020276645984 loss= 0.9999917804985708\n",
            "iteration 2225: weight=1.0000020007925776 loss= 0.9999918893580519\n",
            "iteration 2226: weight=1.0000019742594273 loss= 0.9999919968457022\n",
            "iteration 2227: weight=1.000001948061103 loss= 0.9999921029778817\n",
            "iteration 2228: weight=1.000001922193606 loss= 0.9999922077707674\n",
            "iteration 2229: weight=1.000001896652982 loss= 0.999992311240355\n",
            "iteration 2230: weight=1.0000018714353214 loss= 0.9999924134024611\n",
            "iteration 2231: weight=1.0000018465367584 loss= 0.9999925142727236\n",
            "iteration 2232: weight=1.0000018219534708 loss= 0.9999926138666052\n",
            "iteration 2233: weight=1.0000017976816795 loss= 0.999992712199395\n",
            "iteration 2234: weight=1.0000017737176479 loss= 0.9999928092862086\n",
            "iteration 2235: weight=1.0000017500576817 loss= 0.9999929051419928\n",
            "iteration 2236: weight=1.0000017266981278 loss= 0.9999929997815241\n",
            "iteration 2237: weight=1.000001703635375 loss= 0.9999930932194147\n",
            "iteration 2238: weight=1.0000016808658527 loss= 0.9999931854701095\n",
            "iteration 2239: weight=1.0000016583860305 loss= 0.9999932765478904\n",
            "iteration 2240: weight=1.0000016361924182 loss= 0.999993366466879\n",
            "iteration 2241: weight=1.0000016142815649 loss= 0.9999934552410357\n",
            "iteration 2242: weight=1.0000015926500592 loss= 0.9999935428841642\n",
            "iteration 2243: weight=1.000001571294528 loss= 0.9999936294099094\n",
            "iteration 2244: weight=1.0000015502116368 loss= 0.9999937148317637\n",
            "iteration 2245: weight=1.000001529398089 loss= 0.9999937991630653\n",
            "iteration 2246: weight=1.0000015088506256 loss= 0.9999938824169999\n",
            "iteration 2247: weight=1.000001488566024 loss= 0.9999939646066043\n",
            "iteration 2248: weight=1.0000014685410994 loss= 0.9999940457447674\n",
            "iteration 2249: weight=1.0000014487727025 loss= 0.9999941258442289\n",
            "iteration 2250: weight=1.0000014292577202 loss= 0.9999942049175858\n",
            "iteration 2251: weight=1.0000014099930747 loss= 0.9999942829772904\n",
            "iteration 2252: weight=1.0000013909757242 loss= 0.9999943600356533\n",
            "iteration 2253: weight=1.0000013722026606 loss= 0.9999944361048426\n",
            "iteration 2254: weight=1.000001353670911 loss= 0.9999945111968893\n",
            "iteration 2255: weight=1.000001335377536 loss= 0.999994585323686\n",
            "iteration 2256: weight=1.0000013173196307 loss= 0.9999946584969885\n",
            "iteration 2257: weight=1.0000012994943226 loss= 0.9999947307284185\n",
            "iteration 2258: weight=1.0000012818987727 loss= 0.9999948020294643\n",
            "iteration 2259: weight=1.0000012645301743 loss= 0.9999948724114823\n",
            "iteration 2260: weight=1.0000012473857536 loss= 0.9999949418856987\n",
            "iteration 2261: weight=1.0000012304627681 loss= 0.9999950104632096\n",
            "iteration 2262: weight=1.0000012137585073 loss= 0.9999950781549837\n",
            "iteration 2263: weight=1.0000011972702916 loss= 0.9999951449718638\n",
            "iteration 2264: weight=1.0000011809954725 loss= 0.9999952109245676\n",
            "iteration 2265: weight=1.000001164931432 loss= 0.9999952760236889\n",
            "iteration 2266: weight=1.0000011490755825 loss= 0.9999953402797003\n",
            "iteration 2267: weight=1.000001133425366 loss= 0.9999954037029517\n",
            "iteration 2268: weight=1.0000011179782544 loss= 0.9999954663036746\n",
            "iteration 2269: weight=1.000001102731749 loss= 0.9999955280919819\n",
            "iteration 2270: weight=1.0000010876833796 loss= 0.999995589077868\n",
            "iteration 2271: weight=1.000001072830705 loss= 0.9999956492712139\n",
            "iteration 2272: weight=1.0000010581713121 loss= 0.9999957086817841\n",
            "iteration 2273: weight=1.0000010437028162 loss= 0.9999957673192303\n",
            "iteration 2274: weight=1.0000010294228596 loss= 0.9999958251930926\n",
            "iteration 2275: weight=1.000001015329113 loss= 0.9999958823128003\n",
            "iteration 2276: weight=1.0000010014192735 loss= 0.9999959386876716\n",
            "iteration 2277: weight=1.0000009876910654 loss= 0.9999959943269173\n",
            "iteration 2278: weight=1.0000009741422393 loss= 0.9999960492396406\n",
            "iteration 2279: weight=1.0000009607705722 loss= 0.9999961034348386\n",
            "iteration 2280: weight=1.000000947573867 loss= 0.9999961569214034\n",
            "iteration 2281: weight=1.0000009345499523 loss= 0.9999962097081239\n",
            "iteration 2282: weight=1.000000921696682 loss= 0.9999962618036843\n",
            "iteration 2283: weight=1.0000009090119355 loss= 0.9999963132166697\n",
            "iteration 2284: weight=1.0000008964936165 loss= 0.9999963639555632\n",
            "iteration 2285: weight=1.0000008841396535 loss= 0.999996414028749\n",
            "iteration 2286: weight=1.0000008719479996 loss= 0.9999964634445129\n",
            "iteration 2287: weight=1.0000008599166315 loss= 0.9999965122110428\n",
            "iteration 2288: weight=1.0000008480435498 loss= 0.999996560336432\n",
            "iteration 2289: weight=1.000000836326779 loss= 0.9999966078286774\n",
            "iteration 2290: weight=1.0000008247643664 loss= 0.999996654695682\n",
            "iteration 2291: weight=1.0000008133543823 loss= 0.9999967009452555\n",
            "iteration 2292: weight=1.0000008020949198 loss= 0.9999967465851171\n",
            "iteration 2293: weight=1.000000790984095 loss= 0.9999967916228941\n",
            "iteration 2294: weight=1.0000007800200452 loss= 0.9999968360661231\n",
            "iteration 2295: weight=1.000000769200931 loss= 0.9999968799222528\n",
            "iteration 2296: weight=1.0000007585249335 loss= 0.9999969231986429\n",
            "iteration 2297: weight=1.0000007479902562 loss= 0.9999969659025674\n",
            "iteration 2298: weight=1.0000007375951234 loss= 0.9999970080412133\n",
            "iteration 2299: weight=1.000000727337781 loss= 0.9999970496216825\n",
            "iteration 2300: weight=1.0000007172164946 loss= 0.9999970906509925\n",
            "iteration 2301: weight=1.0000007072295518 loss= 0.999997131136079\n",
            "iteration 2302: weight=1.0000006973752595 loss= 0.9999971710837935\n",
            "iteration 2303: weight=1.0000006876519454 loss= 0.9999972105009072\n",
            "iteration 2304: weight=1.0000006780579564 loss= 0.9999972493941099\n",
            "iteration 2305: weight=1.0000006685916598 loss= 0.9999972877700133\n",
            "iteration 2306: weight=1.0000006592514417 loss= 0.9999973256351488\n",
            "iteration 2307: weight=1.000000650035708 loss= 0.9999973629959719\n",
            "iteration 2308: weight=1.0000006409428832 loss= 0.9999973998588585\n",
            "iteration 2309: weight=1.000000631971411 loss= 0.9999974362301104\n",
            "iteration 2310: weight=1.0000006231197534 loss= 0.9999974721159536\n",
            "iteration 2311: weight=1.0000006143863909 loss= 0.9999975075225397\n",
            "iteration 2312: weight=1.000000605769822 loss= 0.9999975424559464\n",
            "iteration 2313: weight=1.0000005972685637 loss= 0.9999975769221795\n",
            "iteration 2314: weight=1.0000005888811505 loss= 0.9999976109271721\n",
            "iteration 2315: weight=1.0000005806061343 loss= 0.9999976444767851\n",
            "iteration 2316: weight=1.0000005724420844 loss= 0.9999976775768112\n",
            "iteration 2317: weight=1.0000005643875876 loss= 0.9999977102329731\n",
            "iteration 2318: weight=1.0000005564412473 loss= 0.9999977424509239\n",
            "iteration 2319: weight=1.0000005486016843 loss= 0.9999977742362492\n",
            "iteration 2320: weight=1.0000005408675354 loss= 0.9999978055944666\n",
            "iteration 2321: weight=1.0000005332374542 loss= 0.9999978365310285\n",
            "iteration 2322: weight=1.00000052571011 loss= 0.9999978670513207\n",
            "iteration 2323: weight=1.0000005182841893 loss= 0.999997897160665\n",
            "iteration 2324: weight=1.0000005109583932 loss= 0.9999979268643171\n",
            "iteration 2325: weight=1.0000005037314392 loss= 0.9999979561674717\n",
            "iteration 2326: weight=1.0000004966020604 loss= 0.9999979850752581\n",
            "iteration 2327: weight=1.0000004895690047 loss= 0.999998013592745\n",
            "iteration 2328: weight=1.0000004826310356 loss= 0.99999804172494\n",
            "iteration 2329: weight=1.0000004757869314 loss= 0.9999980694767894\n",
            "iteration 2330: weight=1.0000004690354856 loss= 0.9999980968531799\n",
            "iteration 2331: weight=1.0000004623755057 loss= 0.9999981238589377\n",
            "iteration 2332: weight=1.0000004558058142 loss= 0.9999981504988326\n",
            "iteration 2333: weight=1.000000449325248 loss= 0.9999981767775742\n",
            "iteration 2334: weight=1.0000004429326577 loss= 0.999998202699816\n",
            "iteration 2335: weight=1.0000004366269082 loss= 0.999998228270154\n",
            "iteration 2336: weight=1.0000004304068784 loss= 0.9999982534931299\n",
            "iteration 2337: weight=1.0000004242714604 loss= 0.9999982783732275\n",
            "iteration 2338: weight=1.0000004182195603 loss= 0.9999983029148783\n",
            "iteration 2339: weight=1.0000004122500974 loss= 0.9999983271224584\n",
            "iteration 2340: weight=1.000000406362004 loss= 0.9999983510002902\n",
            "iteration 2341: weight=1.0000004005542258 loss= 0.9999983745526443\n",
            "iteration 2342: weight=1.0000003948257212 loss= 0.9999983977837387\n",
            "iteration 2343: weight=1.0000003891754616 loss= 0.9999984206977386\n",
            "iteration 2344: weight=1.0000003836024307 loss= 0.9999984432987593\n",
            "iteration 2345: weight=1.000000378105625 loss= 0.9999984655908658\n",
            "iteration 2346: weight=1.0000003726840532 loss= 0.9999984875780717\n",
            "iteration 2347: weight=1.000000367336736 loss= 0.9999985092643429\n",
            "iteration 2348: weight=1.0000003620627063 loss= 0.9999985306535961\n",
            "iteration 2349: weight=1.000000356861009 loss= 0.9999985517496993\n",
            "iteration 2350: weight=1.0000003517307006 loss= 0.9999985725564735\n",
            "iteration 2351: weight=1.0000003466708494 loss= 0.9999985930776926\n",
            "iteration 2352: weight=1.0000003416805352 loss= 0.9999986133170833\n",
            "iteration 2353: weight=1.000000336758849 loss= 0.9999986332783262\n",
            "iteration 2354: weight=1.0000003319048933 loss= 0.9999986529650575\n",
            "iteration 2355: weight=1.0000003271177815 loss= 0.9999986723808673\n",
            "iteration 2356: weight=1.0000003223966378 loss= 0.9999986915293019\n",
            "iteration 2357: weight=1.0000003177405976 loss= 0.9999987104138645\n",
            "iteration 2358: weight=1.000000313148807 loss= 0.9999987290380133\n",
            "iteration 2359: weight=1.0000003086204226 loss= 0.9999987474051638\n",
            "iteration 2360: weight=1.0000003041546113 loss= 0.9999987655186908\n",
            "iteration 2361: weight=1.0000002997505506 loss= 0.9999987833819249\n",
            "iteration 2362: weight=1.000000295407428 loss= 0.999998800998157\n",
            "iteration 2363: weight=1.0000002911244414 loss= 0.9999988183706368\n",
            "iteration 2364: weight=1.0000002869007987 loss= 0.9999988355025733\n",
            "iteration 2365: weight=1.0000002827357173 loss= 0.9999988523971346\n",
            "iteration 2366: weight=1.0000002786284246 loss= 0.9999988690574506\n",
            "iteration 2367: weight=1.0000002745781578 loss= 0.9999988854866123\n",
            "iteration 2368: weight=1.0000002705841633 loss= 0.9999989016876705\n",
            "iteration 2369: weight=1.000000266645697 loss= 0.9999989176636397\n",
            "iteration 2370: weight=1.0000002627620246 loss= 0.9999989334174961\n",
            "iteration 2371: weight=1.0000002589324202 loss= 0.9999989489521779\n",
            "iteration 2372: weight=1.0000002551561675 loss= 0.9999989642705875\n",
            "iteration 2373: weight=1.0000002514325592 loss= 0.9999989793755903\n",
            "iteration 2374: weight=1.000000247760897 loss= 0.999998994270016\n",
            "iteration 2375: weight=1.0000002441404905 loss= 0.9999990089566579\n",
            "iteration 2376: weight=1.0000002405706592 loss= 0.9999990234382764\n",
            "iteration 2377: weight=1.0000002370507302 loss= 0.9999990377175948\n",
            "iteration 2378: weight=1.0000002335800395 loss= 0.9999990517973042\n",
            "iteration 2379: weight=1.0000002301579316 loss= 0.9999990656800601\n",
            "iteration 2380: weight=1.0000002267837589 loss= 0.9999990793684855\n",
            "iteration 2381: weight=1.000000223456882 loss= 0.9999990928651702\n",
            "iteration 2382: weight=1.0000002201766702 loss= 0.9999991061726713\n",
            "iteration 2383: weight=1.0000002169425 loss= 0.999999119293513\n",
            "iteration 2384: weight=1.000000213753756 loss= 0.9999991322301884\n",
            "iteration 2385: weight=1.000000210609831 loss= 0.9999991449851586\n",
            "iteration 2386: weight=1.0000002075101246 loss= 0.9999991575608538\n",
            "iteration 2387: weight=1.000000204454045 loss= 0.9999991699596736\n",
            "iteration 2388: weight=1.0000002014410074 loss= 0.9999991821839871\n",
            "iteration 2389: weight=1.0000001984704345 loss= 0.9999991942361326\n",
            "iteration 2390: weight=1.0000001955417563 loss= 0.9999992061184194\n",
            "iteration 2391: weight=1.0000001926544102 loss= 0.9999992178331276\n",
            "iteration 2392: weight=1.0000001898078406 loss= 0.9999992293825076\n",
            "iteration 2393: weight=1.000000187001499 loss= 0.9999992407687819\n",
            "iteration 2394: weight=1.0000001842348438 loss= 0.9999992519941441\n",
            "iteration 2395: weight=1.0000001815073407 loss= 0.9999992630607605\n",
            "iteration 2396: weight=1.0000001788184618 loss= 0.9999992739707689\n",
            "iteration 2397: weight=1.000000176167686 loss= 0.9999992847262806\n",
            "iteration 2398: weight=1.000000173554499 loss= 0.9999992953293801\n",
            "iteration 2399: weight=1.000000170978393 loss= 0.9999993057821245\n",
            "iteration 2400: weight=1.0000001684388669 loss= 0.9999993160865449\n",
            "iteration 2401: weight=1.0000001659354256 loss= 0.999999326244646\n",
            "iteration 2402: weight=1.0000001634675808 loss= 0.9999993362584076\n",
            "iteration 2403: weight=1.0000001610348501 loss= 0.9999993461297837\n",
            "iteration 2404: weight=1.0000001586367575 loss= 0.9999993558607032\n",
            "iteration 2405: weight=1.0000001562728331 loss= 0.9999993654530707\n",
            "iteration 2406: weight=1.000000153942613 loss= 0.9999993749087652\n",
            "iteration 2407: weight=1.0000001516456394 loss= 0.9999993842296424\n",
            "iteration 2408: weight=1.00000014938146 loss= 0.9999993934175345\n",
            "iteration 2409: weight=1.000000147149629 loss= 0.9999994024742489\n",
            "iteration 2410: weight=1.0000001449497056 loss= 0.9999994114015709\n",
            "iteration 2411: weight=1.0000001427812553 loss= 0.9999994202012618\n",
            "iteration 2412: weight=1.0000001406438488 loss= 0.9999994288750605\n",
            "iteration 2413: weight=1.0000001385370627 loss= 0.999999437424684\n",
            "iteration 2414: weight=1.000000136460479 loss= 0.9999994458518259\n",
            "iteration 2415: weight=1.0000001344136846 loss= 0.9999994541581588\n",
            "iteration 2416: weight=1.0000001323962724 loss= 0.9999994623453338\n",
            "iteration 2417: weight=1.0000001304078403 loss= 0.9999994704149806\n",
            "iteration 2418: weight=1.0000001284479916 loss= 0.9999994783687068\n",
            "iteration 2419: weight=1.0000001265163343 loss= 0.9999994862080996\n",
            "iteration 2420: weight=1.000000124612482 loss= 0.9999994939347269\n",
            "iteration 2421: weight=1.0000001227360533 loss= 0.9999995015501336\n",
            "iteration 2422: weight=1.0000001208866716 loss= 0.999999509055847\n",
            "iteration 2423: weight=1.000000119063965 loss= 0.9999995164533723\n",
            "iteration 2424: weight=1.0000001172675665 loss= 0.9999995237441971\n",
            "iteration 2425: weight=1.0000001154971145 loss= 0.9999995309297888\n",
            "iteration 2426: weight=1.0000001137522514 loss= 0.9999995380115952\n",
            "iteration 2427: weight=1.0000001120326245 loss= 0.9999995449910462\n",
            "iteration 2428: weight=1.000000110337886 loss= 0.9999995518695521\n",
            "iteration 2429: weight=1.000000108667692 loss= 0.999999558648505\n",
            "iteration 2430: weight=1.0000001070217037 loss= 0.9999995653292794\n",
            "iteration 2431: weight=1.0000001053995866 loss= 0.999999571913231\n",
            "iteration 2432: weight=1.0000001038010107 loss= 0.999999578401698\n",
            "iteration 2433: weight=1.0000001022256497 loss= 0.9999995847960004\n",
            "iteration 2434: weight=1.0000001006731825 loss= 0.999999591097443\n",
            "iteration 2435: weight=1.0000000991432916 loss= 0.9999995973073107\n",
            "iteration 2436: weight=1.0000000976356638 loss= 0.999999603426873\n",
            "iteration 2437: weight=1.0000000961499902 loss= 0.999999609457383\n",
            "iteration 2438: weight=1.0000000946859657 loss= 0.9999996154000762\n",
            "iteration 2439: weight=1.0000000932432895 loss= 0.9999996212561729\n",
            "iteration 2440: weight=1.0000000918216647 loss= 0.9999996270268766\n",
            "iteration 2441: weight=1.000000090420798 loss= 0.9999996327133749\n",
            "iteration 2442: weight=1.0000000890404004 loss= 0.9999996383168409\n",
            "iteration 2443: weight=1.0000000876801864 loss= 0.9999996438384302\n",
            "iteration 2444: weight=1.0000000863398748 loss= 0.999999649279285\n",
            "iteration 2445: weight=1.0000000850191875 loss= 0.9999996546405306\n",
            "iteration 2446: weight=1.0000000837178504 loss= 0.999999659923279\n",
            "iteration 2447: weight=1.000000082435593 loss= 0.9999996651286264\n",
            "iteration 2448: weight=1.0000000811721483 loss= 0.9999996702576551\n",
            "iteration 2449: weight=1.000000079927253 loss= 0.999999675311433\n",
            "iteration 2450: weight=1.000000078700647 loss= 0.9999996802910134\n",
            "iteration 2451: weight=1.0000000774920743 loss= 0.9999996851974363\n",
            "iteration 2452: weight=1.0000000763012815 loss= 0.9999996900317267\n",
            "iteration 2453: weight=1.0000000751280191 loss= 0.9999996947948973\n",
            "iteration 2454: weight=1.000000073972041 loss= 0.999999699487946\n",
            "iteration 2455: weight=1.0000000728331038 loss= 0.9999997041118581\n",
            "iteration 2456: weight=1.000000071710968 loss= 0.9999997086676061\n",
            "iteration 2457: weight=1.000000070605397 loss= 0.9999997131561483\n",
            "iteration 2458: weight=1.0000000695161575 loss= 0.9999997175784316\n",
            "iteration 2459: weight=1.000000068443019 loss= 0.9999997219353893\n",
            "iteration 2460: weight=1.0000000673857545 loss= 0.9999997262279426\n",
            "iteration 2461: weight=1.0000000663441397 loss= 0.9999997304570001\n",
            "iteration 2462: weight=1.0000000653179535 loss= 0.9999997346234587\n",
            "iteration 2463: weight=1.0000000643069777 loss= 0.9999997387282031\n",
            "iteration 2464: weight=1.0000000633109971 loss= 0.9999997427721056\n",
            "iteration 2465: weight=1.0000000623297995 loss= 0.9999997467560275\n",
            "iteration 2466: weight=1.0000000613631752 loss= 0.9999997506808174\n",
            "iteration 2467: weight=1.0000000604109176 loss= 0.9999997545473143\n",
            "iteration 2468: weight=1.0000000594728229 loss= 0.9999997583563441\n",
            "iteration 2469: weight=1.00000005854869 loss= 0.9999997621087225\n",
            "iteration 2470: weight=1.0000000576383201 loss= 0.9999997658052542\n",
            "iteration 2471: weight=1.000000056741518 loss= 0.9999997694467329\n",
            "iteration 2472: weight=1.0000000558580902 loss= 0.9999997730339412\n",
            "iteration 2473: weight=1.0000000549878465 loss= 0.9999997765676518\n",
            "iteration 2474: weight=1.000000054130599 loss= 0.9999997800486261\n",
            "iteration 2475: weight=1.0000000532861621 loss= 0.9999997834776162\n",
            "iteration 2476: weight=1.0000000524543533 loss= 0.9999997868553627\n",
            "iteration 2477: weight=1.0000000516349918 loss= 0.9999997901825978\n",
            "iteration 2478: weight=1.0000000508279 loss= 0.9999997934600433\n",
            "iteration 2479: weight=1.0000000500329025 loss= 0.9999997966884099\n",
            "iteration 2480: weight=1.000000049249826 loss= 0.9999997998684\n",
            "iteration 2481: weight=1.0000000484784997 loss= 0.9999998030007057\n",
            "iteration 2482: weight=1.0000000477187554 loss= 0.9999998060860108\n",
            "iteration 2483: weight=1.0000000469704267 loss= 0.9999998091249875\n",
            "iteration 2484: weight=1.0000000462333498 loss= 0.9999998121183021\n",
            "iteration 2485: weight=1.0000000455073634 loss= 0.9999998150666093\n",
            "iteration 2486: weight=1.0000000447923079 loss= 0.9999998179705546\n",
            "iteration 2487: weight=1.000000044088026 loss= 0.9999998208307765\n",
            "iteration 2488: weight=1.0000000433943623 loss= 0.9999998236479042\n",
            "iteration 2489: weight=1.0000000427111644 loss= 0.9999998264225582\n",
            "iteration 2490: weight=1.000000042038281 loss= 0.9999998291553498\n",
            "iteration 2491: weight=1.0000000413755636 loss= 0.9999998318468828\n",
            "iteration 2492: weight=1.0000000407228655 loss= 0.9999998344977523\n",
            "iteration 2493: weight=1.0000000400800417 loss= 0.9999998371085446\n",
            "iteration 2494: weight=1.0000000394469495 loss= 0.9999998396798395\n",
            "iteration 2495: weight=1.0000000388234482 loss= 0.9999998422122083\n",
            "iteration 2496: weight=1.000000038209399 loss= 0.9999998447062131\n",
            "iteration 2497: weight=1.0000000376046647 loss= 0.99999984716241\n",
            "iteration 2498: weight=1.0000000370091107 loss= 0.9999998495813468\n",
            "iteration 2499: weight=1.0000000364226034 loss= 0.9999998519635628\n",
            "iteration 2500: weight=1.0000000358450116 loss= 0.9999998543095918\n",
            "iteration 2501: weight=1.0000000352762057 loss= 0.9999998566199586\n",
            "iteration 2502: weight=1.000000034716058 loss= 0.9999998588951821\n",
            "iteration 2503: weight=1.0000000341644426 loss= 0.9999998611357727\n",
            "iteration 2504: weight=1.0000000336212351 loss= 0.9999998633422342\n",
            "iteration 2505: weight=1.0000000330863132 loss= 0.999999865515064\n",
            "iteration 2506: weight=1.0000000325595562 loss= 0.9999998676547514\n",
            "iteration 2507: weight=1.0000000320408446 loss= 0.9999998697617796\n",
            "iteration 2508: weight=1.0000000315300612 loss= 0.9999998718366255\n",
            "iteration 2509: weight=1.00000003102709 loss= 0.9999998738797591\n",
            "iteration 2510: weight=1.0000000305318169 loss= 0.9999998758916439\n",
            "iteration 2511: weight=1.000000030044129 loss= 0.9999998778727364\n",
            "iteration 2512: weight=1.0000000295639153 loss= 0.9999998798234878\n",
            "iteration 2513: weight=1.0000000290910664 loss= 0.9999998817443422\n",
            "iteration 2514: weight=1.0000000286254744 loss= 0.9999998836357376\n",
            "iteration 2515: weight=1.0000000281670327 loss= 0.9999998854981056\n",
            "iteration 2516: weight=1.0000000277156362 loss= 0.9999998873318724\n",
            "iteration 2517: weight=1.0000000272711815 loss= 0.9999998891374582\n",
            "iteration 2518: weight=1.0000000268335665 loss= 0.9999998909152769\n",
            "iteration 2519: weight=1.0000000264026903 loss= 0.999999892665737\n",
            "iteration 2520: weight=1.0000000259784538 loss= 0.9999998943892418\n",
            "iteration 2521: weight=1.0000000255607593 loss= 0.9999998960861873\n",
            "iteration 2522: weight=1.0000000251495103 loss= 0.9999998977569655\n",
            "iteration 2523: weight=1.0000000247446115 loss= 0.9999998994019615\n",
            "iteration 2524: weight=1.0000000243459692 loss= 0.9999999010215566\n",
            "iteration 2525: weight=1.000000023953491 loss= 0.9999999026161254\n",
            "iteration 2526: weight=1.0000000235670856 loss= 0.9999999041860385\n",
            "iteration 2527: weight=1.0000000231866633 loss= 0.9999999057316598\n",
            "iteration 2528: weight=1.0000000228121353 loss= 0.9999999072533491\n",
            "iteration 2529: weight=1.0000000224434142 loss= 0.9999999087514609\n",
            "iteration 2530: weight=1.0000000220804142 loss= 0.999999910226345\n",
            "iteration 2531: weight=1.00000002172305 loss= 0.9999999116783453\n",
            "iteration 2532: weight=1.0000000213712381 loss= 0.9999999131078018\n",
            "iteration 2533: weight=1.0000000210248958 loss= 0.9999999145150493\n",
            "iteration 2534: weight=1.000000020683942 loss= 0.9999999159004185\n",
            "iteration 2535: weight=1.0000000203482962 loss= 0.9999999172642341\n",
            "iteration 2536: weight=1.0000000200178796 loss= 0.999999918606817\n",
            "iteration 2537: weight=1.000000019692614 loss= 0.9999999199284832\n",
            "iteration 2538: weight=1.0000000193724228 loss= 0.9999999212295456\n",
            "iteration 2539: weight=1.00000001905723 loss= 0.9999999225103104\n",
            "iteration 2540: weight=1.0000000187469613 loss= 0.9999999237710814\n",
            "iteration 2541: weight=1.0000000184415427 loss= 0.9999999250121562\n",
            "iteration 2542: weight=1.000000018140902 loss= 0.9999999262338304\n",
            "iteration 2543: weight=1.0000000178449673 loss= 0.9999999274363938\n",
            "iteration 2544: weight=1.0000000175536683 loss= 0.9999999286201321\n",
            "iteration 2545: weight=1.0000000172669357 loss= 0.999999929785328\n",
            "iteration 2546: weight=1.0000000169847005 loss= 0.9999999309322586\n",
            "iteration 2547: weight=1.0000000167068956 loss= 0.999999932061199\n",
            "iteration 2548: weight=1.0000000164334544 loss= 0.9999999331724185\n",
            "iteration 2549: weight=1.000000016164311 loss= 0.9999999342661836\n",
            "iteration 2550: weight=1.0000000158994011 loss= 0.999999935342757\n",
            "iteration 2551: weight=1.0000000156386608 loss= 0.9999999364023965\n",
            "iteration 2552: weight=1.0000000153820272 loss= 0.9999999374453578\n",
            "iteration 2553: weight=1.0000000151294384 loss= 0.9999999384718922\n",
            "iteration 2554: weight=1.0000000148808335 loss= 0.9999999394822474\n",
            "iteration 2555: weight=1.0000000146361523 loss= 0.9999999404766671\n",
            "iteration 2556: weight=1.0000000143953354 loss= 0.9999999414553917\n",
            "iteration 2557: weight=1.0000000141583245 loss= 0.9999999424186593\n",
            "iteration 2558: weight=1.000000013925062 loss= 0.9999999433667027\n",
            "iteration 2559: weight=1.0000000136954912 loss= 0.9999999442997528\n",
            "iteration 2560: weight=1.000000013469556 loss= 0.999999945218036\n",
            "iteration 2561: weight=1.0000000132472011 loss= 0.9999999461217771\n",
            "iteration 2562: weight=1.0000000130283726 loss= 0.9999999470111962\n",
            "iteration 2563: weight=1.0000000128130169 loss= 0.9999999478865103\n",
            "iteration 2564: weight=1.0000000126010808 loss= 0.9999999487479332\n",
            "iteration 2565: weight=1.0000000123925128 loss= 0.9999999495956773\n",
            "iteration 2566: weight=1.0000000121872614 loss= 0.9999999504299495\n",
            "iteration 2567: weight=1.000000011985276 loss= 0.9999999512509549\n",
            "iteration 2568: weight=1.000000011786507 loss= 0.9999999520588961\n",
            "iteration 2569: weight=1.0000000115909056 loss= 0.9999999528539721\n",
            "iteration 2570: weight=1.0000000113984229 loss= 0.9999999536363783\n",
            "iteration 2571: weight=1.0000000112090115 loss= 0.9999999544063091\n",
            "iteration 2572: weight=1.0000000110226246 loss= 0.9999999551639546\n",
            "iteration 2573: weight=1.0000000108392157 loss= 0.9999999559095021\n",
            "iteration 2574: weight=1.0000000106587397 loss= 0.9999999566431375\n",
            "iteration 2575: weight=1.000000010481151 loss= 0.9999999573650418\n",
            "iteration 2576: weight=1.000000010306406 loss= 0.9999999580753962\n",
            "iteration 2577: weight=1.0000000101344606 loss= 0.9999999587743766\n",
            "iteration 2578: weight=1.0000000099652722 loss= 0.999999959462158\n",
            "iteration 2579: weight=1.000000009798798 loss= 0.9999999601389118\n",
            "iteration 2580: weight=1.0000000096349966 loss= 0.9999999608048084\n",
            "iteration 2581: weight=1.0000000094738268 loss= 0.999999961460014\n",
            "iteration 2582: weight=1.0000000093152484 loss= 0.999999962104693\n",
            "iteration 2583: weight=1.000000009159221 loss= 0.9999999627390069\n",
            "iteration 2584: weight=1.0000000090057055 loss= 0.9999999633631166\n",
            "iteration 2585: weight=1.0000000088546632 loss= 0.9999999639771783\n",
            "iteration 2586: weight=1.0000000087060559 loss= 0.9999999645813474\n",
            "iteration 2587: weight=1.000000008559846 loss= 0.9999999651757768\n",
            "iteration 2588: weight=1.0000000084159963 loss= 0.9999999657606166\n",
            "iteration 2589: weight=1.0000000082744704 loss= 0.999999966336015\n",
            "iteration 2590: weight=1.0000000081352323 loss= 0.9999999669021185\n",
            "iteration 2591: weight=1.0000000079982465 loss= 0.9999999674590712\n",
            "iteration 2592: weight=1.0000000078634783 loss= 0.9999999680070142\n",
            "iteration 2593: weight=1.000000007730893 loss= 0.999999968546087\n",
            "iteration 2594: weight=1.000000007600457 loss= 0.9999999690764281\n",
            "iteration 2595: weight=1.0000000074721367 loss= 0.9999999695981725\n",
            "iteration 2596: weight=1.0000000073458994 loss= 0.9999999701114535\n",
            "iteration 2597: weight=1.0000000072217126 loss= 0.9999999706164024\n",
            "iteration 2598: weight=1.0000000070995443 loss= 0.99999997111315\n",
            "iteration 2599: weight=1.000000006979363 loss= 0.9999999716018231\n",
            "iteration 2600: weight=1.000000006861138 loss= 0.9999999720825479\n",
            "iteration 2601: weight=1.0000000067448387 loss= 0.9999999725554478\n",
            "iteration 2602: weight=1.0000000066304349 loss= 0.9999999730206455\n",
            "iteration 2603: weight=1.0000000065178969 loss= 0.9999999734782608\n",
            "iteration 2604: weight=1.0000000064071957 loss= 0.9999999739284127\n",
            "iteration 2605: weight=1.0000000062983025 loss= 0.9999999743712175\n",
            "iteration 2606: weight=1.0000000061911891 loss= 0.9999999748067899\n",
            "iteration 2607: weight=1.0000000060858274 loss= 0.9999999752352436\n",
            "iteration 2608: weight=1.00000000598219 loss= 0.9999999756566905\n",
            "iteration 2609: weight=1.00000000588025 loss= 0.9999999760712398\n",
            "iteration 2610: weight=1.0000000057799807 loss= 0.9999999764789999\n",
            "iteration 2611: weight=1.0000000056813556 loss= 0.9999999768800772\n",
            "iteration 2612: weight=1.0000000055843492 loss= 0.9999999772745777\n",
            "iteration 2613: weight=1.0000000054889358 loss= 0.9999999776626033\n",
            "iteration 2614: weight=1.0000000053950904 loss= 0.9999999780442571\n",
            "iteration 2615: weight=1.0000000053027882 loss= 0.9999999784196386\n",
            "iteration 2616: weight=1.000000005212005 loss= 0.9999999787888473\n",
            "iteration 2617: weight=1.0000000051227167 loss= 0.9999999791519799\n",
            "iteration 2618: weight=1.0000000050348998 loss= 0.9999999795091333\n",
            "iteration 2619: weight=1.000000004948531 loss= 0.9999999798604008\n",
            "iteration 2620: weight=1.0000000048635873 loss= 0.9999999802058764\n",
            "iteration 2621: weight=1.0000000047800464 loss= 0.9999999805456509\n",
            "iteration 2622: weight=1.0000000046978859 loss= 0.9999999808798147\n",
            "iteration 2623: weight=1.0000000046170838 loss= 0.9999999812084567\n",
            "iteration 2624: weight=1.0000000045376187 loss= 0.9999999815316648\n",
            "iteration 2625: weight=1.0000000044594695 loss= 0.9999999818495252\n",
            "iteration 2626: weight=1.0000000043826152 loss= 0.9999999821621223\n",
            "iteration 2627: weight=1.0000000043070352 loss= 0.9999999824695395\n",
            "iteration 2628: weight=1.0000000042327093 loss= 0.9999999827718594\n",
            "iteration 2629: weight=1.0000000041596173 loss= 0.9999999830691629\n",
            "iteration 2630: weight=1.00000000408774 loss= 0.9999999833615308\n",
            "iteration 2631: weight=1.0000000040170576 loss= 0.9999999836490404\n",
            "iteration 2632: weight=1.0000000039475512 loss= 0.9999999839317698\n",
            "iteration 2633: weight=1.000000003879202 loss= 0.9999999842097954\n",
            "iteration 2634: weight=1.0000000038119916 loss= 0.9999999844831918\n",
            "iteration 2635: weight=1.0000000037459018 loss= 0.9999999847520337\n",
            "iteration 2636: weight=1.0000000036809145 loss= 0.9999999850163929\n",
            "iteration 2637: weight=1.0000000036170122 loss= 0.9999999852763422\n",
            "iteration 2638: weight=1.0000000035541776 loss= 0.999999985531951\n",
            "iteration 2639: weight=1.0000000034923933 loss= 0.9999999857832895\n",
            "iteration 2640: weight=1.0000000034316425 loss= 0.999999986030427\n",
            "iteration 2641: weight=1.000000003371909 loss= 0.9999999862734299\n",
            "iteration 2642: weight=1.000000003313176 loss= 0.9999999865123641\n",
            "iteration 2643: weight=1.0000000032554277 loss= 0.9999999867472962\n",
            "iteration 2644: weight=1.0000000031986482 loss= 0.9999999869782892\n",
            "iteration 2645: weight=1.0000000031428218 loss= 0.9999999872054071\n",
            "iteration 2646: weight=1.0000000030879332 loss= 0.9999999874287129\n",
            "iteration 2647: weight=1.0000000030339673 loss= 0.999999987648267\n",
            "iteration 2648: weight=1.0000000029809093 loss= 0.9999999878641308\n",
            "iteration 2649: weight=1.0000000029287444 loss= 0.9999999880763628\n",
            "iteration 2650: weight=1.000000002877458 loss= 0.9999999882850226\n",
            "iteration 2651: weight=1.0000000028270364 loss= 0.9999999884901678\n",
            "iteration 2652: weight=1.0000000027774651 loss= 0.9999999886918545\n",
            "iteration 2653: weight=1.0000000027287308 loss= 0.9999999888901394\n",
            "iteration 2654: weight=1.0000000026808196 loss= 0.9999999890850768\n",
            "iteration 2655: weight=1.0000000026337181 loss= 0.9999999892767217\n",
            "iteration 2656: weight=1.0000000025874134 loss= 0.9999999894651275\n",
            "iteration 2657: weight=1.0000000025418925 loss= 0.9999999896503464\n",
            "iteration 2658: weight=1.0000000024971425 loss= 0.9999999898324301\n",
            "iteration 2659: weight=1.000000002453151 loss= 0.99999999001143\n",
            "iteration 2660: weight=1.0000000024099058 loss= 0.9999999901873959\n",
            "iteration 2661: weight=1.0000000023673945 loss= 0.9999999903603767\n",
            "iteration 2662: weight=1.0000000023256053 loss= 0.999999990530422\n",
            "iteration 2663: weight=1.0000000022845263 loss= 0.9999999906975789\n",
            "iteration 2664: weight=1.0000000022441462 loss= 0.9999999908618946\n",
            "iteration 2665: weight=1.0000000022044533 loss= 0.9999999910234152\n",
            "iteration 2666: weight=1.0000000021654363 loss= 0.9999999911821869\n",
            "iteration 2667: weight=1.0000000021270843 loss= 0.9999999913382549\n",
            "iteration 2668: weight=1.0000000020893864 loss= 0.9999999914916629\n",
            "iteration 2669: weight=1.0000000020523319 loss= 0.9999999916424542\n",
            "iteration 2670: weight=1.00000000201591 loss= 0.9999999917906726\n",
            "iteration 2671: weight=1.000000001980111 loss= 0.9999999919363596\n",
            "iteration 2672: weight=1.000000001944924 loss= 0.9999999920795561\n",
            "iteration 2673: weight=1.0000000019103392 loss= 0.999999992220304\n",
            "iteration 2674: weight=1.0000000018763466 loss= 0.9999999923586431\n",
            "iteration 2675: weight=1.0000000018429367 loss= 0.9999999924946135\n",
            "iteration 2676: weight=1.0000000018100996 loss= 0.9999999926282532\n",
            "iteration 2677: weight=1.000000001777826 loss= 0.9999999927596015\n",
            "iteration 2678: weight=1.0000000017461068 loss= 0.9999999928886956\n",
            "iteration 2679: weight=1.0000000017149326 loss= 0.9999999930155727\n",
            "iteration 2680: weight=1.0000000016842945 loss= 0.9999999931402694\n",
            "iteration 2681: weight=1.0000000016541837 loss= 0.9999999932628221\n",
            "iteration 2682: weight=1.0000000016245914 loss= 0.9999999933832653\n",
            "iteration 2683: weight=1.000000001595509 loss= 0.9999999935016346\n",
            "iteration 2684: weight=1.0000000015669281 loss= 0.9999999936179638\n",
            "iteration 2685: weight=1.0000000015388404 loss= 0.9999999937322874\n",
            "iteration 2686: weight=1.0000000015112378 loss= 0.9999999938446384\n",
            "iteration 2687: weight=1.0000000014841122 loss= 0.9999999939550488\n",
            "iteration 2688: weight=1.0000000014574555 loss= 0.9999999940635513\n",
            "iteration 2689: weight=1.0000000014312602 loss= 0.999999994170178\n",
            "iteration 2690: weight=1.0000000014055184 loss= 0.9999999942749591\n",
            "iteration 2691: weight=1.0000000013802226 loss= 0.9999999943779265\n",
            "iteration 2692: weight=1.0000000013553656 loss= 0.9999999944791096\n",
            "iteration 2693: weight=1.0000000013309398 loss= 0.9999999945785376\n",
            "iteration 2694: weight=1.0000000013069381 loss= 0.9999999946762408\n",
            "iteration 2695: weight=1.0000000012833534 loss= 0.9999999947722475\n",
            "iteration 2696: weight=1.0000000012601789 loss= 0.9999999948665863\n",
            "iteration 2697: weight=1.0000000012374075 loss= 0.9999999949592846\n",
            "iteration 2698: weight=1.0000000012150327 loss= 0.99999999505037\n",
            "iteration 2699: weight=1.0000000011930479 loss= 0.999999995139869\n",
            "iteration 2700: weight=1.0000000011714463 loss= 0.9999999952278085\n",
            "iteration 2701: weight=1.0000000011502215 loss= 0.9999999953142149\n",
            "iteration 2702: weight=1.0000000011293673 loss= 0.9999999953991141\n",
            "iteration 2703: weight=1.0000000011088774 loss= 0.9999999954825309\n",
            "iteration 2704: weight=1.000000001088746 loss= 0.9999999955644903\n",
            "iteration 2705: weight=1.0000000010689667 loss= 0.9999999956450161\n",
            "iteration 2706: weight=1.0000000010495336 loss= 0.9999999957241332\n",
            "iteration 2707: weight=1.000000001030441 loss= 0.9999999958018657\n",
            "iteration 2708: weight=1.0000000010116832 loss= 0.9999999958782357\n",
            "iteration 2709: weight=1.0000000009932544 loss= 0.9999999959532673\n",
            "iteration 2710: weight=1.0000000009751493 loss= 0.9999999960269825\n",
            "iteration 2711: weight=1.0000000009573622 loss= 0.9999999960994028\n",
            "iteration 2712: weight=1.0000000009398877 loss= 0.9999999961705512\n",
            "iteration 2713: weight=1.0000000009227208 loss= 0.9999999962404491\n",
            "iteration 2714: weight=1.000000000905856 loss= 0.9999999963091168\n",
            "iteration 2715: weight=1.0000000008892884 loss= 0.9999999963765758\n",
            "iteration 2716: weight=1.000000000873013 loss= 0.9999999964428463\n",
            "iteration 2717: weight=1.0000000008570247 loss= 0.999999996507948\n",
            "iteration 2718: weight=1.0000000008413186 loss= 0.9999999965719013\n",
            "iteration 2719: weight=1.00000000082589 loss= 0.9999999966347257\n",
            "iteration 2720: weight=1.0000000008107341 loss= 0.9999999966964399\n",
            "iteration 2721: weight=1.0000000007958463 loss= 0.9999999967570634\n",
            "iteration 2722: weight=1.000000000781222 loss= 0.9999999968166149\n",
            "iteration 2723: weight=1.0000000007668568 loss= 0.9999999968751121\n",
            "iteration 2724: weight=1.0000000007527463 loss= 0.9999999969325728\n",
            "iteration 2725: weight=1.000000000738886 loss= 0.9999999969890148\n",
            "iteration 2726: weight=1.000000000725272 loss= 0.9999999970444557\n",
            "iteration 2727: weight=1.0000000007118999 loss= 0.9999999970989117\n",
            "iteration 2728: weight=1.0000000006987653 loss= 0.9999999971524005\n",
            "iteration 2729: weight=1.0000000006858645 loss= 0.9999999972049389\n",
            "iteration 2730: weight=1.0000000006731933 loss= 0.9999999972565421\n",
            "iteration 2731: weight=1.0000000006607477 loss= 0.9999999973072269\n",
            "iteration 2732: weight=1.0000000006485241 loss= 0.9999999973570093\n",
            "iteration 2733: weight=1.0000000006365186 loss= 0.9999999974059035\n",
            "iteration 2734: weight=1.0000000006247274 loss= 0.9999999974539255\n",
            "iteration 2735: weight=1.0000000006131466 loss= 0.9999999975010905\n",
            "iteration 2736: weight=1.000000000601773 loss= 0.9999999975474134\n",
            "iteration 2737: weight=1.000000000590603 loss= 0.9999999975929077\n",
            "iteration 2738: weight=1.0000000005796326 loss= 0.9999999976375884\n",
            "iteration 2739: weight=1.0000000005688587 loss= 0.9999999976814697\n",
            "iteration 2740: weight=1.000000000558278 loss= 0.999999997724565\n",
            "iteration 2741: weight=1.0000000005478873 loss= 0.9999999977668876\n",
            "iteration 2742: weight=1.000000000537683 loss= 0.9999999978084508\n",
            "iteration 2743: weight=1.000000000527662 loss= 0.999999997849268\n",
            "iteration 2744: weight=1.0000000005178211 loss= 0.9999999978893523\n",
            "iteration 2745: weight=1.0000000005081573 loss= 0.9999999979287155\n",
            "iteration 2746: weight=1.0000000004986673 loss= 0.9999999979673708\n",
            "iteration 2747: weight=1.0000000004893483 loss= 0.9999999980053307\n",
            "iteration 2748: weight=1.0000000004801974 loss= 0.9999999980426066\n",
            "iteration 2749: weight=1.0000000004712115 loss= 0.9999999980792103\n",
            "iteration 2750: weight=1.0000000004623877 loss= 0.9999999981151539\n",
            "iteration 2751: weight=1.0000000004537233 loss= 0.9999999981504493\n",
            "iteration 2752: weight=1.0000000004452156 loss= 0.9999999981851069\n",
            "iteration 2753: weight=1.0000000004368619 loss= 0.9999999982191374\n",
            "iteration 2754: weight=1.0000000004286593 loss= 0.9999999982525525\n",
            "iteration 2755: weight=1.0000000004206053 loss= 0.9999999982853627\n",
            "iteration 2756: weight=1.0000000004126972 loss= 0.9999999983175787\n",
            "iteration 2757: weight=1.0000000004049325 loss= 0.9999999983492112\n",
            "iteration 2758: weight=1.0000000003973089 loss= 0.9999999983802699\n",
            "iteration 2759: weight=1.0000000003898237 loss= 0.9999999984107646\n",
            "iteration 2760: weight=1.0000000003824745 loss= 0.9999999984407051\n",
            "iteration 2761: weight=1.000000000375259 loss= 0.999999998470102\n",
            "iteration 2762: weight=1.0000000003681748 loss= 0.9999999984989643\n",
            "iteration 2763: weight=1.0000000003612197 loss= 0.9999999985273007\n",
            "iteration 2764: weight=1.0000000003543914 loss= 0.9999999985551211\n",
            "iteration 2765: weight=1.0000000003476877 loss= 0.9999999985824344\n",
            "iteration 2766: weight=1.0000000003411063 loss= 0.9999999986092494\n",
            "iteration 2767: weight=1.000000000334645 loss= 0.999999998635575\n",
            "iteration 2768: weight=1.0000000003283018 loss= 0.9999999986614201\n",
            "iteration 2769: weight=1.0000000003220748 loss= 0.9999999986867927\n",
            "iteration 2770: weight=1.0000000003159617 loss= 0.9999999987117008\n",
            "iteration 2771: weight=1.0000000003099605 loss= 0.9999999987361532\n",
            "iteration 2772: weight=1.0000000003040692 loss= 0.999999998760158\n",
            "iteration 2773: weight=1.000000000298286 loss= 0.9999999987837231\n",
            "iteration 2774: weight=1.000000000292609 loss= 0.9999999988068557\n",
            "iteration 2775: weight=1.0000000002870364 loss= 0.9999999988295638\n",
            "iteration 2776: weight=1.000000000281566 loss= 0.9999999988518544\n",
            "iteration 2777: weight=1.0000000002761964 loss= 0.9999999988737356\n",
            "iteration 2778: weight=1.0000000002709255 loss= 0.9999999988952144\n",
            "iteration 2779: weight=1.0000000002657516 loss= 0.999999998916298\n",
            "iteration 2780: weight=1.0000000002606733 loss= 0.9999999989369934\n",
            "iteration 2781: weight=1.0000000002556886 loss= 0.999999998957307\n",
            "iteration 2782: weight=1.0000000002507958 loss= 0.9999999989772457\n",
            "iteration 2783: weight=1.0000000002459934 loss= 0.9999999989968167\n",
            "iteration 2784: weight=1.0000000002412799 loss= 0.9999999990160262\n",
            "iteration 2785: weight=1.0000000002366534 loss= 0.9999999990348805\n",
            "iteration 2786: weight=1.0000000002321126 loss= 0.9999999990533865\n",
            "iteration 2787: weight=1.000000000227656 loss= 0.9999999990715498\n",
            "iteration 2788: weight=1.0000000002232818 loss= 0.9999999990893764\n",
            "iteration 2789: weight=1.0000000002189888 loss= 0.9999999991068727\n",
            "iteration 2790: weight=1.0000000002147755 loss= 0.9999999991240447\n",
            "iteration 2791: weight=1.0000000002106404 loss= 0.9999999991408979\n",
            "iteration 2792: weight=1.0000000002065823 loss= 0.9999999991574384\n",
            "iteration 2793: weight=1.0000000002025995 loss= 0.9999999991736708\n",
            "iteration 2794: weight=1.0000000001986908 loss= 0.999999999189602\n",
            "iteration 2795: weight=1.000000000194855 loss= 0.9999999992052366\n",
            "iteration 2796: weight=1.0000000001910907 loss= 0.9999999992205799\n",
            "iteration 2797: weight=1.0000000001873965 loss= 0.9999999992356372\n",
            "iteration 2798: weight=1.0000000001837714 loss= 0.9999999992504138\n",
            "iteration 2799: weight=1.000000000180214 loss= 0.9999999992649142\n",
            "iteration 2800: weight=1.000000000176723 loss= 0.9999999992791437\n",
            "iteration 2801: weight=1.0000000001732974 loss= 0.9999999992931077\n",
            "iteration 2802: weight=1.0000000001699358 loss= 0.9999999993068105\n",
            "iteration 2803: weight=1.0000000001666374 loss= 0.9999999993202566\n",
            "iteration 2804: weight=1.0000000001634006 loss= 0.9999999993334505\n",
            "iteration 2805: weight=1.0000000001602245 loss= 0.9999999993463975\n",
            "iteration 2806: weight=1.000000000157108 loss= 0.999999999359102\n",
            "iteration 2807: weight=1.00000000015405 loss= 0.9999999993715676\n",
            "iteration 2808: weight=1.0000000001510496 loss= 0.9999999993837996\n",
            "iteration 2809: weight=1.0000000001481055 loss= 0.9999999993958015\n",
            "iteration 2810: weight=1.000000000145217 loss= 0.9999999994075779\n",
            "iteration 2811: weight=1.0000000001423828 loss= 0.9999999994191322\n",
            "iteration 2812: weight=1.000000000139602 loss= 0.9999999994304689\n",
            "iteration 2813: weight=1.0000000001368738 loss= 0.9999999994415916\n",
            "iteration 2814: weight=1.000000000134197 loss= 0.9999999994525046\n",
            "iteration 2815: weight=1.0000000001315708 loss= 0.9999999994632116\n",
            "iteration 2816: weight=1.0000000001289941 loss= 0.999999999473717\n",
            "iteration 2817: weight=1.0000000001264662 loss= 0.9999999994840234\n",
            "iteration 2818: weight=1.0000000001239862 loss= 0.9999999994941353\n",
            "iteration 2819: weight=1.000000000121553 loss= 0.9999999995040554\n",
            "iteration 2820: weight=1.000000000119166 loss= 0.999999999513788\n",
            "iteration 2821: weight=1.0000000001168243 loss= 0.999999999523336\n",
            "iteration 2822: weight=1.000000000114527 loss= 0.9999999995327027\n",
            "iteration 2823: weight=1.0000000001122735 loss= 0.9999999995418918\n",
            "iteration 2824: weight=1.0000000001100628 loss= 0.9999999995509059\n",
            "iteration 2825: weight=1.0000000001078941 loss= 0.9999999995597486\n",
            "iteration 2826: weight=1.0000000001057667 loss= 0.9999999995684234\n",
            "iteration 2827: weight=1.0000000001036797 loss= 0.9999999995769331\n",
            "iteration 2828: weight=1.0000000001016327 loss= 0.9999999995852811\n",
            "iteration 2829: weight=1.0000000000996245 loss= 0.9999999995934692\n",
            "iteration 2830: weight=1.0000000000976548 loss= 0.9999999996015019\n",
            "iteration 2831: weight=1.0000000000957225 loss= 0.9999999996093809\n",
            "iteration 2832: weight=1.0000000000938274 loss= 0.9999999996171098\n",
            "iteration 2833: weight=1.0000000000919684 loss= 0.9999999996246904\n",
            "iteration 2834: weight=1.000000000090145 loss= 0.9999999996321263\n",
            "iteration 2835: weight=1.0000000000883567 loss= 0.99999999963942\n",
            "iteration 2836: weight=1.0000000000866025 loss= 0.9999999996465734\n",
            "iteration 2837: weight=1.000000000084882 loss= 0.99999999965359\n",
            "iteration 2838: weight=1.0000000000831946 loss= 0.9999999996604716\n",
            "iteration 2839: weight=1.0000000000815394 loss= 0.9999999996672218\n",
            "iteration 2840: weight=1.0000000000799163 loss= 0.9999999996738422\n",
            "iteration 2841: weight=1.0000000000783242 loss= 0.9999999996803348\n",
            "iteration 2842: weight=1.0000000000767628 loss= 0.9999999996867031\n",
            "iteration 2843: weight=1.0000000000752316 loss= 0.9999999996929487\n",
            "iteration 2844: weight=1.00000000007373 loss= 0.9999999996990736\n",
            "iteration 2845: weight=1.000000000072257 loss= 0.9999999997050804\n",
            "iteration 2846: weight=1.0000000000708127 loss= 0.9999999997109716\n",
            "iteration 2847: weight=1.0000000000693963 loss= 0.9999999997167492\n",
            "iteration 2848: weight=1.0000000000680072 loss= 0.9999999997224149\n",
            "iteration 2849: weight=1.000000000066645 loss= 0.9999999997279714\n",
            "iteration 2850: weight=1.000000000065309 loss= 0.9999999997334204\n",
            "iteration 2851: weight=1.000000000063999 loss= 0.9999999997387636\n",
            "iteration 2852: weight=1.0000000000627145 loss= 0.9999999997440039\n",
            "iteration 2853: weight=1.0000000000614548 loss= 0.999999999749142\n",
            "iteration 2854: weight=1.0000000000602196 loss= 0.9999999997541806\n",
            "iteration 2855: weight=1.0000000000590084 loss= 0.9999999997591216\n",
            "iteration 2856: weight=1.0000000000578206 loss= 0.9999999997639666\n",
            "iteration 2857: weight=1.000000000056656 loss= 0.9999999997687175\n",
            "iteration 2858: weight=1.000000000055514 loss= 0.999999999773376\n",
            "iteration 2859: weight=1.0000000000543943 loss= 0.9999999997779438\n",
            "iteration 2860: weight=1.0000000000532965 loss= 0.9999999997824229\n",
            "iteration 2861: weight=1.00000000005222 loss= 0.9999999997868141\n",
            "iteration 2862: weight=1.0000000000511646 loss= 0.99999999979112\n",
            "iteration 2863: weight=1.00000000005013 loss= 0.9999999997953415\n",
            "iteration 2864: weight=1.0000000000491154 loss= 0.9999999997994804\n",
            "iteration 2865: weight=1.0000000000481206 loss= 0.9999999998035385\n",
            "iteration 2866: weight=1.0000000000471454 loss= 0.9999999998075175\n",
            "iteration 2867: weight=1.0000000000461893 loss= 0.9999999998114184\n",
            "iteration 2868: weight=1.0000000000452518 loss= 0.9999999998152429\n",
            "iteration 2869: weight=1.0000000000443328 loss= 0.9999999998189928\n",
            "iteration 2870: weight=1.0000000000434317 loss= 0.999999999822669\n",
            "iteration 2871: weight=1.0000000000425484 loss= 0.9999999998262732\n",
            "iteration 2872: weight=1.0000000000416824 loss= 0.9999999998298064\n",
            "iteration 2873: weight=1.0000000000408336 loss= 0.9999999998332703\n",
            "iteration 2874: weight=1.0000000000400013 loss= 0.9999999998366658\n",
            "iteration 2875: weight=1.0000000000391855 loss= 0.9999999998399947\n",
            "iteration 2876: weight=1.000000000038386 loss= 0.9999999998432578\n",
            "iteration 2877: weight=1.0000000000376021 loss= 0.9999999998464562\n",
            "iteration 2878: weight=1.0000000000368336 loss= 0.9999999998495914\n",
            "iteration 2879: weight=1.0000000000360805 loss= 0.9999999998526654\n",
            "iteration 2880: weight=1.0000000000353422 loss= 0.9999999998556781\n",
            "iteration 2881: weight=1.0000000000346185 loss= 0.9999999998586313\n",
            "iteration 2882: weight=1.000000000033909 loss= 0.9999999998615259\n",
            "iteration 2883: weight=1.0000000000332139 loss= 0.9999999998643636\n",
            "iteration 2884: weight=1.0000000000325324 loss= 0.9999999998671445\n",
            "iteration 2885: weight=1.0000000000318645 loss= 0.9999999998698703\n",
            "iteration 2886: weight=1.0000000000312097 loss= 0.999999999872542\n",
            "iteration 2887: weight=1.000000000030568 loss= 0.9999999998751612\n",
            "iteration 2888: weight=1.000000000029939 loss= 0.999999999877728\n",
            "iteration 2889: weight=1.0000000000293225 loss= 0.9999999998802442\n",
            "iteration 2890: weight=1.0000000000287184 loss= 0.9999999998827098\n",
            "iteration 2891: weight=1.0000000000281262 loss= 0.9999999998851266\n",
            "iteration 2892: weight=1.0000000000275457 loss= 0.9999999998874953\n",
            "iteration 2893: weight=1.000000000026977 loss= 0.999999999889817\n",
            "iteration 2894: weight=1.0000000000264198 loss= 0.9999999998920917\n",
            "iteration 2895: weight=1.0000000000258735 loss= 0.999999999894321\n",
            "iteration 2896: weight=1.0000000000253382 loss= 0.9999999998965059\n",
            "iteration 2897: weight=1.0000000000248135 loss= 0.9999999998986473\n",
            "iteration 2898: weight=1.0000000000242995 loss= 0.9999999999007461\n",
            "iteration 2899: weight=1.0000000000237956 loss= 0.9999999999028022\n",
            "iteration 2900: weight=1.0000000000233018 loss= 0.9999999999048175\n",
            "iteration 2901: weight=1.000000000022818 loss= 0.9999999999067928\n",
            "iteration 2902: weight=1.000000000022344 loss= 0.9999999999087281\n",
            "iteration 2903: weight=1.0000000000218794 loss= 0.9999999999106244\n",
            "iteration 2904: weight=1.0000000000214242 loss= 0.9999999999124825\n",
            "iteration 2905: weight=1.000000000020978 loss= 0.9999999999143032\n",
            "iteration 2906: weight=1.0000000000205411 loss= 0.9999999999160876\n",
            "iteration 2907: weight=1.0000000000201128 loss= 0.9999999999178355\n",
            "iteration 2908: weight=1.0000000000196931 loss= 0.9999999999195488\n",
            "iteration 2909: weight=1.000000000019282 loss= 0.9999999999212275\n",
            "iteration 2910: weight=1.0000000000188791 loss= 0.9999999999228724\n",
            "iteration 2911: weight=1.0000000000184843 loss= 0.9999999999244835\n",
            "iteration 2912: weight=1.0000000000180975 loss= 0.9999999999260627\n",
            "iteration 2913: weight=1.0000000000177187 loss= 0.9999999999276099\n",
            "iteration 2914: weight=1.0000000000173475 loss= 0.9999999999291251\n",
            "iteration 2915: weight=1.0000000000169837 loss= 0.9999999999306102\n",
            "iteration 2916: weight=1.0000000000166274 loss= 0.999999999932065\n",
            "iteration 2917: weight=1.0000000000162783 loss= 0.9999999999334905\n",
            "iteration 2918: weight=1.0000000000159364 loss= 0.9999999999348868\n",
            "iteration 2919: weight=1.0000000000156013 loss= 0.9999999999362545\n",
            "iteration 2920: weight=1.0000000000152731 loss= 0.9999999999375948\n",
            "iteration 2921: weight=1.0000000000149516 loss= 0.9999999999389075\n",
            "iteration 2922: weight=1.0000000000146365 loss= 0.9999999999401936\n",
            "iteration 2923: weight=1.0000000000143279 loss= 0.9999999999414539\n",
            "iteration 2924: weight=1.0000000000140257 loss= 0.9999999999426885\n",
            "iteration 2925: weight=1.0000000000137295 loss= 0.9999999999438973\n",
            "iteration 2926: weight=1.0000000000134395 loss= 0.9999999999450822\n",
            "iteration 2927: weight=1.0000000000131553 loss= 0.9999999999462421\n",
            "iteration 2928: weight=1.000000000012877 loss= 0.999999999947379\n",
            "iteration 2929: weight=1.0000000000126044 loss= 0.9999999999484919\n",
            "iteration 2930: weight=1.0000000000123372 loss= 0.9999999999495826\n",
            "iteration 2931: weight=1.0000000000120757 loss= 0.999999999950651\n",
            "iteration 2932: weight=1.0000000000118194 loss= 0.9999999999516973\n",
            "iteration 2933: weight=1.0000000000115685 loss= 0.9999999999527223\n",
            "iteration 2934: weight=1.0000000000113227 loss= 0.9999999999537259\n",
            "iteration 2935: weight=1.000000000011082 loss= 0.9999999999547091\n",
            "iteration 2936: weight=1.0000000000108462 loss= 0.9999999999556719\n",
            "iteration 2937: weight=1.0000000000106153 loss= 0.9999999999566151\n",
            "iteration 2938: weight=1.000000000010389 loss= 0.9999999999575389\n",
            "iteration 2939: weight=1.0000000000101674 loss= 0.9999999999584439\n",
            "iteration 2940: weight=1.0000000000099505 loss= 0.9999999999593303\n",
            "iteration 2941: weight=1.000000000009738 loss= 0.9999999999601981\n",
            "iteration 2942: weight=1.00000000000953 loss= 0.999999999961048\n",
            "iteration 2943: weight=1.000000000009326 loss= 0.9999999999618803\n",
            "iteration 2944: weight=1.0000000000091265 loss= 0.9999999999626956\n",
            "iteration 2945: weight=1.000000000008931 loss= 0.9999999999634941\n",
            "iteration 2946: weight=1.0000000000087397 loss= 0.9999999999642757\n",
            "iteration 2947: weight=1.0000000000085523 loss= 0.9999999999650413\n",
            "iteration 2948: weight=1.0000000000083689 loss= 0.9999999999657909\n",
            "iteration 2949: weight=1.0000000000081892 loss= 0.9999999999665246\n",
            "iteration 2950: weight=1.0000000000080134 loss= 0.9999999999672431\n",
            "iteration 2951: weight=1.000000000007841 loss= 0.9999999999679465\n",
            "iteration 2952: weight=1.0000000000076723 loss= 0.9999999999686358\n",
            "iteration 2953: weight=1.000000000007507 loss= 0.9999999999693108\n",
            "iteration 2954: weight=1.0000000000073455 loss= 0.9999999999699716\n",
            "iteration 2955: weight=1.0000000000071871 loss= 0.9999999999706182\n",
            "iteration 2956: weight=1.0000000000070322 loss= 0.9999999999712514\n",
            "iteration 2957: weight=1.0000000000068803 loss= 0.9999999999718714\n",
            "iteration 2958: weight=1.0000000000067317 loss= 0.9999999999724789\n",
            "iteration 2959: weight=1.0000000000065863 loss= 0.9999999999730731\n",
            "iteration 2960: weight=1.0000000000064437 loss= 0.9999999999736549\n",
            "iteration 2961: weight=1.0000000000063043 loss= 0.9999999999742251\n",
            "iteration 2962: weight=1.0000000000061677 loss= 0.9999999999747828\n",
            "iteration 2963: weight=1.000000000006034 loss= 0.9999999999753291\n",
            "iteration 2964: weight=1.000000000005903 loss= 0.9999999999758638\n",
            "iteration 2965: weight=1.000000000005775 loss= 0.9999999999763878\n",
            "iteration 2966: weight=1.0000000000056495 loss= 0.9999999999769003\n",
            "iteration 2967: weight=1.0000000000055267 loss= 0.9999999999774021\n",
            "iteration 2968: weight=1.0000000000054063 loss= 0.9999999999778932\n",
            "iteration 2969: weight=1.0000000000052887 loss= 0.9999999999783746\n",
            "iteration 2970: weight=1.0000000000051734 loss= 0.9999999999788454\n",
            "iteration 2971: weight=1.0000000000050606 loss= 0.9999999999793063\n",
            "iteration 2972: weight=1.0000000000049503 loss= 0.9999999999797575\n",
            "iteration 2973: weight=1.0000000000048421 loss= 0.999999999980199\n",
            "iteration 2974: weight=1.0000000000047364 loss= 0.9999999999806315\n",
            "iteration 2975: weight=1.000000000004633 loss= 0.9999999999810543\n",
            "iteration 2976: weight=1.0000000000045315 loss= 0.9999999999814682\n",
            "iteration 2977: weight=1.0000000000044322 loss= 0.999999999981874\n",
            "iteration 2978: weight=1.0000000000043352 loss= 0.9999999999822711\n",
            "iteration 2979: weight=1.0000000000042402 loss= 0.9999999999826592\n",
            "iteration 2980: weight=1.0000000000041471 loss= 0.9999999999830393\n",
            "iteration 2981: weight=1.000000000004056 loss= 0.9999999999834115\n",
            "iteration 2982: weight=1.000000000003967 loss= 0.9999999999837756\n",
            "iteration 2983: weight=1.0000000000038798 loss= 0.9999999999841318\n",
            "iteration 2984: weight=1.0000000000037945 loss= 0.9999999999844809\n",
            "iteration 2985: weight=1.000000000003711 loss= 0.9999999999848219\n",
            "iteration 2986: weight=1.0000000000036293 loss= 0.9999999999851559\n",
            "iteration 2987: weight=1.0000000000035494 loss= 0.9999999999854827\n",
            "iteration 2988: weight=1.000000000003471 loss= 0.9999999999858025\n",
            "iteration 2989: weight=1.0000000000033944 loss= 0.999999999986116\n",
            "iteration 2990: weight=1.0000000000033193 loss= 0.9999999999864224\n",
            "iteration 2991: weight=1.0000000000032458 loss= 0.9999999999867226\n",
            "iteration 2992: weight=1.0000000000031741 loss= 0.9999999999870166\n",
            "iteration 2993: weight=1.0000000000031037 loss= 0.9999999999873035\n",
            "iteration 2994: weight=1.000000000003035 loss= 0.999999999987585\n",
            "iteration 2995: weight=1.0000000000029676 loss= 0.9999999999878604\n",
            "iteration 2996: weight=1.0000000000029017 loss= 0.9999999999881295\n",
            "iteration 2997: weight=1.0000000000028373 loss= 0.9999999999883933\n",
            "iteration 2998: weight=1.0000000000027742 loss= 0.9999999999886509\n",
            "iteration 2999: weight=1.0000000000027125 loss= 0.9999999999889031\n",
            "iteration 3000: weight=1.000000000002652 loss= 0.99999999998915\n",
            "iteration 3001: weight=1.000000000002593 loss= 0.9999999999893916\n",
            "iteration 3002: weight=1.0000000000025353 loss= 0.9999999999896279\n",
            "iteration 3003: weight=1.0000000000024787 loss= 0.9999999999898588\n",
            "iteration 3004: weight=1.0000000000024234 loss= 0.9999999999900853\n",
            "iteration 3005: weight=1.0000000000023692 loss= 0.9999999999903064\n",
            "iteration 3006: weight=1.0000000000023164 loss= 0.9999999999905231\n",
            "iteration 3007: weight=1.0000000000022646 loss= 0.9999999999907345\n",
            "iteration 3008: weight=1.000000000002214 loss= 0.9999999999909415\n",
            "iteration 3009: weight=1.0000000000021645 loss= 0.999999999991144\n",
            "iteration 3010: weight=1.000000000002116 loss= 0.999999999991342\n",
            "iteration 3011: weight=1.0000000000020686 loss= 0.9999999999915357\n",
            "iteration 3012: weight=1.0000000000020222 loss= 0.9999999999917257\n",
            "iteration 3013: weight=1.0000000000019766 loss= 0.9999999999919114\n",
            "iteration 3014: weight=1.0000000000019322 loss= 0.9999999999920934\n",
            "iteration 3015: weight=1.0000000000018887 loss= 0.9999999999922711\n",
            "iteration 3016: weight=1.0000000000018463 loss= 0.9999999999924452\n",
            "iteration 3017: weight=1.0000000000018048 loss= 0.9999999999926148\n",
            "iteration 3018: weight=1.0000000000017641 loss= 0.9999999999927809\n",
            "iteration 3019: weight=1.0000000000017244 loss= 0.9999999999929434\n",
            "iteration 3020: weight=1.0000000000016855 loss= 0.9999999999931024\n",
            "iteration 3021: weight=1.0000000000016476 loss= 0.9999999999932578\n",
            "iteration 3022: weight=1.0000000000016103 loss= 0.9999999999934097\n",
            "iteration 3023: weight=1.0000000000015739 loss= 0.9999999999935589\n",
            "iteration 3024: weight=1.0000000000015383 loss= 0.9999999999937046\n",
            "iteration 3025: weight=1.0000000000015035 loss= 0.9999999999938467\n",
            "iteration 3026: weight=1.0000000000014695 loss= 0.9999999999939861\n",
            "iteration 3027: weight=1.0000000000014362 loss= 0.999999999994122\n",
            "iteration 3028: weight=1.0000000000014035 loss= 0.9999999999942553\n",
            "iteration 3029: weight=1.0000000000013716 loss= 0.9999999999943858\n",
            "iteration 3030: weight=1.0000000000013405 loss= 0.9999999999945137\n",
            "iteration 3031: weight=1.00000000000131 loss= 0.9999999999946381\n",
            "iteration 3032: weight=1.0000000000012803 loss= 0.9999999999947597\n",
            "iteration 3033: weight=1.0000000000012512 loss= 0.9999999999948788\n",
            "iteration 3034: weight=1.0000000000012228 loss= 0.9999999999949951\n",
            "iteration 3035: weight=1.0000000000011948 loss= 0.9999999999951088\n",
            "iteration 3036: weight=1.0000000000011675 loss= 0.9999999999952207\n",
            "iteration 3037: weight=1.0000000000011409 loss= 0.99999999999533\n",
            "iteration 3038: weight=1.0000000000011149 loss= 0.9999999999954365\n",
            "iteration 3039: weight=1.0000000000010894 loss= 0.9999999999955405\n",
            "iteration 3040: weight=1.0000000000010645 loss= 0.9999999999956426\n",
            "iteration 3041: weight=1.00000000000104 loss= 0.9999999999957421\n",
            "iteration 3042: weight=1.0000000000010163 loss= 0.9999999999958398\n",
            "iteration 3043: weight=1.000000000000993 loss= 0.9999999999959348\n",
            "iteration 3044: weight=1.0000000000009701 loss= 0.9999999999960281\n",
            "iteration 3045: weight=1.000000000000948 loss= 0.9999999999961195\n",
            "iteration 3046: weight=1.0000000000009261 loss= 0.9999999999962084\n",
            "iteration 3047: weight=1.0000000000009048 loss= 0.9999999999962954\n",
            "iteration 3048: weight=1.000000000000884 loss= 0.9999999999963807\n",
            "iteration 3049: weight=1.0000000000008635 loss= 0.9999999999964642\n",
            "iteration 3050: weight=1.0000000000008435 loss= 0.9999999999965459\n",
            "iteration 3051: weight=1.000000000000824 loss= 0.9999999999966258\n",
            "iteration 3052: weight=1.000000000000805 loss= 0.999999999996704\n",
            "iteration 3053: weight=1.0000000000007863 loss= 0.9999999999967804\n",
            "iteration 3054: weight=1.000000000000768 loss= 0.999999999996855\n",
            "iteration 3055: weight=1.0000000000007503 loss= 0.9999999999969278\n",
            "iteration 3056: weight=1.000000000000733 loss= 0.9999999999969988\n",
            "iteration 3057: weight=1.0000000000007159 loss= 0.9999999999970681\n",
            "iteration 3058: weight=1.0000000000006992 loss= 0.9999999999971365\n",
            "iteration 3059: weight=1.000000000000683 loss= 0.9999999999972031\n",
            "iteration 3060: weight=1.0000000000006672 loss= 0.999999999997268\n",
            "iteration 3061: weight=1.0000000000006517 loss= 0.999999999997331\n",
            "iteration 3062: weight=1.0000000000006366 loss= 0.9999999999973932\n",
            "iteration 3063: weight=1.0000000000006217 loss= 0.9999999999974536\n",
            "iteration 3064: weight=1.0000000000006073 loss= 0.9999999999975131\n",
            "iteration 3065: weight=1.000000000000593 loss= 0.9999999999975708\n",
            "iteration 3066: weight=1.0000000000005793 loss= 0.9999999999976277\n",
            "iteration 3067: weight=1.0000000000005658 loss= 0.9999999999976827\n",
            "iteration 3068: weight=1.0000000000005524 loss= 0.9999999999977369\n",
            "iteration 3069: weight=1.0000000000005396 loss= 0.9999999999977902\n",
            "iteration 3070: weight=1.000000000000527 loss= 0.9999999999978417\n",
            "iteration 3071: weight=1.0000000000005145 loss= 0.9999999999978924\n",
            "iteration 3072: weight=1.0000000000005025 loss= 0.9999999999979421\n",
            "iteration 3073: weight=1.0000000000004907 loss= 0.99999999999799\n",
            "iteration 3074: weight=1.0000000000004792 loss= 0.9999999999980371\n",
            "iteration 3075: weight=1.0000000000004678 loss= 0.9999999999980833\n",
            "iteration 3076: weight=1.0000000000004567 loss= 0.9999999999981286\n",
            "iteration 3077: weight=1.000000000000446 loss= 0.999999999998173\n",
            "iteration 3078: weight=1.0000000000004357 loss= 0.9999999999982156\n",
            "iteration 3079: weight=1.0000000000004254 loss= 0.9999999999982574\n",
            "iteration 3080: weight=1.0000000000004154 loss= 0.9999999999982983\n",
            "iteration 3081: weight=1.0000000000004057 loss= 0.9999999999983382\n",
            "iteration 3082: weight=1.0000000000003961 loss= 0.9999999999983773\n",
            "iteration 3083: weight=1.0000000000003868 loss= 0.9999999999984155\n",
            "iteration 3084: weight=1.0000000000003777 loss= 0.9999999999984528\n",
            "iteration 3085: weight=1.0000000000003688 loss= 0.9999999999984892\n",
            "iteration 3086: weight=1.0000000000003602 loss= 0.9999999999985247\n",
            "iteration 3087: weight=1.0000000000003517 loss= 0.9999999999985594\n",
            "iteration 3088: weight=1.0000000000003433 loss= 0.9999999999985931\n",
            "iteration 3089: weight=1.000000000000335 loss= 0.9999999999986269\n",
            "iteration 3090: weight=1.000000000000327 loss= 0.9999999999986597\n",
            "iteration 3091: weight=1.0000000000003193 loss= 0.9999999999986917\n",
            "iteration 3092: weight=1.0000000000003118 loss= 0.9999999999987228\n",
            "iteration 3093: weight=1.0000000000003044 loss= 0.999999999998753\n",
            "iteration 3094: weight=1.000000000000297 loss= 0.9999999999987823\n",
            "iteration 3095: weight=1.00000000000029 loss= 0.9999999999988116\n",
            "iteration 3096: weight=1.000000000000283 loss= 0.99999999999884\n",
            "iteration 3097: weight=1.0000000000002764 loss= 0.9999999999988676\n",
            "iteration 3098: weight=1.0000000000002698 loss= 0.9999999999988942\n",
            "iteration 3099: weight=1.0000000000002633 loss= 0.9999999999989209\n",
            "iteration 3100: weight=1.0000000000002571 loss= 0.9999999999989466\n",
            "iteration 3101: weight=1.000000000000251 loss= 0.9999999999989715\n",
            "iteration 3102: weight=1.000000000000245 loss= 0.9999999999989964\n",
            "iteration 3103: weight=1.0000000000002391 loss= 0.9999999999990203\n",
            "iteration 3104: weight=1.0000000000002334 loss= 0.9999999999990434\n",
            "iteration 3105: weight=1.0000000000002278 loss= 0.9999999999990665\n",
            "iteration 3106: weight=1.0000000000002223 loss= 0.9999999999990887\n",
            "iteration 3107: weight=1.000000000000217 loss= 0.9999999999991109\n",
            "iteration 3108: weight=1.0000000000002118 loss= 0.9999999999991322\n",
            "iteration 3109: weight=1.0000000000002067 loss= 0.9999999999991527\n",
            "iteration 3110: weight=1.0000000000002018 loss= 0.9999999999991731\n",
            "iteration 3111: weight=1.000000000000197 loss= 0.9999999999991926\n",
            "iteration 3112: weight=1.0000000000001923 loss= 0.9999999999992122\n",
            "iteration 3113: weight=1.0000000000001876 loss= 0.9999999999992308\n",
            "iteration 3114: weight=1.0000000000001832 loss= 0.9999999999992495\n",
            "iteration 3115: weight=1.0000000000001787 loss= 0.9999999999992673\n",
            "iteration 3116: weight=1.0000000000001745 loss= 0.999999999999285\n",
            "iteration 3117: weight=1.0000000000001703 loss= 0.9999999999993019\n",
            "iteration 3118: weight=1.000000000000166 loss= 0.9999999999993188\n",
            "iteration 3119: weight=1.000000000000162 loss= 0.9999999999993356\n",
            "iteration 3120: weight=1.000000000000158 loss= 0.9999999999993516\n",
            "iteration 3121: weight=1.0000000000001543 loss= 0.9999999999993676\n",
            "iteration 3122: weight=1.0000000000001505 loss= 0.9999999999993827\n",
            "iteration 3123: weight=1.000000000000147 loss= 0.9999999999993978\n",
            "iteration 3124: weight=1.0000000000001434 loss= 0.999999999999412\n",
            "iteration 3125: weight=1.0000000000001399 loss= 0.9999999999994262\n",
            "iteration 3126: weight=1.0000000000001366 loss= 0.9999999999994404\n",
            "iteration 3127: weight=1.0000000000001332 loss= 0.9999999999994538\n",
            "iteration 3128: weight=1.00000000000013 loss= 0.9999999999994671\n",
            "iteration 3129: weight=1.0000000000001268 loss= 0.9999999999994804\n",
            "iteration 3130: weight=1.0000000000001237 loss= 0.9999999999994929\n",
            "iteration 3131: weight=1.0000000000001206 loss= 0.9999999999995053\n",
            "iteration 3132: weight=1.0000000000001177 loss= 0.9999999999995177\n",
            "iteration 3133: weight=1.0000000000001148 loss= 0.9999999999995293\n",
            "iteration 3134: weight=1.000000000000112 loss= 0.9999999999995408\n",
            "iteration 3135: weight=1.0000000000001092 loss= 0.9999999999995524\n",
            "iteration 3136: weight=1.0000000000001066 loss= 0.999999999999563\n",
            "iteration 3137: weight=1.000000000000104 loss= 0.9999999999995737\n",
            "iteration 3138: weight=1.0000000000001013 loss= 0.9999999999995843\n",
            "iteration 3139: weight=1.0000000000000988 loss= 0.999999999999595\n",
            "iteration 3140: weight=1.0000000000000964 loss= 0.9999999999996048\n",
            "iteration 3141: weight=1.000000000000094 loss= 0.9999999999996145\n",
            "iteration 3142: weight=1.0000000000000917 loss= 0.9999999999996243\n",
            "iteration 3143: weight=1.0000000000000895 loss= 0.9999999999996332\n",
            "iteration 3144: weight=1.0000000000000873 loss= 0.9999999999996421\n",
            "iteration 3145: weight=1.000000000000085 loss= 0.999999999999651\n",
            "iteration 3146: weight=1.000000000000083 loss= 0.9999999999996598\n",
            "iteration 3147: weight=1.000000000000081 loss= 0.9999999999996678\n",
            "iteration 3148: weight=1.000000000000079 loss= 0.9999999999996758\n",
            "iteration 3149: weight=1.000000000000077 loss= 0.9999999999996838\n",
            "iteration 3150: weight=1.000000000000075 loss= 0.9999999999996918\n",
            "iteration 3151: weight=1.0000000000000733 loss= 0.9999999999996998\n",
            "iteration 3152: weight=1.0000000000000715 loss= 0.9999999999997069\n",
            "iteration 3153: weight=1.0000000000000697 loss= 0.999999999999714\n",
            "iteration 3154: weight=1.000000000000068 loss= 0.9999999999997211\n",
            "iteration 3155: weight=1.0000000000000662 loss= 0.9999999999997282\n",
            "iteration 3156: weight=1.0000000000000646 loss= 0.9999999999997353\n",
            "iteration 3157: weight=1.000000000000063 loss= 0.9999999999997415\n",
            "iteration 3158: weight=1.0000000000000615 loss= 0.9999999999997478\n",
            "iteration 3159: weight=1.00000000000006 loss= 0.999999999999754\n",
            "iteration 3160: weight=1.0000000000000584 loss= 0.9999999999997602\n",
            "iteration 3161: weight=1.0000000000000568 loss= 0.9999999999997664\n",
            "iteration 3162: weight=1.0000000000000555 loss= 0.9999999999997726\n",
            "iteration 3163: weight=1.0000000000000542 loss= 0.999999999999778\n",
            "iteration 3164: weight=1.0000000000000528 loss= 0.9999999999997833\n",
            "iteration 3165: weight=1.0000000000000515 loss= 0.9999999999997886\n",
            "iteration 3166: weight=1.0000000000000502 loss= 0.9999999999997939\n",
            "iteration 3167: weight=1.0000000000000488 loss= 0.9999999999997993\n",
            "iteration 3168: weight=1.0000000000000475 loss= 0.9999999999998046\n",
            "iteration 3169: weight=1.0000000000000464 loss= 0.9999999999998099\n",
            "iteration 3170: weight=1.0000000000000453 loss= 0.9999999999998144\n",
            "iteration 3171: weight=1.0000000000000442 loss= 0.9999999999998188\n",
            "iteration 3172: weight=1.000000000000043 loss= 0.9999999999998233\n",
            "iteration 3173: weight=1.000000000000042 loss= 0.9999999999998277\n",
            "iteration 3174: weight=1.0000000000000409 loss= 0.9999999999998321\n",
            "iteration 3175: weight=1.0000000000000397 loss= 0.9999999999998366\n",
            "iteration 3176: weight=1.0000000000000386 loss= 0.999999999999841\n",
            "iteration 3177: weight=1.0000000000000377 loss= 0.9999999999998455\n",
            "iteration 3178: weight=1.0000000000000369 loss= 0.999999999999849\n",
            "iteration 3179: weight=1.000000000000036 loss= 0.9999999999998526\n",
            "iteration 3180: weight=1.000000000000035 loss= 0.9999999999998561\n",
            "iteration 3181: weight=1.0000000000000342 loss= 0.9999999999998597\n",
            "iteration 3182: weight=1.0000000000000333 loss= 0.9999999999998632\n",
            "iteration 3183: weight=1.0000000000000324 loss= 0.9999999999998668\n",
            "iteration 3184: weight=1.0000000000000315 loss= 0.9999999999998703\n",
            "iteration 3185: weight=1.0000000000000306 loss= 0.9999999999998739\n",
            "iteration 3186: weight=1.0000000000000298 loss= 0.9999999999998774\n",
            "iteration 3187: weight=1.000000000000029 loss= 0.999999999999881\n",
            "iteration 3188: weight=1.0000000000000284 loss= 0.9999999999998836\n",
            "iteration 3189: weight=1.0000000000000278 loss= 0.9999999999998863\n",
            "iteration 3190: weight=1.000000000000027 loss= 0.999999999999889\n",
            "iteration 3191: weight=1.0000000000000264 loss= 0.9999999999998916\n",
            "iteration 3192: weight=1.0000000000000258 loss= 0.9999999999998943\n",
            "iteration 3193: weight=1.000000000000025 loss= 0.999999999999897\n",
            "iteration 3194: weight=1.0000000000000244 loss= 0.9999999999998996\n",
            "iteration 3195: weight=1.0000000000000238 loss= 0.9999999999999023\n",
            "iteration 3196: weight=1.000000000000023 loss= 0.999999999999905\n",
            "iteration 3197: weight=1.0000000000000224 loss= 0.9999999999999076\n",
            "iteration 3198: weight=1.0000000000000218 loss= 0.9999999999999103\n",
            "iteration 3199: weight=1.000000000000021 loss= 0.999999999999913\n",
            "iteration 3200: weight=1.0000000000000207 loss= 0.9999999999999156\n",
            "iteration 3201: weight=1.0000000000000202 loss= 0.9999999999999174\n",
            "iteration 3202: weight=1.0000000000000198 loss= 0.9999999999999192\n",
            "iteration 3203: weight=1.0000000000000193 loss= 0.999999999999921\n",
            "iteration 3204: weight=1.0000000000000189 loss= 0.9999999999999227\n",
            "iteration 3205: weight=1.0000000000000184 loss= 0.9999999999999245\n",
            "iteration 3206: weight=1.000000000000018 loss= 0.9999999999999263\n",
            "iteration 3207: weight=1.0000000000000175 loss= 0.9999999999999281\n",
            "iteration 3208: weight=1.000000000000017 loss= 0.9999999999999298\n",
            "iteration 3209: weight=1.0000000000000167 loss= 0.9999999999999316\n",
            "iteration 3210: weight=1.0000000000000162 loss= 0.9999999999999334\n",
            "iteration 3211: weight=1.0000000000000158 loss= 0.9999999999999352\n",
            "iteration 3212: weight=1.0000000000000153 loss= 0.9999999999999369\n",
            "iteration 3213: weight=1.0000000000000149 loss= 0.9999999999999387\n",
            "iteration 3214: weight=1.0000000000000144 loss= 0.9999999999999405\n",
            "iteration 3215: weight=1.000000000000014 loss= 0.9999999999999423\n",
            "iteration 3216: weight=1.0000000000000135 loss= 0.999999999999944\n",
            "iteration 3217: weight=1.000000000000013 loss= 0.9999999999999458\n",
            "iteration 3218: weight=1.0000000000000127 loss= 0.9999999999999476\n",
            "iteration 3219: weight=1.0000000000000122 loss= 0.9999999999999494\n",
            "iteration 3220: weight=1.000000000000012 loss= 0.9999999999999512\n",
            "iteration 3221: weight=1.0000000000000118 loss= 0.999999999999952\n",
            "iteration 3222: weight=1.0000000000000115 loss= 0.9999999999999529\n",
            "iteration 3223: weight=1.0000000000000113 loss= 0.9999999999999538\n",
            "iteration 3224: weight=1.000000000000011 loss= 0.9999999999999547\n",
            "iteration 3225: weight=1.0000000000000109 loss= 0.9999999999999556\n",
            "iteration 3226: weight=1.0000000000000107 loss= 0.9999999999999565\n",
            "iteration 3227: weight=1.0000000000000104 loss= 0.9999999999999574\n",
            "iteration 3228: weight=1.0000000000000102 loss= 0.9999999999999583\n",
            "iteration 3229: weight=1.00000000000001 loss= 0.9999999999999591\n",
            "iteration 3230: weight=1.0000000000000098 loss= 0.99999999999996\n",
            "iteration 3231: weight=1.0000000000000095 loss= 0.9999999999999609\n",
            "iteration 3232: weight=1.0000000000000093 loss= 0.9999999999999618\n",
            "iteration 3233: weight=1.000000000000009 loss= 0.9999999999999627\n",
            "iteration 3234: weight=1.0000000000000089 loss= 0.9999999999999636\n",
            "iteration 3235: weight=1.0000000000000087 loss= 0.9999999999999645\n",
            "iteration 3236: weight=1.0000000000000084 loss= 0.9999999999999654\n",
            "iteration 3237: weight=1.0000000000000082 loss= 0.9999999999999662\n",
            "iteration 3238: weight=1.000000000000008 loss= 0.9999999999999671\n",
            "iteration 3239: weight=1.0000000000000078 loss= 0.999999999999968\n",
            "iteration 3240: weight=1.0000000000000075 loss= 0.9999999999999689\n",
            "iteration 3241: weight=1.0000000000000073 loss= 0.9999999999999698\n",
            "iteration 3242: weight=1.000000000000007 loss= 0.9999999999999707\n",
            "iteration 3243: weight=1.0000000000000069 loss= 0.9999999999999716\n",
            "iteration 3244: weight=1.0000000000000067 loss= 0.9999999999999725\n",
            "iteration 3245: weight=1.0000000000000064 loss= 0.9999999999999734\n",
            "iteration 3246: weight=1.0000000000000062 loss= 0.9999999999999742\n",
            "iteration 3247: weight=1.000000000000006 loss= 0.9999999999999751\n",
            "iteration 3248: weight=1.0000000000000058 loss= 0.999999999999976\n",
            "iteration 3249: weight=1.0000000000000056 loss= 0.9999999999999769\n",
            "iteration 3250: weight=1.0000000000000053 loss= 0.9999999999999778\n",
            "iteration 3251: weight=1.000000000000005 loss= 0.9999999999999787\n",
            "iteration 3252: weight=1.0000000000000049 loss= 0.9999999999999796\n",
            "iteration 3253: weight=1.0000000000000047 loss= 0.9999999999999805\n",
            "iteration 3254: weight=1.0000000000000044 loss= 0.9999999999999813\n",
            "iteration 3255: weight=1.0000000000000042 loss= 0.9999999999999822\n",
            "iteration 3256: weight=1.000000000000004 loss= 0.9999999999999831\n",
            "iteration 3257: weight=1.0000000000000038 loss= 0.999999999999984\n",
            "iteration 3258: weight=1.0000000000000036 loss= 0.9999999999999849\n",
            "iteration 3259: weight=1.0000000000000033 loss= 0.9999999999999858\n",
            "converged after 3260 iterations\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the curve for cost vs epoch\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "plt.plot(itr, loss)\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "nYtqb54qWTu3",
        "outputId": "00c37023-5b6b-4c89-aa58-be3291fbd273"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "class MyMomentumOptimizer ():\n",
        "  def __init__ (self, learning_rate, momentum = 0.9 ):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.momentum = momentum \n",
        "    self.w = 0\n",
        "    self.b = 0\n",
        "    self.momentum_vector_w = 0\n",
        "    self.momentum_vector_b = 0\n",
        "  def _get_batch (self, X, y, batch_size):\n",
        "    indexes = np.random.randint(len(X), size=batch_size)\n",
        "    return X[indexes,:], y[indexes,:]\n",
        "  def _get_momentum_vector (self, X_batch, y_batch):\n",
        "    f = y_batch - (self.w * X_batch + self.b)\n",
        "    self.momentum_vector_w = self.momentum * self.momentum_vector_w + \\\n",
        "    self.learning_rate * ( -2 * X_batch.dot(f.T).sum() / len(X_batch))\n",
        "    self.momentum_vector_b = self.momentum * self.momentum_vector_b + \\\n",
        "    self.learning_rate * ( -2 * f.sum() / len(X_batch))\n",
        "  def fit (self, X, y, batch_size = 32 , epochs = 100 ):\n",
        "    history = []\n",
        "    for e in range(epochs):\n",
        "      indexes = np.random.randint(len(X), size=batch_size)\n",
        "      X_batch, y_batch = self._get_batch(X, y, batch_size)\n",
        "      self._get_momentum_vector(X_batch, y_batch)\n",
        "      self.w -= self.momentum_vector_w\n",
        "      self.b -= self.momentum_vector_b\n",
        "      loss = mean_squared_error(y_batch, (self.w * X_batch + self.b))\n",
        "      if e % 100 == 0 :\n",
        "        print( f\"Epoch: {e} , Loss: {loss} )\" )\n",
        "      history.append(loss)\n",
        "    return history\n",
        "  def predict (self, X):\n",
        "    return self.w * X + self.b "
      ],
      "metadata": {
        "id": "npwr2abKcL8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pUhTyRe0h-R0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}